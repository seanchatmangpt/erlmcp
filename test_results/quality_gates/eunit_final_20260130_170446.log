[0;32m===> Verifying dependencies...
[0m[0;32m===> Analyzing applications...
[0m[0;32m===> Compiling erlmcp_core
[0m     â”Œâ”€ apps/erlmcp_core/src/erlmcp_server.erl:
     â”‚
 887 â”‚                  [1;31m{[0mnoreply, State};[0m
     â”‚                  [1;31mâ•°â”€â”€[0m[0m Warning: a term is constructed, but never used

     â”Œâ”€ apps/erlmcp_core/src/erlmcp_server.erl:
     â”‚
 897 â”‚                                  [1;31m{[0mnoreply, State};[0m
     â”‚                                  [1;31mâ•°â”€â”€[0m[0m Warning: a term is constructed, but never used

     â”Œâ”€ apps/erlmcp_core/src/erlmcp_server.erl:
     â”‚
 902 â”‚                                  [1;31m{[0mnoreply, State}[0m
     â”‚                                  [1;31mâ•°â”€â”€[0m[0m Warning: a term is constructed, but never used

     â”Œâ”€ apps/erlmcp_core/src/erlmcp_server.erl:
     â”‚
 908 â”‚                          [1;31m{[0mnoreply, State}[0m
     â”‚                          [1;31mâ•°â”€â”€[0m[0m Warning: a term is constructed, but never used


[0;32m===> Compiling erlmcp_transports
[0m[0;32m===> Compiling erlmcp_validation
[0m[0;32m===> Compiling erlmcp_observability
[0m[0;32m===> Analyzing applications...
[0m[0;32m===> Compiling extra_test
[0m[0;35m===> Cover compilation failed: {no_abstract_code,
                                       "/Users/sac/erlmcp/_build/test/lib/erlmcp_core/ebin/erlmcp_cache_tests.beam"}
[0m[0;35m===> Cover compilation failed: {no_abstract_code,
                                       "/Users/sac/erlmcp/_build/test/lib/erlmcp_core/ebin/erlmcp_rate_limiting_tests.beam"}
[0m[0;32m===> Performing EUnit tests...
[0m======================== EUnit ========================
file "erlmcp_validation.app"
  application 'erlmcp_validation'
    module 'erlmcp_compliance_report'
      module 'erlmcp_compliance_report_tests'
        erlmcp_compliance_report_tests: calculate_overall_compliance_test...ok
        erlmcp_compliance_report_tests: calculate_section_compliance_test...ok
        erlmcp_compliance_report_tests: handle_empty_test_results_test...ok
        erlmcp_compliance_report_tests: handle_missing_requirements_test...ok
        erlmcp_compliance_report_tests: generate_markdown_report_test...*failed*
in function erlmcp_compliance_report_tests:generate_markdown_report_test/0 (/Users/sac/erlmcp/apps/erlmcp_validation/test/erlmcp_compliance_report_tests.erl, line 146)
in call from eunit_test:'-mf_wrapper/2-fun-0-'/2 (eunit_test.erl, line 274)
in call from eunit_test:run_testfun/1 (eunit_test.erl, line 72)
in call from eunit_proc:run_test/1 (eunit_proc.erl, line 544)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 369)
in call from eunit_proc:handle_test/2 (eunit_proc.erl, line 527)
in call from eunit_proc:tests_inorder/3 (eunit_proc.erl, line 469)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 359)
**error:{badmatch,{error,{report_generation_failed,{badkey,status}}}}
  output:<<"">>

        erlmcp_compliance_report_tests: generate_json_report_test...*failed*
in function erlmcp_compliance_report_tests:generate_json_report_test/0 (/Users/sac/erlmcp/apps/erlmcp_validation/test/erlmcp_compliance_report_tests.erl, line 172)
in call from eunit_test:'-mf_wrapper/2-fun-0-'/2 (eunit_test.erl, line 274)
in call from eunit_test:run_testfun/1 (eunit_test.erl, line 72)
in call from eunit_proc:run_test/1 (eunit_proc.erl, line 544)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 369)
in call from eunit_proc:handle_test/2 (eunit_proc.erl, line 527)
in call from eunit_proc:tests_inorder/3 (eunit_proc.erl, line 469)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 359)
**error:{badmatch,#{<<"by_section">> =>
                #{<<"Lifecycle">> => 100.0,<<"Prompts">> => 0.0,
                  <<"Resources">> => 0.0,<<"Tools">> => 100.0,
                  <<"Transports">> => 100.0},
            <<"details">> =>
                #{<<"by_section">> =>
                      #{<<"Lifecycle">> => 100.0,<<"Prompts">> => 0.0,
                        <<"Resources">> => 0.0,<<"Tools">> => 100.0,
                        <<"Transports">> => 100.0},
                  <<"passed_tests">> => 3,<<"total_requirements">> => 5},
            <<"evidence">> =>
                [#{<<"evidence">> =>
                       <<"Request before initialize returned error">>,
                   <<"requirement">> => <<"initialize_required">>,
                   <<"status">> => <<"passed">>,
                   <<"test">> => <<"initialize_must_be_first_test">>,
                   <<"timestamp">> => <<"2026-01-30T12:00:00Z">>},
                 #{<<"evidence">> =>
                       <<"tools/list returned array of 3 tools">>,
                   <<"requirement">> => <<"tools_list_array">>,
                   <<"status">> => <<"passed">>,
                   <<"test">> => <<"tools_list_returns_array_test">>,
                   <<"timestamp">> => <<"2026-01-30T12:01:00Z">>},
                 #{<<"evidence">> => <<"Messages properly delimited">>,
                   <<"requirement">> => <<"stdio_format">>,
                   <<"status">> => <<"passed">>,
                   <<"test">> => <<"stdio_newline_delimited_test">>,
                   <<"timestamp">> => <<"2026-01-30T12:03:00Z">>}],
            <<"gaps">> =>
                [#{<<"recommendation">> =>
                       <<"Create tests to validate this requirement">>,
                   <<"requirement">> =>
                       #{<<"description">> =>
                             <<"prompts/get retrieves prompt by name">>,
                         <<"id">> => <<"req-005">>,
                         <<"name">> => <<"prompts_get">>,
                         <<"section">> => <<"Prompts">>},
                   <<"severity">> => <<"high">>,<<"status">> => <<"missing">>},
                 #{<<"recommendation">> =>
                       <<"Fix failing tests for this requirement">>,
                   <<"requirement">> =>
                       #{<<"description">> =>
                             <<"Server must support resource subscriptions">>,
                         <<"id">> => <<"req-003">>,
                         <<"name">> => <<"resources_subscribe">>,
                         <<"section">> => <<"Resources">>},
                   <<"severity">> => <<"high">>,<<"status">> => <<"failed">>}],
            <<"overall">> => 60.0,
            <<"recommendations">> =>
                [<<"Review this requirement">>,
                 <<"Warning: Compliance below 80%. Review and fix gaps.">>],
            <<"spec_version">> => <<"2025-11-25">>,
            <<"timestamp">> => <<"2026-01-30T12:00:00Z">>,
            <<"traceability">> =>
                #{<<"req-001">> =>
                      #{<<"last_tested">> => <<"2026-01-30T12:00:00Z">>,
                        <<"requirement">> => <<"initialize_required">>,
                        <<"section">> => <<"Lifecycle">>,
                        <<"status">> => <<"passed">>,
                        <<"tests">> => [<<"initialize_must_be_first_test">>]},
                  <<"req-002">> =>
                      #{<<"last_tested">> => <<"2026-01-30T12:01:00Z">>,
                        <<"requirement">> => <<"tools_list_array">>,
                        <<"section">> => <<"Tools">>,
                        <<"status">> => <<"passed">>,
                        <<"tests">> => [<<"tools_list_returns_array_test">>]},
                  <<"req-003">> =>
                      #{<<"last_tested">> => <<"2026-01-30T12:02:00Z">>,
                        <<"requirement">> => <<"resources_subscribe">>,
                        <<"section">> => <<"Resources">>,
                        <<"status">> => <<"failed">>,
                        <<"tests">> => [<<"resources_subscribe_test">>]},
                  <<"req-004">> =>
                      #{<<"last_tested">> => <<"2026-01-30T12:03:00Z">>,
                        <<"requirement">> => <<"stdio_format">>,
                        <<"section">> => <<"Transports">>,
                        <<"status">> => <<"passed">>,
                        <<"tests">> => [<<"stdio_newline_delimited_test">>]},
                  <<"req-005">> =>
                      #{<<"last_tested">> => <<"never">>,
                        <<"requirement">> => <<"prompts_get">>,
                        <<"section">> => <<"Prompts">>,
                        <<"status">> => <<"untested">>,<<"tests">> => []}}}}
  output:<<"">>

        erlmcp_compliance_report_tests: generate_html_report_test...*failed*
in function erlmcp_compliance_report_tests:generate_html_report_test/0 (/Users/sac/erlmcp/apps/erlmcp_validation/test/erlmcp_compliance_report_tests.erl, line 191)
in call from eunit_test:'-mf_wrapper/2-fun-0-'/2 (eunit_test.erl, line 274)
in call from eunit_test:run_testfun/1 (eunit_test.erl, line 72)
in call from eunit_proc:run_test/1 (eunit_proc.erl, line 544)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 369)
in call from eunit_proc:handle_test/2 (eunit_proc.erl, line 527)
in call from eunit_proc:tests_inorder/3 (eunit_proc.erl, line 469)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 359)
**error:{badmatch,{error,{report_generation_failed,{badkey,status}}}}
  output:<<"">>

        erlmcp_compliance_report_tests: matrix_maps_requirements_to_tests_test...ok
        erlmcp_compliance_report_tests: matrix_includes_status_test...ok
        erlmcp_compliance_report_tests: handles_untested_requirements_test...ok
        erlmcp_compliance_report_tests: identifies_missing_tests_test...*failed*
in function erlmcp_compliance_report_tests:identifies_missing_tests_test/0 (/Users/sac/erlmcp/apps/erlmcp_validation/test/erlmcp_compliance_report_tests.erl, line 251)
in call from eunit_test:'-mf_wrapper/2-fun-0-'/2 (eunit_test.erl, line 274)
in call from eunit_test:run_testfun/1 (eunit_test.erl, line 72)
in call from eunit_proc:run_test/1 (eunit_proc.erl, line 544)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 369)
in call from eunit_proc:handle_test/2 (eunit_proc.erl, line 527)
in call from eunit_proc:tests_inorder/3 (eunit_proc.erl, line 469)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 359)
**error:{assert,[{module,erlmcp_compliance_report_tests},
         {line,251},
         {expression,"length ( MissingGaps ) > 0"},
         {expected,true},
         {value,false}]}
  output:<<"">>

        erlmcp_compliance_report_tests: identifies_failed_tests_test...*failed*
in function erlmcp_compliance_report_tests:identifies_failed_tests_test/0 (/Users/sac/erlmcp/apps/erlmcp_validation/test/erlmcp_compliance_report_tests.erl, line 259)
in call from eunit_test:'-mf_wrapper/2-fun-0-'/2 (eunit_test.erl, line 274)
in call from eunit_test:run_testfun/1 (eunit_test.erl, line 72)
in call from eunit_proc:run_test/1 (eunit_proc.erl, line 544)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 369)
in call from eunit_proc:handle_test/2 (eunit_proc.erl, line 527)
in call from eunit_proc:tests_inorder/3 (eunit_proc.erl, line 469)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 359)
**error:{assert,[{module,erlmcp_compliance_report_tests},
         {line,259},
         {expression,"length ( FailedGaps ) > 0"},
         {expected,true},
         {value,false}]}
  output:<<"">>

        erlmcp_compliance_report_tests: classifies_gaps_by_severity_test...ok
        erlmcp_compliance_report_tests: summary_includes_overall_compliance_test...*failed*
in function erlmcp_compliance_report_tests:summary_includes_overall_compliance_test/0 (/Users/sac/erlmcp/apps/erlmcp_validation/test/erlmcp_compliance_report_tests.erl, line 278)
in call from eunit_test:'-mf_wrapper/2-fun-0-'/2 (eunit_test.erl, line 274)
in call from eunit_test:run_testfun/1 (eunit_test.erl, line 72)
in call from eunit_proc:run_test/1 (eunit_proc.erl, line 544)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 369)
in call from eunit_proc:handle_test/2 (eunit_proc.erl, line 527)
in call from eunit_proc:tests_inorder/3 (eunit_proc.erl, line 469)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 359)
**error:{badmatch,#{<<"by_section">> =>
                #{<<"Lifecycle">> => 100.0,<<"Prompts">> => 0.0,
                  <<"Resources">> => 0.0,<<"Tools">> => 100.0,
                  <<"Transports">> => 100.0},
            <<"details">> =>
                #{<<"by_section">> =>
                      #{<<"Lifecycle">> => 100.0,<<"Prompts">> => 0.0,
                        <<"Resources">> => 0.0,<<"Tools">> => 100.0,
                        <<"Transports">> => 100.0},
                  <<"passed_tests">> => 3,<<"total_requirements">> => 5},
            <<"evidence">> =>
                [#{<<"evidence">> =>
                       <<"Request before initialize returned error">>,
                   <<"requirement">> => <<"initialize_required">>,
                   <<"status">> => <<"passed">>,
                   <<"test">> => <<"initialize_must_be_first_test">>,
                   <<"timestamp">> => <<"2026-01-30T12:00:00Z">>},
                 #{<<"evidence">> =>
                       <<"tools/list returned array of 3 tools">>,
                   <<"requirement">> => <<"tools_list_array">>,
                   <<"status">> => <<"passed">>,
                   <<"test">> => <<"tools_list_returns_array_test">>,
                   <<"timestamp">> => <<"2026-01-30T12:01:00Z">>},
                 #{<<"evidence">> => <<"Messages properly delimited">>,
                   <<"requirement">> => <<"stdio_format">>,
                   <<"status">> => <<"passed">>,
                   <<"test">> => <<"stdio_newline_delimited_test">>,
                   <<"timestamp">> => <<"2026-01-30T12:03:00Z">>}],
            <<"gaps">> =>
                [#{<<"recommendation">> =>
                       <<"Create tests to validate this requirement">>,
                   <<"requirement">> =>
                       #{<<"description">> =>
                             <<"prompts/get retrieves prompt by name">>,
                         <<"id">> => <<"req-005">>,
                         <<"name">> => <<"prompts_get">>,
                         <<"section">> => <<"Prompts">>},
                   <<"severity">> => <<"high">>,<<"status">> => <<"missing">>},
                 #{<<"recommendation">> =>
                       <<"Fix failing tests for this requirement">>,
                   <<"requirement">> =>
                       #{<<"description">> =>
                             <<"Server must support resource subscriptions">>,
                         <<"id">> => <<"req-003">>,
                         <<"name">> => <<"resources_subscribe">>,
                         <<"section">> => <<"Resources">>},
                   <<"severity">> => <<"high">>,<<"status">> => <<"failed">>}],
            <<"overall">> => 60.0,
            <<"recommendations">> =>
                [<<"Review this requirement">>,
                 <<"Warning: Compliance below 80%. Review and fix gaps.">>],
            <<"spec_version">> => <<"2025-11-25">>,
            <<"timestamp">> => <<"2026-01-30T12:00:00Z">>,
            <<"traceability">> =>
                #{<<"req-001">> =>
                      #{<<"last_tested">> => <<"2026-01-30T12:00:00Z">>,
                        <<"requirement">> => <<"initialize_required">>,
                        <<"section">> => <<"Lifecycle">>,
                        <<"status">> => <<"passed">>,
                        <<"tests">> => [<<"initialize_must_be_first_test">>]},
                  <<"req-002">> =>
                      #{<<"last_tested">> => <<"2026-01-30T12:01:00Z">>,
                        <<"requirement">> => <<"tools_list_array">>,
                        <<"section">> => <<"Tools">>,
                        <<"status">> => <<"passed">>,
                        <<"tests">> => [<<"tools_list_returns_array_test">>]},
                  <<"req-003">> =>
                      #{<<"last_tested">> => <<"2026-01-30T12:02:00Z">>,
                        <<"requirement">> => <<"resources_subscribe">>,
                        <<"section">> => <<"Resources">>,
                        <<"status">> => <<"failed">>,
                        <<"tests">> => [<<"resources_subscribe_test">>]},
                  <<"req-004">> =>
                      #{<<"last_tested">> => <<"2026-01-30T12:03:00Z">>,
                        <<"requirement">> => <<"stdio_format">>,
                        <<"section">> => <<"Transports">>,
                        <<"status">> => <<"passed">>,
                        <<"tests">> => [<<"stdio_newline_delimited_test">>]},
                  <<"req-005">> =>
                      #{<<"last_tested">> => <<"never">>,
                        <<"requirement">> => <<"prompts_get">>,
                        <<"section">> => <<"Prompts">>,
                        <<"status">> => <<"untested">>,<<"tests">> => []}}}}
  output:<<"">>

        erlmcp_compliance_report_tests: summary_counts_sections_test...*failed*
in function erlmcp_compliance_report_tests:summary_counts_sections_test/0 (/Users/sac/erlmcp/apps/erlmcp_validation/test/erlmcp_compliance_report_tests.erl, line 288)
in call from eunit_test:'-mf_wrapper/2-fun-0-'/2 (eunit_test.erl, line 274)
in call from eunit_test:run_testfun/1 (eunit_test.erl, line 72)
in call from eunit_proc:run_test/1 (eunit_proc.erl, line 544)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 369)
in call from eunit_proc:handle_test/2 (eunit_proc.erl, line 527)
in call from eunit_proc:tests_inorder/3 (eunit_proc.erl, line 469)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 359)
**error:{badmatch,#{<<"by_section">> =>
                #{<<"Lifecycle">> => 100.0,<<"Prompts">> => 0.0,
                  <<"Resources">> => 0.0,<<"Tools">> => 100.0,
                  <<"Transports">> => 100.0},
            <<"details">> =>
                #{<<"by_section">> =>
                      #{<<"Lifecycle">> => 100.0,<<"Prompts">> => 0.0,
                        <<"Resources">> => 0.0,<<"Tools">> => 100.0,
                        <<"Transports">> => 100.0},
                  <<"passed_tests">> => 3,<<"total_requirements">> => 5},
            <<"evidence">> =>
                [#{<<"evidence">> =>
                       <<"Request before initialize returned error">>,
                   <<"requirement">> => <<"initialize_required">>,
                   <<"status">> => <<"passed">>,
                   <<"test">> => <<"initialize_must_be_first_test">>,
                   <<"timestamp">> => <<"2026-01-30T12:00:00Z">>},
                 #{<<"evidence">> =>
                       <<"tools/list returned array of 3 tools">>,
                   <<"requirement">> => <<"tools_list_array">>,
                   <<"status">> => <<"passed">>,
                   <<"test">> => <<"tools_list_returns_array_test">>,
                   <<"timestamp">> => <<"2026-01-30T12:01:00Z">>},
                 #{<<"evidence">> => <<"Messages properly delimited">>,
                   <<"requirement">> => <<"stdio_format">>,
                   <<"status">> => <<"passed">>,
                   <<"test">> => <<"stdio_newline_delimited_test">>,
                   <<"timestamp">> => <<"2026-01-30T12:03:00Z">>}],
            <<"gaps">> =>
                [#{<<"recommendation">> =>
                       <<"Create tests to validate this requirement">>,
                   <<"requirement">> =>
                       #{<<"description">> =>
                             <<"prompts/get retrieves prompt by name">>,
                         <<"id">> => <<"req-005">>,
                         <<"name">> => <<"prompts_get">>,
                         <<"section">> => <<"Prompts">>},
                   <<"severity">> => <<"high">>,<<"status">> => <<"missing">>},
                 #{<<"recommendation">> =>
                       <<"Fix failing tests for this requirement">>,
                   <<"requirement">> =>
                       #{<<"description">> =>
                             <<"Server must support resource subscriptions">>,
                         <<"id">> => <<"req-003">>,
                         <<"name">> => <<"resources_subscribe">>,
                         <<"section">> => <<"Resources">>},
                   <<"severity">> => <<"high">>,<<"status">> => <<"failed">>}],
            <<"overall">> => 60.0,
            <<"recommendations">> =>
                [<<"Review this requirement">>,
                 <<"Warning: Compliance below 80%. Review and fix gaps.">>],
            <<"spec_version">> => <<"2025-11-25">>,
            <<"timestamp">> => <<"2026-01-30T12:00:00Z">>,
            <<"traceability">> =>
                #{<<"req-001">> =>
                      #{<<"last_tested">> => <<"2026-01-30T12:00:00Z">>,
                        <<"requirement">> => <<"initialize_required">>,
                        <<"section">> => <<"Lifecycle">>,
                        <<"status">> => <<"passed">>,
                        <<"tests">> => [<<"initialize_must_be_first_test">>]},
                  <<"req-002">> =>
                      #{<<"last_tested">> => <<"2026-01-30T12:01:00Z">>,
                        <<"requirement">> => <<"tools_list_array">>,
                        <<"section">> => <<"Tools">>,
                        <<"status">> => <<"passed">>,
                        <<"tests">> => [<<"tools_list_returns_array_test">>]},
                  <<"req-003">> =>
                      #{<<"last_tested">> => <<"2026-01-30T12:02:00Z">>,
                        <<"requirement">> => <<"resources_subscribe">>,
                        <<"section">> => <<"Resources">>,
                        <<"status">> => <<"failed">>,
                        <<"tests">> => [<<"resources_subscribe_test">>]},
                  <<"req-004">> =>
                      #{<<"last_tested">> => <<"2026-01-30T12:03:00Z">>,
                        <<"requirement">> => <<"stdio_format">>,
                        <<"section">> => <<"Transports">>,
                        <<"status">> => <<"passed">>,
                        <<"tests">> => [<<"stdio_newline_delimited_test">>]},
                  <<"req-005">> =>
                      #{<<"last_tested">> => <<"never">>,
                        <<"requirement">> => <<"prompts_get">>,
                        <<"section">> => <<"Prompts">>,
                        <<"status">> => <<"untested">>,<<"tests">> => []}}}}
  output:<<"">>

        erlmcp_compliance_report_tests: full_report_generation_workflow_test...*failed*
in function erlmcp_compliance_report_tests:full_report_generation_workflow_test/0 (/Users/sac/erlmcp/apps/erlmcp_validation/test/erlmcp_compliance_report_tests.erl, line 303)
in call from eunit_test:'-mf_wrapper/2-fun-0-'/2 (eunit_test.erl, line 274)
in call from eunit_test:run_testfun/1 (eunit_test.erl, line 72)
in call from eunit_proc:run_test/1 (eunit_proc.erl, line 544)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 369)
in call from eunit_proc:handle_test/2 (eunit_proc.erl, line 527)
in call from eunit_proc:tests_inorder/3 (eunit_proc.erl, line 469)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 359)
**error:{badmatch,{error,{report_generation_failed,{badkey,status}}}}
  output:<<"">>

        erlmcp_compliance_report_tests: all_formats_consistent_test...*failed*
in function erlmcp_compliance_report_tests:all_formats_consistent_test/0 (/Users/sac/erlmcp/apps/erlmcp_validation/test/erlmcp_compliance_report_tests.erl, line 319)
in call from eunit_test:'-mf_wrapper/2-fun-0-'/2 (eunit_test.erl, line 274)
in call from eunit_test:run_testfun/1 (eunit_test.erl, line 72)
in call from eunit_proc:run_test/1 (eunit_proc.erl, line 544)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 369)
in call from eunit_proc:handle_test/2 (eunit_proc.erl, line 527)
in call from eunit_proc:tests_inorder/3 (eunit_proc.erl, line 469)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 359)
**error:{badmatch,{error,{report_generation_failed,{badkey,status}}}}
  output:<<"">>

        [done in 0.070 s]
      [done in 0.070 s]
    module 'erlmcp_memory_manager'
      module 'erlmcp_memory_manager_tests'
        erlmcp_memory_manager_tests:25: -cache_basic_test_/0-fun-1-...*failed*
in function erlmcp_memory_manager_tests:'-cache_basic_test_/0-fun-1-'/0 (/Users/sac/erlmcp/apps/erlmcp_validation/test/erlmcp_memory_manager_tests.erl, line 27)
in call from eunit_test:run_testfun/1 (eunit_test.erl, line 72)
in call from eunit_proc:run_test/1 (eunit_proc.erl, line 544)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 369)
in call from eunit_proc:handle_test/2 (eunit_proc.erl, line 527)
in call from eunit_proc:tests_inorder/3 (eunit_proc.erl, line 469)
in call from eunit_proc:with_timeout/3 (eunit_proc.erl, line 359)
in call from eunit_proc:run_group/2 (eunit_proc.erl, line 583)
**error:{assertMatch,[{module,erlmcp_memory_manager_tests},
              {line,27},
              {expression,"erlmcp_memory_manager : cache_spec ( Spec )"},
              {pattern,"{ ok , _ }"},
              {value,ok}]}
  output:<<"">>

        erlmcp_memory_manager_tests:37: -memory_usage_test_/0-fun-2-...=ERROR REPORT==== 30-Jan-2026::17:04:53.134028 ===
** Generic server erlmcp_memory_manager terminating 
** Last message in was get_memory_usage
** When Server state == {state,#{},100,104857600,
                               #Ref<0.4278733892.1335099393.228623>,
                               #Ref<0.4278733892.1335099393.228624>,168369390,
                               #{cache_hits => 0,cache_misses => 0,
                                 cache_evictions => 0,gc_runs => 0}}
** Reason for termination ==
** {function_clause,
       [{lists,foldl,
            [#Fun<erlmcp_memory_manager.5.88738527>,0,[]],
            [{file,"lists.erl"},{line,2144}]},
        {erlmcp_memory_manager,calculate_memory_stats,1,
            [{file,
                 "/Users/sac/erlmcp/apps/erlmcp_validation/src/erlmcp_memory_manager.erl"},
             {line,230}]},
        {erlmcp_memory_manager,handle_call,3,
            [{file,
                 "/Users/sac/erlmcp/apps/erlmcp_validation/src/erlmcp_memory_manager.erl"},
             {line,102}]},
        {gen_server,try_handle_call,4,[{file,"gen_server.erl"},{line,2381}]},
        {gen_server,handle_msg,6,[{file,"gen_server.erl"},{line,2410}]},
        {proc_lib,init_p_do_apply,3,[{file,"proc_lib.erl"},{line,329}]}]}
** Client <0.2864.0> stacktrace
** [{gen,do_call,4,[{file,"gen.erl"},{line,260}]},
    {gen_server,call,2,[{file,"gen_server.erl"},{line,1138}]},
    {erlmcp_memory_manager_tests,'-memory_usage_test_/0-fun-2-',0,
        [{file,
             "/Users/sac/erlmcp/apps/erlmcp_validation/test/erlmcp_memory_manager_tests.erl"},
         {line,38}]},
    {eunit_test,run_testfun,1,[{file,"eunit_test.erl"},{line,72}]},
    {eunit_proc,run_test,1,[{file,"eunit_proc.erl"},{line,544}]},
    {eunit_proc,with_timeout,3,[{file,"eunit_proc.erl"},{line,369}]},
    {eunit_proc,handle_test,2,[{file,"eunit_proc.erl"},{line,527}]},
    {eunit_proc,tests_inorder,3,[{file,"eunit_proc.erl"},{line,469}]}]

=CRASH REPORT==== 30-Jan-2026::17:04:53.134207 ===
  crasher:
    initial call: erlmcp_memory_manager:init/1
    pid: <0.2862.0>
    registered_name: erlmcp_memory_manager
    exception error: no function clause matching 
                     lists:foldl(#Fun<erlmcp_memory_manager.5.88738527>,0,[]) (lists.erl, line 2144)
      in function  erlmcp_memory_manager:calculate_memory_stats/1 (/Users/sac/erlmcp/apps/erlmcp_validation/src/erlmcp_memory_manager.erl, line 230)
      in call from erlmcp_memory_manager:handle_call/3 (/Users/sac/erlmcp/apps/erlmcp_validation/src/erlmcp_memory_manager.erl, line 102)
      in call from gen_server:try_handle_call/4 (gen_server.erl, line 2381)
      in call from gen_server:handle_msg/6 (gen_server.erl, line 2410)
    ancestors: [<0.2834.0>]
    message_queue_len: 0
    messages: []
    links: [<0.2834.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 28690
    stack_size: 29
    reductions: 25730
  neighbours:
    neighbour:
      pid: <0.2834.0>
      registered_name: []
      initial_call: {erlang,apply,2}
      current_function: {eunit_proc,wait_for_tasks,2}
      ancestors: []
      message_queue_len: 0
      links: [<0.2862.0>,<0.2863.0>,<0.2833.0>]
      trap_exit: false
      status: waiting
      heap_size: 4185
      stack_size: 81
      reductions: 95682
      current_stacktrace: [{eunit_proc,wait_for_tasks,2,
                              [{file,"eunit_proc.erl"},{line,428}]},
                  {eunit_proc,tests_inorder,3,
                              [{file,"eunit_proc.erl"},{line,469}]},
                  {eunit_proc,with_timeout,3,
                              [{file,"eunit_proc.erl"},{line,359}]},
                  {eunit_test,enter_context,4,
                              [{file,"eunit_test.erl"},{line,347}]},
                  {eunit_proc,run_group,2,
                              [{file,"eunit_proc.erl"},{line,583}]},
                  {eunit_proc,tests_inorder,3,
                              [{file,"eunit_proc.erl"},{line,469}]},
                  {eunit_proc,with_timeout,3,
                              [{file,"eunit_proc.erl"},{line,359}]},
                  {eunit_proc,run_group,2,
                              [{file,"eunit_proc.erl"},{line,583}]}]
*skipped*
undefined
*unexpected termination of test process*
::{function_clause,
      [{lists,foldl,
           [#Fun<erlmcp_memory_manager.5.88738527>,0,[]],
           [{file,"lists.erl"},{line,2144}]},
       {erlmcp_memory_manager,calculate_memory_stats,1,
           [{file,
                "/Users/sac/erlmcp/apps/erlmcp_validation/src/erlmcp_memory_manager.erl"},
            {line,230}]},
       {erlmcp_memory_manager,handle_call,3,
           [{file,
                "/Users/sac/erlmcp/apps/erlmcp_validation/src/erlmcp_memory_manager.erl"},
            {line,102}]},
       {gen_server,try_handle_call,4,[{file,"gen_server.erl"},{line,2381}]},
       {gen_server,handle_msg,6,[{file,"gen_server.erl"},{line,2410}]},
       {proc_lib,init_p_do_apply,3,[{file,"proc_lib.erl"},{line,329}]}]}

=======================================================
  Failed: 10.  Skipped: 0.  Passed: 8.
One or more tests were cancelled.
[0;31m===> [1mError running tests
[0m[0m