{
    "sourceFile": "src/erlmcp_health_monitor.erl",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1756186304594,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1756186304594,
            "name": "Commit-0",
            "content": "-module(erlmcp_health_monitor).\n-behaviour(gen_server).\n\n%% API\n-export([\n    start_link/0, start_link/1,\n    register_component/2, register_component/3,\n    unregister_component/1,\n    get_system_health/0, get_component_health/1,\n    get_all_component_health/0,\n    set_health_check_config/2, get_health_check_config/1,\n    report_circuit_breaker/2, report_degradation/1,\n    reset_health_status/0,\n    trigger_health_check/1, trigger_system_health_check/0\n]).\n\n%% gen_server callbacks\n-export([init/1, handle_call/3, handle_cast/2, handle_info/2,\n         terminate/2, code_change/3]).\n\n-include_lib(\"kernel/include/logger.hrl\").\n\n%% Types\n-type component_id() :: atom().\n-type health_status() :: healthy | unhealthy | degraded | unknown.\n-type health_check_fun() :: fun(() -> health_status() | {health_status(), term()}).\n-type health_config() :: #{\n    check_interval => pos_integer(),\n    timeout => pos_integer(),\n    max_consecutive_failures => pos_integer(),\n    recovery_check_interval => pos_integer()\n}.\n\n-record(component_health, {\n    id :: component_id(),\n    pid :: pid() | undefined,\n    status = unknown :: health_status(),\n    last_check :: undefined | erlang:timestamp(),\n    consecutive_failures = 0 :: non_neg_integer(),\n    total_checks = 0 :: non_neg_integer(),\n    successful_checks = 0 :: non_neg_integer(),\n    config :: health_config(),\n    check_fun :: health_check_fun() | undefined,\n    last_error :: term() | undefined,\n    circuit_breaker_active = false :: boolean(),\n    degraded = false :: boolean()\n}).\n\n-record(state, {\n    components = #{} :: #{component_id() => #component_health{}},\n    default_config :: health_config(),\n    system_health = unknown :: health_status(),\n    system_metrics = #{} :: map(),\n    health_timer :: undefined | timer:tref(),\n    alerts = [] :: list()\n}).\n\n-define(DEFAULT_CONFIG, #{\n    check_interval => 30000,          % 30 seconds\n    timeout => 5000,                  % 5 seconds\n    max_consecutive_failures => 3,    % 3 failures trigger unhealthy\n    recovery_check_interval => 10000  % 10 seconds for unhealthy components\n}).\n\n-define(SYSTEM_HEALTH_CHECK_INTERVAL, 15000). % 15 seconds\n-define(MEMORY_WARNING_THRESHOLD, 0.85).      % 85% memory usage warning\n-define(MEMORY_CRITICAL_THRESHOLD, 0.95).     % 95% memory usage critical\n\n%%====================================================================\n%% API Functions\n%%====================================================================\n\n-spec start_link() -> {ok, pid()} | {error, term()}.\nstart_link() ->\n    start_link([]).\n\n-spec start_link(list()) -> {ok, pid()} | {error, term()}.\nstart_link(Opts) ->\n    gen_server:start_link({local, ?MODULE}, ?MODULE, Opts, []).\n\n-spec register_component(component_id(), pid()) -> ok | {error, term()}.\nregister_component(ComponentId, Pid) ->\n    register_component(ComponentId, Pid, undefined).\n\n-spec register_component(component_id(), pid(), health_check_fun() | undefined) -> \n    ok | {error, term()}.\nregister_component(ComponentId, Pid, CheckFun) ->\n    gen_server:call(?MODULE, {register_component, ComponentId, Pid, CheckFun}).\n\n-spec unregister_component(component_id()) -> ok.\nunregister_component(ComponentId) ->\n    gen_server:cast(?MODULE, {unregister_component, ComponentId}).\n\n-spec get_system_health() -> map().\nget_system_health() ->\n    gen_server:call(?MODULE, get_system_health).\n\n-spec get_component_health(component_id()) -> health_status() | not_found.\nget_component_health(ComponentId) ->\n    gen_server:call(?MODULE, {get_component_health, ComponentId}).\n\n-spec get_all_component_health() -> map().\nget_all_component_health() ->\n    gen_server:call(?MODULE, get_all_component_health).\n\n-spec set_health_check_config(component_id(), health_config()) -> ok | {error, term()}.\nset_health_check_config(ComponentId, Config) ->\n    gen_server:call(?MODULE, {set_health_check_config, ComponentId, Config}).\n\n-spec get_health_check_config(component_id()) -> {ok, health_config()} | {error, not_found}.\nget_health_check_config(ComponentId) ->\n    gen_server:call(?MODULE, {get_health_check_config, ComponentId}).\n\n-spec report_circuit_breaker(component_id(), open | closed) -> ok.\nreport_circuit_breaker(ComponentId, State) ->\n    gen_server:cast(?MODULE, {report_circuit_breaker, ComponentId, State}).\n\n-spec report_degradation(component_id()) -> ok.\nreport_degradation(ComponentId) ->\n    gen_server:cast(?MODULE, {report_degradation, ComponentId}).\n\n-spec reset_health_status() -> ok.\nreset_health_status() ->\n    gen_server:cast(?MODULE, reset_health_status).\n\n-spec trigger_health_check(component_id()) -> ok | {error, term()}.\ntrigger_health_check(ComponentId) ->\n    gen_server:cast(?MODULE, {trigger_health_check, ComponentId}),\n    ok.\n\n-spec trigger_system_health_check() -> ok.\ntrigger_system_health_check() ->\n    gen_server:cast(?MODULE, trigger_system_health_check),\n    ok.\n\n%%====================================================================\n%% gen_server callbacks\n%%====================================================================\n\ninit(Opts) ->\n    ?LOG_INFO(\"Starting health monitor with options: ~p\", [Opts]),\n    \n    % Set up process monitoring\n    process_flag(trap_exit, true),\n    \n    DefaultConfig = maps:merge(?DEFAULT_CONFIG, \n                              proplists:get_value(default_config, Opts, #{})),\n    \n    % Start periodic system health checks\n    {ok, Timer} = timer:send_interval(?SYSTEM_HEALTH_CHECK_INTERVAL, system_health_check),\n    \n    State = #state{\n        default_config = DefaultConfig,\n        health_timer = Timer,\n        system_metrics = #{\n            memory_status => ok,\n            process_count => 0,\n            system_load => 0.0,\n            last_check => erlang:timestamp()\n        }\n    },\n    \n    ?LOG_INFO(\"Health monitor initialized\"),\n    {ok, State}.\n\nhandle_call({register_component, ComponentId, Pid, CheckFun}, _From, State) ->\n    ?LOG_INFO(\"Registering component ~p with PID ~p for health monitoring\", \n              [ComponentId, Pid]),\n    \n    % Monitor the component process\n    case Pid of\n        undefined -> ok;\n        _ -> erlang:monitor(process, Pid)\n    end,\n    \n    Component = #component_health{\n        id = ComponentId,\n        pid = Pid,\n        config = State#state.default_config,\n        check_fun = CheckFun\n    },\n    \n    NewComponents = maps:put(ComponentId, Component, State#state.components),\n    NewState = State#state{components = NewComponents},\n    \n    % Schedule initial health check\n    schedule_health_check(ComponentId, maps:get(check_interval, State#state.default_config)),\n    \n    {reply, ok, NewState};\n\nhandle_call(get_system_health, _From, State) ->\n    SystemHealth = generate_system_health_report(State),\n    {reply, SystemHealth, State};\n\nhandle_call({get_component_health, ComponentId}, _From, State) ->\n    case maps:find(ComponentId, State#state.components) of\n        {ok, Component} ->\n            {reply, Component#component_health.status, State};\n        error ->\n            {reply, not_found, State}\n    end;\n\nhandle_call(get_all_component_health, _From, State) ->\n    AllHealth = maps:map(fun(_Id, Component) ->\n        #{\n            status => Component#component_health.status,\n            last_check => Component#component_health.last_check,\n            consecutive_failures => Component#component_health.consecutive_failures,\n            total_checks => Component#component_health.total_checks,\n            successful_checks => Component#component_health.successful_checks,\n            circuit_breaker_active => Component#component_health.circuit_breaker_active,\n            degraded => Component#component_health.degraded\n        }\n    end, State#state.components),\n    {reply, AllHealth, State};\n\nhandle_call({set_health_check_config, ComponentId, Config}, _From, State) ->\n    case maps:find(ComponentId, State#state.components) of\n        {ok, Component} ->\n            MergedConfig = maps:merge(State#state.default_config, Config),\n            UpdatedComponent = Component#component_health{config = MergedConfig},\n            NewComponents = maps:put(ComponentId, UpdatedComponent, State#state.components),\n            NewState = State#state{components = NewComponents},\n            {reply, ok, NewState};\n        error ->\n            {reply, {error, not_found}, State}\n    end;\n\nhandle_call({get_health_check_config, ComponentId}, _From, State) ->\n    case maps:find(ComponentId, State#state.components) of\n        {ok, Component} ->\n            {reply, {ok, Component#component_health.config}, State};\n        error ->\n            {reply, {error, not_found}, State}\n    end;\n\nhandle_call(_Request, _From, State) ->\n    {reply, {error, unknown_request}, State}.\n\nhandle_cast({unregister_component, ComponentId}, State) ->\n    ?LOG_INFO(\"Unregistering component ~p from health monitoring\", [ComponentId]),\n    NewComponents = maps:remove(ComponentId, State#state.components),\n    NewState = State#state{components = NewComponents},\n    {noreply, NewState};\n\nhandle_cast({report_circuit_breaker, ComponentId, CircuitState}, State) ->\n    case maps:find(ComponentId, State#state.components) of\n        {ok, Component} ->\n            IsActive = CircuitState =:= open,\n            UpdatedComponent = Component#component_health{\n                circuit_breaker_active = IsActive\n            },\n            NewComponents = maps:put(ComponentId, UpdatedComponent, State#state.components),\n            NewState = State#state{components = NewComponents},\n            \n            ?LOG_INFO(\"Circuit breaker ~p for component ~p\", \n                     [CircuitState, ComponentId]),\n            {noreply, NewState};\n        error ->\n            ?LOG_WARNING(\"Circuit breaker report for unknown component ~p\", [ComponentId]),\n            {noreply, State}\n    end;\n\nhandle_cast({report_degradation, ComponentId}, State) ->\n    case maps:find(ComponentId, State#state.components) of\n        {ok, Component} ->\n            UpdatedComponent = Component#component_health{\n                degraded = true,\n                status = degraded\n            },\n            NewComponents = maps:put(ComponentId, UpdatedComponent, State#state.components),\n            NewState = State#state{components = NewComponents},\n            \n            ?LOG_INFO(\"Component ~p reported degraded\", [ComponentId]),\n            {noreply, NewState};\n        error ->\n            ?LOG_WARNING(\"Degradation report for unknown component ~p\", [ComponentId]),\n            {noreply, State}\n    end;\n\nhandle_cast(reset_health_status, State) ->\n    ?LOG_INFO(\"Resetting all health statuses\"),\n    ResetComponents = maps:map(fun(_Id, Component) ->\n        Component#component_health{\n            status = unknown,\n            consecutive_failures = 0,\n            last_error = undefined,\n            circuit_breaker_active = false,\n            degraded = false\n        }\n    end, State#state.components),\n    \n    NewState = State#state{\n        components = ResetComponents,\n        system_health = unknown,\n        alerts = []\n    },\n    {noreply, NewState};\n\nhandle_cast({trigger_health_check, ComponentId}, State) ->\n    NewState = perform_component_health_check(ComponentId, State),\n    {noreply, NewState};\n\nhandle_cast(trigger_system_health_check, State) ->\n    NewState = perform_system_health_check(State),\n    {noreply, NewState};\n\nhandle_cast(_Msg, State) ->\n    {noreply, State}.\n\nhandle_info({'DOWN', _Ref, process, Pid, Reason}, State) ->\n    ?LOG_WARNING(\"Monitored process ~p died with reason: ~p\", [Pid, Reason]),\n    case find_component_by_pid(Pid, State#state.components) of\n        {ok, ComponentId} ->\n            NewState = handle_component_process_death(ComponentId, Reason, State),\n            {noreply, NewState};\n        error ->\n            ?LOG_WARNING(\"Unknown monitored process ~p died\", [Pid]),\n            {noreply, State}\n    end;\n\nhandle_info({health_check, ComponentId}, State) ->\n    NewState = perform_component_health_check(ComponentId, State),\n    \n    % Schedule next health check\n    case maps:find(ComponentId, NewState#state.components) of\n        {ok, Component} ->\n            CheckInterval = determine_check_interval(Component),\n            schedule_health_check(ComponentId, CheckInterval);\n        error ->\n            ok % Component was removed\n    end,\n    \n    {noreply, NewState};\n\nhandle_info(system_health_check, State) ->\n    NewState = perform_system_health_check(State),\n    {noreply, NewState};\n\nhandle_info(Info, State) ->\n    ?LOG_DEBUG(\"Unexpected info: ~p\", [Info]),\n    {noreply, State}.\n\nterminate(Reason, State) ->\n    ?LOG_INFO(\"Health monitor terminating: ~p\", [Reason]),\n    \n    % Cancel health timer\n    case State#state.health_timer of\n        undefined -> ok;\n        Timer -> timer:cancel(Timer)\n    end,\n    \n    ok.\n\ncode_change(_OldVsn, State, _Extra) ->\n    {ok, State}.\n\n%%====================================================================\n%% Internal Functions\n%%====================================================================\n\n-spec perform_component_health_check(component_id(), #state{}) -> #state{}.\nperform_component_health_check(ComponentId, State) ->\n    case maps:find(ComponentId, State#state.components) of\n        {ok, Component} ->\n            ?LOG_DEBUG(\"Performing health check for component ~p\", [ComponentId]),\n            \n            CheckStart = erlang:timestamp(),\n            CheckResult = execute_health_check(Component),\n            CheckDuration = timer:now_diff(erlang:timestamp(), CheckStart) div 1000,\n            \n            UpdatedComponent = update_component_health_status(\n                Component, CheckResult, CheckDuration\n            ),\n            \n            NewComponents = maps:put(ComponentId, UpdatedComponent, State#state.components),\n            \n            % Trigger recovery if needed\n            maybe_trigger_recovery(UpdatedComponent),\n            \n            State#state{components = NewComponents};\n        error ->\n            ?LOG_WARNING(\"Health check requested for unknown component ~p\", [ComponentId]),\n            State\n    end.\n\n-spec execute_health_check(#component_health{}) -> \n    {health_status(), term()} | health_status().\nexecute_health_check(Component) ->\n    Config = Component#component_health.config,\n    Timeout = maps:get(timeout, Config, 5000),\n    \n    try\n        case Component#component_health.check_fun of\n            undefined ->\n                % Default health check - check if process is alive\n                basic_process_health_check(Component);\n            CheckFun when is_function(CheckFun, 0) ->\n                % Execute custom health check with timeout\n                execute_with_timeout(CheckFun, Timeout);\n            _ ->\n                {unhealthy, invalid_check_function}\n        end\n    catch\n        Class:Exception:Stacktrace ->\n            ?LOG_ERROR(\"Health check failed for ~p: ~p:~p~n~p\", \n                      [Component#component_health.id, Class, Exception, Stacktrace]),\n            {unhealthy, {exception, {Class, Exception}}}\n    end.\n\n-spec basic_process_health_check(#component_health{}) -> health_status().\nbasic_process_health_check(Component) ->\n    case Component#component_health.pid of\n        undefined -> \n            unknown;\n        Pid ->\n            case is_process_alive(Pid) of\n                true -> healthy;\n                false -> unhealthy\n            end\n    end.\n\n-spec execute_with_timeout(fun(() -> term()), pos_integer()) -> \n    {health_status(), term()} | health_status().\nexecute_with_timeout(Fun, Timeout) ->\n    Parent = self(),\n    Ref = make_ref(),\n    \n    Pid = spawn(fun() ->\n        Result = Fun(),\n        Parent ! {Ref, Result}\n    end),\n    \n    receive\n        {Ref, Result} -> Result\n    after Timeout ->\n        exit(Pid, kill),\n        {unhealthy, timeout}\n    end.\n\n-spec update_component_health_status(#component_health{}, \n    {health_status(), term()} | health_status(), pos_integer()) -> #component_health{}.\nupdate_component_health_status(Component, CheckResult, _CheckDuration) ->\n    {NewStatus, LastError} = case CheckResult of\n        {Status, Error} -> {Status, Error};\n        Status when is_atom(Status) -> {Status, undefined}\n    end,\n    \n    TotalChecks = Component#component_health.total_checks + 1,\n    \n    {NewConsecutiveFailures, NewSuccessfulChecks} = case NewStatus of\n        healthy ->\n            {0, Component#component_health.successful_checks + 1};\n        _ ->\n            {Component#component_health.consecutive_failures + 1, \n             Component#component_health.successful_checks}\n    end,\n    \n    % Determine final status based on consecutive failures\n    FinalStatus = determine_final_health_status(NewStatus, NewConsecutiveFailures, Component),\n    \n    Component#component_health{\n        status = FinalStatus,\n        last_check = erlang:timestamp(),\n        consecutive_failures = NewConsecutiveFailures,\n        total_checks = TotalChecks,\n        successful_checks = NewSuccessfulChecks,\n        last_error = LastError\n    }.\n\n-spec determine_final_health_status(health_status(), non_neg_integer(), #component_health{}) -> \n    health_status().\ndetermine_final_health_status(CheckStatus, ConsecutiveFailures, Component) ->\n    MaxFailures = maps:get(max_consecutive_failures, Component#component_health.config, 3),\n    \n    case {CheckStatus, ConsecutiveFailures >= MaxFailures} of\n        {healthy, _} -> healthy;\n        {degraded, _} -> degraded;\n        {_, true} -> unhealthy;\n        {unhealthy, false} -> unhealthy;\n        {unknown, false} -> unknown\n    end.\n\n-spec determine_check_interval(#component_health{}) -> pos_integer().\ndetermine_check_interval(Component) ->\n    Config = Component#component_health.config,\n    case Component#component_health.status of\n        unhealthy ->\n            maps:get(recovery_check_interval, Config, 10000);\n        _ ->\n            maps:get(check_interval, Config, 30000)\n    end.\n\n-spec maybe_trigger_recovery(#component_health{}) -> ok.\nmaybe_trigger_recovery(Component) ->\n    case Component#component_health.status of\n        unhealthy ->\n            % Trigger recovery through recovery manager\n            case Component#component_health.consecutive_failures >= 3 of\n                true ->\n                    ?LOG_WARNING(\"Component ~p is unhealthy, triggering recovery\", \n                                [Component#component_health.id]),\n                    erlmcp_recovery_manager:trigger_recovery(\n                        Component#component_health.id, \n                        health_check_failure\n                    );\n                false ->\n                    ok\n            end;\n        _ ->\n            ok\n    end.\n\n-spec handle_component_process_death(component_id(), term(), #state{}) -> #state{}.\nhandle_component_process_death(ComponentId, Reason, State) ->\n    case maps:find(ComponentId, State#state.components) of\n        {ok, Component} ->\n            UpdatedComponent = Component#component_health{\n                status = unhealthy,\n                pid = undefined,\n                last_error = {process_death, Reason},\n                consecutive_failures = Component#component_health.consecutive_failures + 1\n            },\n            \n            NewComponents = maps:put(ComponentId, UpdatedComponent, State#state.components),\n            \n            % Trigger recovery\n            erlmcp_recovery_manager:trigger_recovery(ComponentId, {process_death, Reason}),\n            \n            State#state{components = NewComponents};\n        error ->\n            State\n    end.\n\n-spec perform_system_health_check(#state{}) -> #state{}.\nperform_system_health_check(State) ->\n    ?LOG_DEBUG(\"Performing system health check\"),\n    \n    % Collect system metrics\n    SystemMetrics = collect_system_metrics(),\n    \n    % Check component health summary\n    ComponentHealthSummary = analyze_component_health(State#state.components),\n    \n    % Determine overall system health\n    SystemHealth = determine_system_health(SystemMetrics, ComponentHealthSummary),\n    \n    % Generate alerts if needed\n    Alerts = generate_health_alerts(SystemMetrics, ComponentHealthSummary, State#state.alerts),\n    \n    ?LOG_INFO(\"System health check completed: ~p\", [SystemHealth]),\n    \n    State#state{\n        system_health = SystemHealth,\n        system_metrics = SystemMetrics,\n        alerts = Alerts\n    }.\n\n-spec collect_system_metrics() -> map().\ncollect_system_metrics() ->\n    % Memory usage\n    MemoryTotal = erlang:memory(total),\n    MemoryProcesses = erlang:memory(processes),\n    MemoryUsage = MemoryProcesses / MemoryTotal,\n    \n    MemoryStatus = if\n        MemoryUsage > ?MEMORY_CRITICAL_THRESHOLD -> critical;\n        MemoryUsage > ?MEMORY_WARNING_THRESHOLD -> warning;\n        true -> ok\n    end,\n    \n    % Process count\n    ProcessCount = erlang:system_info(process_count),\n    ProcessLimit = erlang:system_info(process_limit),\n    ProcessUsage = ProcessCount / ProcessLimit,\n    \n    ProcessStatus = if\n        ProcessUsage > 0.9 -> critical;\n        ProcessUsage > 0.8 -> warning;\n        true -> ok\n    end,\n    \n    % System load (simplified)\n    {ReductionCount, _} = statistics(reductions),\n    \n    #{\n        memory_total => MemoryTotal,\n        memory_processes => MemoryProcesses,\n        memory_usage => MemoryUsage,\n        memory_status => MemoryStatus,\n        process_count => ProcessCount,\n        process_limit => ProcessLimit,\n        process_usage => ProcessUsage,\n        process_status => ProcessStatus,\n        reduction_count => ReductionCount,\n        last_check => erlang:timestamp()\n    }.\n\n-spec analyze_component_health(#{component_id() => #component_health{}}) -> map().\nanalyze_component_health(Components) ->\n    ComponentList = maps:values(Components),\n    Total = length(ComponentList),\n    \n    StatusCounts = lists:foldl(fun(Component, Acc) ->\n        Status = Component#component_health.status,\n        maps:update_with(Status, fun(Count) -> Count + 1 end, 1, Acc)\n    end, #{}, ComponentList),\n    \n    Healthy = maps:get(healthy, StatusCounts, 0),\n    Unhealthy = maps:get(unhealthy, StatusCounts, 0),\n    Degraded = maps:get(degraded, StatusCounts, 0),\n    Unknown = maps:get(unknown, StatusCounts, 0),\n    \n    HealthyPercentage = case Total of\n        0 -> 1.0;\n        _ -> Healthy / Total\n    end,\n    \n    #{\n        total_components => Total,\n        healthy_count => Healthy,\n        unhealthy_count => Unhealthy,\n        degraded_count => Degraded,\n        unknown_count => Unknown,\n        healthy_percentage => HealthyPercentage,\n        status_counts => StatusCounts\n    }.\n\n-spec determine_system_health(map(), map()) -> health_status().\ndetermine_system_health(SystemMetrics, ComponentHealth) ->\n    % Check memory status\n    MemoryStatus = maps:get(memory_status, SystemMetrics),\n    ProcessStatus = maps:get(process_status, SystemMetrics),\n    \n    % Check component health\n    HealthyPercentage = maps:get(healthy_percentage, ComponentHealth),\n    UnhealthyCount = maps:get(unhealthy_count, ComponentHealth),\n    \n    case {MemoryStatus, ProcessStatus, HealthyPercentage, UnhealthyCount} of\n        {critical, _, _, _} -> unhealthy;\n        {_, critical, _, _} -> unhealthy;\n        {_, _, Percentage, _} when Percentage < 0.5 -> unhealthy;\n        {_, _, _, Count} when Count > 3 -> unhealthy;\n        {warning, _, _, _} -> degraded;\n        {_, warning, _, _} -> degraded;\n        {_, _, Percentage, _} when Percentage < 0.8 -> degraded;\n        {_, _, _, Count} when Count > 0 -> degraded;\n        _ -> healthy\n    end.\n\n-spec generate_health_alerts(map(), map(), list()) -> list().\ngenerate_health_alerts(SystemMetrics, ComponentHealth, _ExistingAlerts) ->\n    Alerts = [],\n    \n    % Memory alerts\n    MemoryAlerts = case maps:get(memory_status, SystemMetrics) of\n        critical -> [{memory_critical, erlang:timestamp()}];\n        warning -> [{memory_warning, erlang:timestamp()}];\n        ok -> []\n    end,\n    \n    % Process alerts\n    ProcessAlerts = case maps:get(process_status, SystemMetrics) of\n        critical -> [{process_limit_critical, erlang:timestamp()}];\n        warning -> [{process_limit_warning, erlang:timestamp()}];\n        ok -> []\n    end,\n    \n    % Component health alerts\n    UnhealthyCount = maps:get(unhealthy_count, ComponentHealth),\n    ComponentAlerts = case UnhealthyCount of\n        Count when Count > 3 -> [{multiple_components_unhealthy, Count, erlang:timestamp()}];\n        Count when Count > 0 -> [{components_unhealthy, Count, erlang:timestamp()}];\n        0 -> []\n    end,\n    \n    lists:flatten([Alerts, MemoryAlerts, ProcessAlerts, ComponentAlerts]).\n\n-spec generate_system_health_report(#state{}) -> map().\ngenerate_system_health_report(State) ->\n    #{\n        overall_status => State#state.system_health,\n        system_metrics => State#state.system_metrics,\n        component_health => analyze_component_health(State#state.components),\n        active_alerts => State#state.alerts,\n        last_check => erlang:timestamp()\n    }.\n\n-spec find_component_by_pid(pid(), #{component_id() => #component_health{}}) -> \n    {ok, component_id()} | error.\nfind_component_by_pid(Pid, Components) ->\n    case maps:to_list(maps:filter(fun(_Id, Component) -> \n        Component#component_health.pid =:= Pid \n    end, Components)) of\n        [{ComponentId, _Component}] -> {ok, ComponentId};\n        [] -> error;\n        Multiple -> \n            ?LOG_WARNING(\"Multiple components with same PID: ~p\", [Multiple]),\n            {ok, element(1, hd(Multiple))}\n    end.\n\n-spec schedule_health_check(component_id(), pos_integer()) -> ok.\nschedule_health_check(ComponentId, Interval) ->\n    erlang:send_after(Interval, self(), {health_check, ComponentId}),\n    ok."
        }
    ]
}