# erlmcp v3 High-Performance Cluster Architecture

## Overview

This document describes the high-performance cluster deployment architecture for erlmcp v3, designed for scalability, low latency, and resilience.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          GLOBAL LOAD BALANCER                      â”‚
â”‚                           (HAProxy/NGINX)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          REGION A (us-east-1)          â”‚           REGION B (eu-west-1)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        CLUSTER GATEWAY           â”‚   â”‚        CLUSTER GATEWAY           â”‚   â”‚
â”‚  â”‚   (Kubernetes Ingress)           â”‚   â”‚   (Kubernetes Ingress)           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                â”‚                                     â”‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                   CONNECTION POOL MANAGER                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                â”‚                     â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  erlmcp_node_1        â”‚  â”‚  erlmcp_node_2   â”‚  â”‚  erlmcp_node_3       â”‚       â”‚
â”‚  â”‚  â€¢ Connection Pool    â”‚  â”‚  â€¢ Connection Poolâ”‚  â”‚  â€¢ Connection Pool  â”‚       â”‚
â”‚  â”‚  â€¢ Shard Manager      â”‚  â”‚  â€¢ Shard Manager  â”‚  â”‚  â€¢ Shard Manager    â”‚       â”‚
â”‚  â”‚  â€¢ Cache Client       â”‚  â”‚  â€¢ Cache Client  â”‚  â”‚  â€¢ Cache Client     â”‚       â”‚
â”‚  â”‚  â€¢ Metrics Exporter   â”‚  â”‚  â€¢ Metrics Exporterâ”‚  â”‚  â€¢ Metrics Exporter â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                          SHARED SERVICES                                   â”‚   â”‚
â”‚  â”‚  â€¢ Redis Cluster (Sentinel)                                              â”‚   â”‚
â”‚  â”‚  â€¢ PostgreSQL Cluster (Patroni)                                           â”‚
â”‚  â”‚  â€¢ Circuit Breaker Monitor                                               â”‚   â”‚
â”‚  â”‚  â€¢ Chaos Engineering Controller                                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Components

### 1. Connection Pool Management

```yaml
# config/pool.yaml
connection_pools:
  tcp_pool:
    size: 1000
    max_overflow: 500
    timeout: 30000
    keep_alive: 60000
    health_check_interval: 5000
    retry_policy:
      max_attempts: 3
      backoff: exponential
      base_delay: 100
    circuit_breaker:
      failure_threshold: 50
      recovery_timeout: 30000
      expected_exception: all

  http_pool:
    max_connections: 500
    max_requests_per_connection: 100
    keep_alive: 30000
    connection_timeout: 10000
    read_timeout: 30000
    pool_recycle: 3600000

  websocket_pool:
    max_connections: 2000
    max_size: 10000
    connection_timeout: 10000
    ping_interval: 30000
    ping_timeout: 5000
    queue_size: 1000
```

### 2. Load Balancing Configuration

```yaml
# config/load-balancer.yaml
load_balancer:
  strategy: "least_connections"
  health_check:
    path: "/health"
    interval: 5s
    timeout: 3s
    healthy_threshold: 3
    unhealthy_threshold: 3
    expected_status: 200

  nodes:
    - host: "erlmcp-node-1"
      port: 8080
      weight: 100
      max_connections: 10000
      status: "active"
    - host: "erlmcp-node-2"
      port: 8080
      weight: 100
      max_connections: 10000
      status: "active"
    - host: "erlmcp-node-3"
      port: 8080
      weight: 100
      max_connections: 10000
      status: "active"

  sticky_sessions: false
  session_timeout: 86400
  graceful_shutdown:
    timeout: 30s
    drain_connections: true
```

### 3. Sharding Strategy

```yaml
# config/sharding.yaml
sharding:
  strategy: "consistent_hashing"
  virtual_nodes: 160
  hash_ring_size: 32768

  shards:
    - id: "shard-1"
      nodes: ["node-1", "node-2", "node-3"]
      replicas: 2
      load_threshold: 0.8
      rebalance_threshold: 0.9

    - id: "shard-2"
      nodes: ["node-2", "node-3", "node-4"]
      replicas: 2
      load_threshold: 0.8
      rebalance_threshold: 0.9

    - id: "shard-3"
      nodes: ["node-3", "node-4", "node-1"]
      replicas: 2
      load_threshold: 0.8
      rebalance_threshold: 0.9

  routing:
    method: "dynamic"
    failover_mode: "priority"
    migration_timeout: 30000
    consistency_level: "eventual"
```

### 4. Caching Layer Configuration

```yaml
# config/cache.yaml
cache:
  redis:
    cluster:
      nodes:
        - host: "redis-node-1"
          port: 6379
          db: 0
        - host: "redis-node-2"
          port: 6379
          db: 0
        - host: "redis-node-3"
          port: 6379
          db: 0
      cluster_mode: true
      cluster_nodes: 3
      replicas: 1
      timeout: 3000
      max_retries: 3
      reconnect_delay: 1

    local_cache:
      size: 1000
      ttl: 60000
      eviction_policy: "allkeys-lru"
      max_memory: 256MB

    pattern:
      write_through: true
      read_through: true
      cache_miss_penalty: 100
      batch_size: 100

  memcached:
    servers:
      - "memcached-1:11211"
      - "memcached-2:11211"
      - "memcached-3:11211"

    configuration:
      timeout: 1000
      max_connections: 100
      ketama: true

    hot_keys:
      pattern: "^session:|^cache:|^metrics:"
      ttl: 3600
```

### 5. Database Replication and Failover

```yaml
# config/database.yaml
database:
  postgresql:
    primary:
      host: "postgres-primary"
      port: 5432
      database: "erlmcp"
      username: "erlmcp_user"
      password: "${DB_PASSWORD}"
      pool_size: 50
      max_connections: 200
      timeout: 5000

    replicas:
      - host: "postgres-replica-1"
        port: 5432
        priority: 1
        backup: true
      - host: "postgres-replica-2"
        port: 5432
        priority: 2
        backup: true
      - host: "postgres-replica-3"
        port: 5432
        priority: 3
        backup: true

    replication:
      synchronous: false
      commit_timeout: 5000
      replication_slots: 10
      wal_level: "replica"
      max_wal_senders: 5

    failover:
      manager: "patroni"
      health_check_interval: 5s
      switchover_timeout: 30s
      auto_failover: true
      cluster_check_interval: 10s

    connection_pooler:
      type: "pgbouncer"
      mode: "transaction"
      pool_size: 100
      max_connections: 1000
      ignore_startup_parameters: ["application_name"]
```

### 6. Network Topology Optimization

```yaml
# config/network.yaml
network:
  topology:
    # Multi-region configuration
    regions:
      - name: "us-east-1"
        latency_ms: 0
        priority: 1
        availability_zone: "us-east-1a"
      - name: "eu-west-1"
        latency_ms: 50
        priority: 2
        availability_zone: "eu-west-1a"
      - name: "ap-southeast-1"
        latency_ms: 150
        priority: 3
        availability_zone: "ap-southeast-1a"

    # Latency-based routing
    latency_thresholds:
      fast: 10    # ms
      medium: 50  # ms
      slow: 100   # ms

    # Bandwidth allocation
    bandwidth_allocation:
      erlmcp_core: 50    # Mbps
      transports: 30     # Mbps
      observability: 20  # Mbps

  tls:
    enabled: true
    cipher_suites:
      - "TLS_AES_256_GCM_SHA384"
      - "TLS_AES_128_GCM_SHA256"
      - "TLS_CHACHA20_POLY1305_SHA256"
    protocol_versions: ["TLSv1.2", "TLSv1.3"]
    ocsp_stapling: true
    session_resumption: true

  compression:
    enabled: true
    algorithms: ["gzip", "brotli"]
    level: 6
    min_size: 1024
```

### 7. Circuit Breakers and Resilience

```yaml
# config/resilience.yaml
resilience:
  circuit_breakers:
    default:
      failure_threshold: 50
      recovery_timeout: 30000
      half_open_requests: 10
      expected_exception: "all"
      exclude_exceptions: ["timeout"]

    service_specific:
      erlmcp_core:
        failure_threshold: 30
        recovery_timeout: 15000
        half_open_requests: 5

      transport_tcp:
        failure_threshold: 70
        recovery_timeout: 45000

      database:
        failure_threshold: 20
        recovery_timeout: 60000

  retry:
    max_attempts: 3
    initial_delay: 100
    max_delay: 10000
    backoff_multiplier: 2
    jitter: true
    retryable_exceptions: ["timeout", "connection_error"]

  bulkheads:
    max_concurrent_requests: 100
    max_wait_time: 5000
    queue_capacity: 1000

  timeouts:
    connect: 5000
    read: 30000
    write: 30000
    idle: 30000
```

### 8. Resource Monitoring and Autoscaling

```yaml
# config/monitoring.yaml
monitoring:
  metrics:
    collection_interval: 5s
    retention_period: 7d
    export:
      prometheus:
        enabled: true
        port: 9090
        path: "/metrics"
      opentelemetry:
        enabled: true
        endpoint: "http://otel-collector:4317"

  resource_usage:
    cpu_thresholds:
      warning: 70
      critical: 85
      alert: 95

    memory_thresholds:
      warning: 75
      critical: 85
      alert: 95

    disk_thresholds:
      warning: 80
      critical: 90
      alert: 95

  autoscaling:
    enabled: true
    min_replicas: 3
    max_replicas: 15
    target_cpu_utilization: 70
    target_memory_utilization: 70
    scale_up_threshold: 80
    scale_down_threshold: 30
    stabilization_window: 300

    scaling_policy:
      metrics:
        - type: "resource"
          resource:
            name: "cpu"
            target:
              type: "Utilization"
              average_utilization: 70
      behavior:
        scale_up:
          policies:
            - type: "Percent"
              value: 100
              period_seconds: 60
        scale_down:
          policies:
            - type: "Percent"
              value: 100
              period_seconds: 300
```

### 9. Multi-Region Deployment

```yaml
# config/multi-region.yaml
multi_region:
  deployment:
    regions:
      - name: "primary"
        location: "us-east-1"
        priority: 1
        active: true
        capacity: 100

      - name: "secondary"
        location: "eu-west-1"
        priority: 2
        active: true
        capacity: 50

      - name: "tertiary"
        location: "ap-southeast-1"
        priority: 3
        active: false
        capacity: 25

  failover:
    automatic_failover: true
    health_check_interval: 10s
    failover_timeout: 30s
    manual_failover_required: false

    data_sync:
      method: "async"
      consistency_level: "eventual"
      sync_interval: 100ms
      conflict_resolution: "last_write_wins"

    traffic_routing:
      primary_weight: 100
      secondary_weight: 0
      tertiary_weight: 0

    failover_weights:
      primary_to_secondary: 80
      primary_to_tertiary: 20
```

### 10. Chaos Engineering Configuration

```yaml
# config/chaos.yaml
chaos:
  experiments:
    - name: "network-latency"
      enabled: true
      frequency: "0 2 * * *"
      duration: "30m"
      intensity: 0.3
      scope: "region"

      failure_scenarios:
        - type: "latency"
          target: "inter-region"
          distribution: "normal"
          mean_ms: 500
          stddev_ms: 100
          probability: 0.5

        - type: "packet_loss"
          target: "inter-region"
          percentage: 10
          correlation: true

    - name: "cpu-pressure"
      enabled: true
      frequency: "0 6 * * *"
      duration: "20m"
      intensity: 0.5
      scope: "node"

      failure_scenarios:
        - type: "cpu_spike"
          target: "all"
          percentage: 95
          duration: "10m"

        - type: "memory_pressure"
          target: "all"
          usage_percentage: 90

    - name: "service-failure"
      enabled: true
      frequency: "0 10 * * *"
      duration: "15m"
      intensity: 0.2
      scope: "service"

      failure_scenarios:
        - type: "pod-kill"
          target: "random"
          probability: 0.1

        - type: "pod-restart"
          target: "random"
          probability: 0.05

  fault_injection:
    delays:
      - name: "external-api-delay"
        target: "http"
        source: "erlmcp_core"
        distribution: "normal"
        mean_ms: 100
        stddev_ms: 50
        percentage: 20

    errors:
      - name: "database-connection-error"
        target: "database"
        source: "erlmcp_core"
        error_type: "connection_error"
        percentage: 5

    bandwidth:
      - name: "bandwidth-throttling"
        target: "network"
        source: "all"
        bandwidth_mbps: 10
        duration: "5m"

  monitoring:
    metrics:
      response_time_increase: 0.5  # 50% increase
      error_rate_increase: 0.3    # 30% increase
      availability_decrease: 0.01  # 1% decrease

    notifications:
      slack_webhook: "${SLACK_WEBHOOK}"
      pagerduty_service: "erlmcp-chaos"

    recovery_time:
      mttr: 600  # 10 minutes maximum
      rpo: 300   # 5 minutes RPO
      rto: 60    # 1 minute RTO
```

## Performance Optimization Settings

### Erlang VM Tuning

```erlang
% config/vm.config
[
  {kernel, [
    {inet_dist_listen_min, 9100},
    {inet_dist_listen_max, 9109},
    {net_ticktime, 60},
    {dist_auto_connect, once},
    {scheduler_width, 2},
    {num_schedulers_online, 8},
    {fullsweep_after, 10}
  ]},

  {erl_distribution, [
    {max_ports, 65536},
    {dist_buffer_size, 8192},
    {dist_node_name, "erlmcp@$(hostname)"},
    {dist_net_ticktime, 60}
  ]},

  {sasl, [
    {sasl_error_logger, {file, "/var/log/erlmcp/sasl.log"}},
    {errlog_type, error},
    {error_logger_mf_dir, "/var/log/erlmcp"},
    {error_logger_mf_maxbytes, 10485760},
    {error_logger_mf_maxfiles, 5}
  ]}
]
```

### TCP/Network Configuration

```bash
# sysctl.conf optimization for high-performance networking
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.ipv4.tcp_rmem = 4096 87380 134217728
net.ipv4.tcp_wmem = 4096 65536 134217728
net.ipv4.tcp_congestion_control = bbr
net.ipv4.tcp_mtu_probing = 1
net.ipv4.tcp_ecn = 1
net.core.netdev_max_backlog = 10000
net.ipv4.tcp_max_syn_backlog = 10240
net.ipv4.tcp_fin_timeout = 10
net.core.somaxconn = 65535
net.ipv4.tcp_timestamps = 1
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_recycle = 1
```

### Kubernetes Resource Limits

```yaml
# k8s/resources.yaml
resources:
  limits:
    cpu: "4000m"
    memory: "8Gi"
    ephemeral-storage: "40Gi"
  requests:
    cpu: "2000m"
    memory: "4Gi"
    ephemeral-storage: "20Gi"

  liveness_probe:
    http_get:
      path: /health
      port: 8080
    initial_delay_seconds: 30
    period_seconds: 10
    timeout_seconds: 5
    failure_threshold: 3

  readiness_probe:
    http_get:
      path: /ready
      port: 8080
    initial_delay_seconds: 5
    period_seconds: 5
    timeout_seconds: 3
    failure_threshold: 3

  startup_probe:
    http_get:
      path: /startup
      port: 8080
    initial_delay_seconds: 60
    period_seconds: 10
    timeout_seconds: 5
    failure_threshold: 10
```

## Deployment Scripts

```bash
#!/bin/bash
# scripts/deploy-cluster.sh

# High-performance cluster deployment script for erlmcp v3
set -e

# Configuration
CLUSTER_NAME="erlmcp-cluster-3"
REGION="us-east-1"
NODE_COUNT=9
REPLICAS=3

echo "ðŸš€ Starting erlmcp v3 cluster deployment..."

# 1. Initialize Kubernetes cluster
echo "Initializing Kubernetes cluster..."
kops create cluster \
  --name=${CLUSTER_NAME} \
  --state=s3://erlmcp-kops-state \
  --zones=${REGION}a,${REGION}b,${REGION}c \
  --master-size=t3.large \
  --node-size=t3.xlarge \
  --node-count=${NODE_COUNT} \
  --kubernetes-version=1.28.0 \
  --networking=calico \
  --topology=public \
  --yes

# 2. Install monitoring stack
echo "Installing monitoring stack..."
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update

# Install Prometheus
helm install prometheus prometheus-community/kube-prometheus-stack \
  --set prometheus.prometheusSpec.retention=30d \
  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi \
  --set prometheus.prometheusSpec.resources.requests.memory=2Gi \
  --set prometheus.prometheusSpec.resources.requests.cpu=1 \
  --set prometheus.prometheusSpec.resources.limits.memory=4Gi \
  --set prometheus.prometheusSpec.resources.limits.cpu=2

# Install Grafana
helm install grafana grafana/grafana \
  --set persistence.enabled=true \
  --set persistence.size=20Gi \
  --set adminPassword="grafanaadmin" \
  --set resources.requests.memory=1Gi \
  --set resources.requests.cpu=0.5 \
  --set resources.limits.memory=2Gi \
  --set resources.limits.cpu=1

# 3. Install Redis cluster
echo "Installing Redis cluster..."
helm install redis bitnami/redis-cluster \
  --set architecture=cluster \
  --set replica.replicaCount=${REPLICAS} \
  --set master.persistence.size=50Gi \
  --set replica.persistence.size=50Gi \
  --set redis.resources.requests.memory=2Gi \
  --set redis.resources.requests.cpu=1 \
  --set redis.resources.limits.memory=4Gi \
  --set redis.resources.limits.cpu=2

# 4. Install PostgreSQL cluster
echo "Installing PostgreSQL cluster..."
helm install postgres bitnami/postgresql \
  --set global.postgresql.auth.password="${DB_PASSWORD}" \
  --set global.postgresql.auth.username="erlmcp_user" \
  --set global.postgresql.auth.database="erlmcp" \
  --set global.postgresql.replicaCount=${REPLICAS} \
  --set primary.persistence.size=100Gi \
  --set replica.persistence.size=100Gi \
  --set postgresql.resources.requests.memory=2Gi \
  --set postgresql.resources.requests.cpu=1 \
  --set postgresql.resources.limits.memory=4Gi \
  --set postgresql.resources.limits.cpu=2

# 5. Install monitoring agents
echo "Installing monitoring agents..."
kubectl apply -f manifests/erlmcp-metrics.yaml

# 6. Configure Load Balancer
echo "Configuring HAProxy load balancer..."
kubectl apply -f manifests/haproxy-configmap.yaml
kubectl apply -f manifests/haproxy-deployment.yaml

# 7. Deploy erlmcp nodes
echo "Deploying erlmcp nodes..."
kubectl apply -f manifests/erlmcp-node.yaml

# 8. Configure network policies
echo "Configuring network policies..."
kubectl apply -f manifests/network-policies.yaml

# 9. Install chaos engineering tools
echo "Installing chaos engineering tools..."
helm install chaos-mesh chaos-mesh/chaos-mesh \
  --set chaosDaemon.runtime=containerd \
  --set chaosDaemon.hostNetwork=true

# 10. Configure autoscaling
echo "Configuring autoscaling..."
kubectl apply -f manifests/hpa.yaml

echo "âœ… Cluster deployment completed!"
echo "ðŸ“Š Access Grafana at: http://$(kubectl get ingress grafana -o jsonpath='{.spec.rules[0].host}')"
echo "ðŸ“ˆ Access Prometheus at: http://$(kubectl get ingress prometheus -o jsonpath='{.spec.rules[0].host}')"
```

```bash
#!/bin/bash
# scripts/optimize-network.sh

# Network optimization script for erlmcp v3 cluster
echo "ðŸ”§ Optimizing network configuration..."

# Apply sysctl optimizations
sysctl -p

# Configure TCP BBR congestion control
echo "net.ipv4.tcp_congestion_control = bbr" | tee -a /etc/sysctl.conf
sysctl -p

# Configure kernel parameters for high-performance
cat > /etc/security/limits.conf << EOF
* soft nofile 65536
* hard nofile 65536
* soft nproc unlimited
* hard nproc unlimited
* soft memlock unlimited
* hard memlock unlimited
EOF

# Configure cgroups limits
cat > /etc/cgconfig.conf << EOF
mount {
    cpuset  = /cgroup/cpuset;
    cpu     = /cgroup/cpu;
    cpuacct = /cgroup/cpuacct;
    memory  = /cgroup/memory;
    devices = /cgroup/devices;
    net_cls  = /cgroup/net_cls;
    freezer = /cgroup/freezer;
    blkio   = /cgroup/blkio;
}

group erlmcp {
    cpuset {
        cpus = 0-7;
        mems = 0;
    }
    cpu {
        cpu.shares = 1024;
    }
    memory {
        memory.limit_in_bytes = 8G;
    }
    blkio {
        blkio.weight = 1000;
    }
}
EOF

# Apply cgroup configuration
cgconfigparser -l /etc/cgconfig.conf

echo "âœ… Network optimization completed!"
```

```bash
#!/bin/bash
# scripts/start-chaos-experiment.sh

# Chaos engineering experiment script
EXPERIMENT_NAME=${1:-"network-latency"}
DURATION=${2:-"30m"}
INTENSITY=${3:-"0.3"}

echo "ðŸŒªï¸ Starting chaos experiment: ${EXPERIMENT_NAME}"

# Apply chaos experiment configuration
cat > /tmp/chaos-experiment.yaml << EOF
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: ${EXPERIMENT_NAME}
  namespace: default
spec:
  action: delay
  delay:
    latency: "${INTENSITY}s"
    correlation: "0.8"
  selector:
    namespaces:
      - default
  mode: "one"
  duration: "${DURATION}"
  scheduler:
    cron: "*/5 * * * *"
EOF

kubectl apply -f /tmp/chaos-experiment.yaml

echo "âœ… Chaos experiment started: ${EXPERIMENT_NAME}"
echo "ðŸ“Š Monitor chaos experiment: kubectl get chaos --watch"
echo "ðŸ›‘ Stop experiment: kubectl delete chaos ${EXPERIMENT_NAME}"
```

## Health Check and Monitoring Dashboard

```yaml
# manifests/health-check.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: erlmcp-health-check
data:
  health-check.sh: |
    #!/bin/bash
    while true; do
      # Check erlmcp service health
      response_code=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8080/health)

      # Check database connectivity
      db_check=$(pg_isready -h postgres-primary -p 5432 -U erlmcp_user 2>/dev/null)

      # Check Redis connectivity
      redis_check=$(redis-cli -h redis-cluster-clusterip 6379 ping 2>/dev/null)

      # Report metrics
      echo "erlmcp_health $response_code $(date +%s)" | curl -X POST --data-binary @- http://prometheus-pushgateway:9091/metrics/job/erlmcp_health

      sleep 10
    done
```

This comprehensive cluster architecture provides:
- **Scalability**: Through connection pooling, load balancing, and sharding
- **Resilience**: Via circuit breakers, retry policies, and multi-region deployment
- **Performance**: With optimized network configuration and caching layers
- **Observability**: Through comprehensive monitoring and chaos engineering
- **Automation**: Through deployment scripts and autoscaling policies