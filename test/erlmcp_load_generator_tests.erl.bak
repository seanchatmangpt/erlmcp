-module(erlmcp_load_generator_tests).

-include_lib("eunit/include/eunit.hrl").
-include_lib("common_test/include/ct.hrl").

%% Test exports
-export([
    all/0,
    init_per_suite/1,
    end_per_suite/1,
    init_per_testcase/2,
    end_per_testcase/2
]).

%% Test cases
-export([
    test_constant_load_generation/1,
    test_burst_load_generation/1,
    test_ramp_up_load_generation/1,
    test_sine_wave_load_generation/1,
    test_random_walk_load_generation/1,
    test_poisson_load_generation/1,
    test_message_size_patterns/1,
    test_connection_patterns/1,
    test_workload_patterns/1,
    test_protocol_handlers/1,
    test_metrics_collection/1,
    test_concurrent_generators/1,
    test_load_scaling/1,
    test_error_handling/1,
    test_tracing_integration/1,
    test_performance_limits/1
]).

all() ->
    [
        test_constant_load_generation,
        test_burst_load_generation,
        test_ramp_up_load_generation,
        test_sine_wave_load_generation,
        test_random_walk_load_generation,
        test_poisson_load_generation,
        test_message_size_patterns,
        test_connection_patterns,
        test_workload_patterns,
        test_protocol_handlers,
        test_metrics_collection,
        test_concurrent_generators,
        test_load_scaling,
        test_error_handling,
        test_tracing_integration,
        test_performance_limits
    ].

init_per_suite(Config) ->
    % Start required applications
    application:ensure_all_started(jsx),
    application:ensure_all_started(opentelemetry),
    
    % Initialize tracing
    otel_tracer_provider:setup(),
    
    % Start load generator
    {ok, _Pid} = erlmcp_load_generator:start_link(),
    
    ct:log("Load generator test suite initialized"),
    Config.

end_per_suite(_Config) ->
    % Stop load generator
    gen_server:stop(erlmcp_load_generator),
    
    % Stop applications
    application:stop(opentelemetry),
    application:stop(jsx),
    ok.

init_per_testcase(TestCase, Config) ->
    ct:log("Starting test case: ~p", [TestCase]),
    Config.

end_per_testcase(TestCase, _Config) ->
    ct:log("Completed test case: ~p", [TestCase]),
    
    % Clean up any active generators
    case erlmcp_load_generator:list_active_generators() of
        {ok, Generators} ->
            lists:foreach(fun({Id, _, _}) ->
                erlmcp_load_generator:stop_generation(Id)
            end, Generators);
        _ -> ok
    end,
    
    timer:sleep(100), % Allow cleanup
    ok.

%% Test constant load generation pattern
test_constant_load_generation(_Config) ->
    ct:log("Testing constant load generation"),
    
    Config = #{
        pattern => constant,
        rate => 10,
        duration => 5000,
        message_size => small,
        protocol => http,
        trace_every_request => true
    },
    
    {ok, GeneratorId} = erlmcp_load_generator:generate_traffic(Config),
    
    % Wait for generation to complete
    timer:sleep(6000),
    
    % Get metrics
    {ok, Metrics} = erlmcp_load_generator:get_metrics(GeneratorId),
    
    % Validate metrics
    ?assert(maps:get(requests_sent, Metrics) >= 45), % Allow some variance
    ?assert(maps:get(requests_sent, Metrics) =< 55),
    ?assert(maps:get(success_rate, Metrics) > 0.8),
    ?assert(maps:get(throughput_rps, Metrics) > 8),
    ?assert(maps:get(throughput_rps, Metrics) < 12),
    
    ct:log("Constant load metrics: ~p", [Metrics]),
    ok.

%% Test burst load generation pattern
test_burst_load_generation(_Config) ->
    ct:log("Testing burst load generation"),
    
    Config = #{
        pattern => burst,
        rate => 5,  % Base rate
        duration => 10000,
        message_size => small,
        protocol => http,
        trace_every_request => false % Reduce overhead for burst testing
    },
    
    {ok, GeneratorId} = erlmcp_load_generator:generate_traffic(Config),
    
    % Wait for generation to complete
    timer:sleep(11000),
    
    % Get metrics
    {ok, Metrics} = erlmcp_load_generator:get_metrics(GeneratorId),
    
    % During burst periods, rate should be much higher than base rate
    ?assert(maps:get(requests_sent, Metrics) > 50), % Should be more than constant rate
    ?assert(maps:get(success_rate, Metrics) > 0.7), % Allow for some burst-related failures
    
    ct:log("Burst load metrics: ~p", [Metrics]),
    ok.

%% Test ramp-up load generation pattern
test_ramp_up_load_generation(_Config) ->
    ct:log("Testing ramp-up load generation"),
    
    Config = #{
        pattern => ramp_up,
        rate => 20, % Max rate
        duration => 6000,
        message_size => small,
        protocol => websocket,
        trace_every_request => true
    },
    
    {ok, GeneratorId} = erlmcp_load_generator:generate_traffic(Config),
    
    % Wait for generation to complete
    timer:sleep(7000),
    
    % Get metrics
    {ok, Metrics} = erlmcp_load_generator:get_metrics(GeneratorId),
    
    % Should start slow and ramp up
    ?assert(maps:get(requests_sent, Metrics) > 30),
    ?assert(maps:get(requests_sent, Metrics) < 80), % Less than full rate duration
    ?assert(maps:get(success_rate, Metrics) > 0.85),
    
    ct:log("Ramp-up load metrics: ~p", [Metrics]),
    ok.

%% Test sine wave load generation pattern
test_sine_wave_load_generation(_Config) ->
    ct:log("Testing sine wave load generation"),
    
    Config = #{
        pattern => sine_wave,
        rate => 10, % Base rate
        duration => 8000,
        message_size => medium,
        protocol => tcp,
        trace_every_request => true
    },
    
    {ok, GeneratorId} = erlmcp_load_generator:generate_traffic(Config),
    
    % Wait for generation to complete
    timer:sleep(9000),
    
    % Get metrics
    {ok, Metrics} = erlmcp_load_generator:get_metrics(GeneratorId),
    
    % Should vary around the base rate
    ?assert(maps:get(requests_sent, Metrics) > 40),
    ?assert(maps:get(requests_sent, Metrics) < 100),
    ?assert(maps:get(success_rate, Metrics) > 0.9), % TCP should be very reliable
    
    ct:log("Sine wave load metrics: ~p", [Metrics]),
    ok.

%% Test random walk load generation pattern
test_random_walk_load_generation(_Config) ->
    ct:log("Testing random walk load generation"),
    
    Config = #{
        pattern => random_walk,
        rate => 15, % Base rate
        duration => 6000,
        message_size => small,
        protocol => mcp,
        trace_every_request => true
    },
    
    {ok, GeneratorId} = erlmcp_load_generator:generate_traffic(Config),
    
    % Wait for generation to complete
    timer:sleep(7000),
    
    % Get metrics
    {ok, Metrics} = erlmcp_load_generator:get_metrics(GeneratorId),
    
    % Should vary unpredictably
    ?assert(maps:get(requests_sent, Metrics) > 20),
    ?assert(maps:get(requests_sent, Metrics) < 120),
    ?assert(maps:get(success_rate, Metrics) > 0.8), % MCP has some error simulation
    
    ct:log("Random walk load metrics: ~p", [Metrics]),
    ok.

%% Test Poisson distribution load generation
test_poisson_load_generation(_Config) ->
    ct:log("Testing Poisson load generation"),
    
    Config = #{
        pattern => poisson,
        rate => 8, % Lambda (average rate)
        duration => 5000,
        message_size => large,
        protocol => http,
        trace_every_request => false % Large messages, reduce tracing overhead
    },
    
    {ok, GeneratorId} = erlmcp_load_generator:generate_traffic(Config),
    
    % Wait for generation to complete
    timer:sleep(6000),
    
    % Get metrics
    {ok, Metrics} = erlmcp_load_generator:get_metrics(GeneratorId),
    
    % Should follow Poisson distribution around lambda
    ?assert(maps:get(requests_sent, Metrics) > 20),
    ?assert(maps:get(requests_sent, Metrics) < 60),
    ?assert(maps:get(success_rate, Metrics) > 0.85),
    
    ct:log("Poisson load metrics: ~p", [Metrics]),
    ok.

%% Test different message size patterns
test_message_size_patterns(_Config) ->
    ct:log("Testing message size patterns"),
    
    Sizes = [small, medium, large, mixed],
    
    lists:foreach(fun(Size) ->
        ct:log("Testing message size: ~p", [Size]),
        
        Config = #{
            pattern => constant,
            rate => 5,
            duration => 3000,
            message_size => Size,
            protocol => http,
            trace_every_request => true
        },
        
        {ok, GeneratorId} = erlmcp_load_generator:generate_traffic(Config),
        timer:sleep(4000),
        
        {ok, Metrics} = erlmcp_load_generator:get_metrics(GeneratorId),
        ?assert(maps:get(requests_sent, Metrics) > 10),
        ?assert(maps:get(success_rate, Metrics) > 0.8),
        
        ct:log("Size ~p metrics: ~p", [Size, maps:get(requests_sent, Metrics)])
    end, Sizes),
    
    ok.

%% Test different connection patterns
test_connection_patterns(_Config) ->
    ct:log("Testing connection patterns"),
    
    Patterns = [long_lived, short_lived, pooled],
    
    lists:foreach(fun(Pattern) ->
        ct:log("Testing connection pattern: ~p", [Pattern]),
        
        Config = #{
            pattern => constant,
            rate => 8,
            duration => 3000,
            message_size => small,
            connection_type => Pattern,
            max_connections => 5,
            protocol => websocket,
            trace_every_request => true
        },
        
        {ok, GeneratorId} = erlmcp_load_generator:generate_traffic(Config),
        timer:sleep(4000),
        
        {ok, Metrics} = erlmcp_load_generator:get_metrics(GeneratorId),
        ?assert(maps:get(requests_sent, Metrics) > 15),
        ?assert(maps:get(success_rate, Metrics) > 0.85),
        
        ct:log("Connection ~p metrics: ~p", [Pattern, maps:get(success_rate, Metrics)])
    end, Patterns),
    
    ok.

%% Test different workload patterns
test_workload_patterns(_Config) ->
    ct:log("Testing workload patterns"),
    
    Workloads = [read_heavy, write_heavy, balanced, batch, stream],
    
    lists:foreach(fun(Workload) ->
        ct:log("Testing workload: ~p", [Workload]),
        
        Config = #{
            pattern => constant,
            rate => 6,
            duration => 3000,
            message_size => medium,
            workload => Workload,
            protocol => tcp,
            trace_every_request => true
        },
        
        {ok, GeneratorId} = erlmcp_load_generator:generate_traffic(Config),
        timer:sleep(4000),
        
        {ok, Metrics} = erlmcp_load_generator:get_metrics(GeneratorId),
        ?assert(maps:get(requests_sent, Metrics) > 12),
        ?assert(maps:get(success_rate, Metrics) > 0.85),
        
        ct:log("Workload ~p metrics: ~p", [Workload, maps:get(throughput_rps, Metrics)])
    end, Workloads),
    
    ok.

%% Test protocol-specific handlers
test_protocol_handlers(_Config) ->
    ct:log("Testing protocol handlers"),
    
    Protocols = [http, websocket, tcp, mcp],
    
    lists:foreach(fun(Protocol) ->
        ct:log("Testing protocol: ~p", [Protocol]),
        
        Config = #{
            pattern => constant,
            rate => 10,
            duration => 2000,
            message_size => small,
            protocol => Protocol,
            trace_every_request => true
        },
        
        {ok, GeneratorId} = erlmcp_load_generator:generate_traffic(Config),
        timer:sleep(3000),
        
        {ok, Metrics} = erlmcp_load_generator:get_metrics(GeneratorId),
        ?assert(maps:get(requests_sent, Metrics) > 15),
        
        % Each protocol should have different characteristics
        case Protocol of
            mcp ->
                % MCP should have higher latency and potentially more errors
                ?assert(maps:get(avg_latency_us, Metrics) > 20000); % > 20ms
            tcp ->
                % TCP should be fastest and most reliable
                ?assert(maps:get(success_rate, Metrics) > 0.95);
            websocket ->
                % WebSocket should be reliable with moderate latency
                ?assert(maps:get(success_rate, Metrics) > 0.90);
            http ->
                % HTTP should have good success rate
                ?assert(maps:get(success_rate, Metrics) > 0.90)
        end,
        
        ct:log("Protocol ~p metrics: latency=~p, success=~p", 
               [Protocol, maps:get(avg_latency_us, Metrics), maps:get(success_rate, Metrics)])
    end, Protocols),
    
    ok.

%% Test comprehensive metrics collection
test_metrics_collection(_Config) ->
    ct:log("Testing metrics collection"),
    
    Config = #{
        pattern => ramp_up,
        rate => 20,
        duration => 4000,
        message_size => mixed,
        protocol => http,
        trace_every_request => true
    },
    
    {ok, GeneratorId} = erlmcp_load_generator:generate_traffic(Config),
    
    % Check metrics during generation
    timer:sleep(2000),
    {ok, IntermediateMetrics} = erlmcp_load_generator:get_metrics(GeneratorId),
    ?assert(maps:get(requests_sent, IntermediateMetrics) > 0),
    
    % Wait for completion
    timer:sleep(3000),
    {ok, FinalMetrics} = erlmcp_load_generator:get_metrics(GeneratorId),
    
    % Validate all expected metrics are present
    RequiredFields = [
        generator_id, pattern, duration_ms, requests_sent, responses_received,
        errors, success_rate, throughput_rps, avg_latency_us, p50_latency_us,
        p95_latency_us, p99_latency_us, start_time
    ],
    
    lists:foreach(fun(Field) ->
        ?assert(maps:is_key(Field, FinalMetrics)),
        ?assert(maps:get(Field, FinalMetrics) =/= undefined)
    end, RequiredFields),
    
    % Validate metric relationships
    ?assert(maps:get(requests_sent, FinalMetrics) >= maps:get(responses_received, FinalMetrics)),
    ?assert(maps:get(p95_latency_us, FinalMetrics) >= maps:get(p50_latency_us, FinalMetrics)),
    ?assert(maps:get(p99_latency_us, FinalMetrics) >= maps:get(p95_latency_us, FinalMetrics)),
    ?assert(maps:get(success_rate, FinalMetrics) =< 1.0),
    
    ct:log("Final metrics validation passed: ~p", [FinalMetrics]),
    ok.

%% Test concurrent generators
test_concurrent_generators(_Config) ->
    ct:log("Testing concurrent generators"),
    
    % Start multiple generators concurrently
    Generators = lists:map(fun(I) ->
        Config = #{
            pattern => constant,
            rate => 5 + I, % Vary the rates
            duration => 3000,
            message_size => small,
            protocol => http,
            trace_every_request => false % Reduce overhead for concurrency test
        },
        {ok, Id} = erlmcp_load_generator:generate_traffic(Config),
        {I, Id}
    end, lists:seq(1, 5)),
    
    % Wait for all to complete
    timer:sleep(4000),
    
    % Validate all generators ran successfully
    lists:foreach(fun({I, Id}) ->
        {ok, Metrics} = erlmcp_load_generator:get_metrics(Id),
        ExpectedRequests = (5 + I) * 3, % rate * duration_seconds
        ?assert(maps:get(requests_sent, Metrics) >= ExpectedRequests - 5),
        ?assert(maps:get(requests_sent, Metrics) =< ExpectedRequests + 5),
        ct:log("Generator ~p sent ~p requests", [I, maps:get(requests_sent, Metrics)])
    end, Generators),
    
    % Test generator listing
    {ok, ActiveGenerators} = erlmcp_load_generator:list_active_generators(),
    ?assert(length(ActiveGenerators) =< 5), % Some may have completed
    
    ok.

%% Test load scaling and system limits
test_load_scaling(_Config) ->
    ct:log("Testing load scaling"),
    
    % Test increasing loads to find system limits
    Rates = [10, 50, 100, 200],
    
    lists:foreach(fun(Rate) ->
        ct:log("Testing load at ~p req/sec", [Rate]),
        
        Config = #{
            pattern => constant,
            rate => Rate,
            duration => 2000,
            message_size => small,
            protocol => http,
            trace_every_request => false % Reduce overhead at high rates
        },
        
        {ok, GeneratorId} = erlmcp_load_generator:generate_traffic(Config),
        timer:sleep(3000),
        
        {ok, Metrics} = erlmcp_load_generator:get_metrics(GeneratorId),
        ActualThroughput = maps:get(throughput_rps, Metrics),
        SuccessRate = maps:get(success_rate, Metrics),
        
        ct:log("Rate ~p: actual_throughput=~p, success_rate=~p", 
               [Rate, ActualThroughput, SuccessRate]),
        
        % At higher rates, we expect some degradation
        if Rate =< 50 ->
            ?assert(SuccessRate > 0.9);
           true ->
            ?assert(SuccessRate > 0.7) % Allow more errors at high rates
        end
    end, Rates),
    
    ok.

%% Test error handling and recovery
test_error_handling(_Config) ->
    ct:log("Testing error handling"),
    
    % Test with invalid configuration
    InvalidConfig = #{
        pattern => invalid_pattern,
        rate => -1,
        duration => 1000
    },
    
    Result = erlmcp_load_generator:generate_traffic(InvalidConfig),
    % Should still work due to default value handling in parse_config
    ?assertMatch({ok, _}, Result),
    
    % Test stopping non-existent generator
    StopResult = erlmcp_load_generator:stop_generation(<<"non_existent">>),
    ?assertEqual({error, not_found}, StopResult),
    
    % Test getting metrics for non-existent generator
    MetricsResult = erlmcp_load_generator:get_metrics(<<"non_existent">>),
    ?assertEqual({error, not_found}, MetricsResult),
    
    ok.

%% Test OpenTelemetry tracing integration
test_tracing_integration(_Config) ->
    ct:log("Testing tracing integration"),
    
    % Start a parent span
    ParentSpan = otel_tracer:start_span(<<"test_load_generation">>),
    otel_span:set_attribute(ParentSpan, <<"test_case">>, <<"tracing_integration">>),
    
    Config = #{
        pattern => constant,
        rate => 15,
        duration => 2000,
        message_size => small,
        protocol => mcp,
        trace_every_request => true
    },
    
    {ok, GeneratorId} = erlmcp_load_generator:generate_traffic(Config, ParentSpan),
    
    % Wait for generation
    timer:sleep(3000),
    
    {ok, Metrics} = erlmcp_load_generator:get_metrics(GeneratorId),
    ?assert(maps:get(requests_sent, Metrics) > 20),
    
    % End parent span
    otel_span:add_event(ParentSpan, <<"generation_completed">>, [
        {<<"generator_id">>, GeneratorId},
        {<<"total_requests">>, maps:get(requests_sent, Metrics)}
    ]),
    otel_span:end_span(ParentSpan),
    
    ct:log("Tracing integration test completed with ~p requests", 
           [maps:get(requests_sent, Metrics)]),
    ok.

%% Test performance limits and bottleneck detection
test_performance_limits(_Config) ->
    ct:log("Testing performance limits"),
    
    % Test extreme burst to expose limits
    Config = #{
        pattern => burst,
        rate => 1000, % Very high base rate
        duration => 5000,
        message_size => large,
        protocol => http,
        trace_every_request => false % Reduce overhead
    },
    
    StartTime = erlang:system_time(millisecond),
    {ok, GeneratorId} = erlmcp_load_generator:generate_traffic(Config),
    
    % Monitor during generation
    timer:sleep(1000),
    {ok, EarlyMetrics} = erlmcp_load_generator:get_metrics(GeneratorId),
    EarlyThroughput = maps:get(throughput_rps, EarlyMetrics),
    
    timer:sleep(2000),
    {ok, MidMetrics} = erlmcp_load_generator:get_metrics(GeneratorId),
    MidThroughput = maps:get(throughput_rps, MidMetrics),
    
    % Wait for completion
    timer:sleep(3000),
    {ok, FinalMetrics} = erlmcp_load_generator:get_metrics(GeneratorId),
    
    EndTime = erlang:system_time(millisecond),
    TestDuration = EndTime - StartTime,
    
    % Analyze performance characteristics
    FinalThroughput = maps:get(throughput_rps, FinalMetrics),
    SuccessRate = maps:get(success_rate, FinalMetrics),
    AvgLatency = maps:get(avg_latency_us, FinalMetrics),
    P99Latency = maps:get(p99_latency_us, FinalMetrics),
    
    ct:log("Performance test results:"),
    ct:log("  Test duration: ~p ms", [TestDuration]),
    ct:log("  Early throughput: ~p req/sec", [EarlyThroughput]),
    ct:log("  Mid throughput: ~p req/sec", [MidThroughput]),
    ct:log("  Final throughput: ~p req/sec", [FinalThroughput]),
    ct:log("  Success rate: ~p", [SuccessRate]),
    ct:log("  Avg latency: ~p μs", [AvgLatency]),
    ct:log("  P99 latency: ~p μs", [P99Latency]),
    
    % Performance assertions
    ?assert(FinalThroughput > 0),
    ?assert(SuccessRate >= 0), % May degrade under extreme load
    ?assert(P99Latency >= AvgLatency),
    
    % Detect if system is hitting limits
    if SuccessRate < 0.5 ->
        ct:log("WARNING: Success rate below 50%, system may be overloaded");
       true -> ok
    end,
    
    if P99Latency > AvgLatency * 10 ->
        ct:log("WARNING: High latency tail, possible bottleneck detected");
       true -> ok
    end,
    
    ok.