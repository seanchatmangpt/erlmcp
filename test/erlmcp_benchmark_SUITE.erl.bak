-module(erlmcp_benchmark_SUITE).
-compile(export_all).

-include_lib("common_test/include/ct.hrl").
-include_lib("eunit/include/eunit.hrl").

%% OpenTelemetry integration
-include_lib("opentelemetry_api/include/otel_tracer.hrl").

%% Benchmark result record for structured output
-record(benchmark_result, {
    test_name :: string(),
    transport_type :: atom(),
    timestamp :: integer(),
    duration_us :: integer(),
    throughput_bps :: float(),
    latency_p50 :: integer(),
    latency_p95 :: integer(),
    latency_p99 :: integer(),
    latency_p999 :: integer(),
    memory_peak_mb :: float(),
    cpu_avg_percent :: float(),
    operations_count :: integer(),
    error_count :: integer(),
    success_rate :: float(),
    connections_count :: integer(),
    message_size :: integer(),
    regression_detected :: boolean(),
    baseline_comparison :: map()
}).

%% Baseline thresholds for regression detection
-define(THROUGHPUT_REGRESSION_THRESHOLD, 0.15). % 15% degradation
-define(LATENCY_REGRESSION_THRESHOLD, 0.20).    % 20% increase
-define(MEMORY_REGRESSION_THRESHOLD, 0.25).     % 25% increase

%% Test configuration constants
-define(MESSAGE_SIZES, [1, 128, 1024, 8192, 65536, 1048576, 10485760]). % 1B to 10MB
-define(CONNECTION_SCALES, [1, 10, 50, 100, 500, 1000, 5000, 10000]).
-define(SUSTAINED_LOAD_DURATION, 300000). % 5 minutes

%%====================================================================
%% Common Test Callbacks
%%====================================================================

suite() ->
    [{timetrap, {minutes, 60}}]. % Extended timeout for comprehensive benchmarks

all() ->
    [
        {group, throughput_benchmarks},
        {group, latency_benchmarks}, 
        {group, resource_benchmarks},
        {group, scalability_benchmarks},
        {group, regression_detection},
        {group, baseline_comparison},
        {group, sustained_load_tests}
    ].

groups() ->
    [
        {throughput_benchmarks, [parallel], [
            throughput_stdio_small_messages,
            throughput_stdio_medium_messages,
            throughput_stdio_large_messages,
            throughput_tcp_small_messages,
            throughput_tcp_medium_messages,
            throughput_tcp_large_messages,
            throughput_http_small_messages,
            throughput_http_medium_messages,
            throughput_http_large_messages,
            throughput_concurrent_connections,
            throughput_message_size_scaling
        ]},
        {latency_benchmarks, [parallel], [
            latency_end_to_end_stdio,
            latency_end_to_end_tcp,
            latency_end_to_end_http,
            latency_percentiles_analysis,
            latency_under_load_stdio,
            latency_under_load_tcp,
            latency_under_load_http,
            latency_connection_establishment
        ]},
        {resource_benchmarks, [parallel], [
            memory_usage_per_connection,
            memory_usage_message_size_impact,
            cpu_utilization_patterns,
            file_descriptor_scaling,
            process_count_scaling,
            garbage_collection_impact
        ]},
        {scalability_benchmarks, [], [
            linear_scaling_test,
            breakpoint_detection,
            degradation_curve_analysis,
            connection_limit_discovery,
            throughput_plateau_identification
        ]},
        {regression_detection, [], [
            detect_throughput_regression,
            detect_latency_regression,
            detect_memory_regression,
            detect_scalability_regression
        ]},
        {baseline_comparison, [], [
            compare_against_baseline,
            update_baseline_metrics,
            generate_trend_analysis
        ]},
        {sustained_load_tests, [], [
            sustained_load_stdio_5min,
            sustained_load_tcp_5min,
            sustained_load_http_5min,
            sustained_load_mixed_transport
        ]}
    ].

init_per_suite(Config) ->
    %% Initialize OpenTelemetry
    application:ensure_started(opentelemetry),
    application:ensure_started(opentelemetry_exporter),
    
    %% Setup tracing
    otel_tracer:set_default_tracer({erlmcp_benchmark, <<"1.0.0">>}),
    
    %% Initialize performance monitoring
    application:ensure_started(crypto),
    application:ensure_started(ssl),
    application:ensure_started(inets),
    
    %% Start performance analyzer
    {ok, _AnalyzerPid} = erlmcp_performance_analysis:start_link(),
    
    %% Create results directory
    ResultsDir = filename:join([code:priv_dir(erlmcp), "benchmark_results"]),
    ok = filelib:ensure_dir(filename:join(ResultsDir, "dummy")),
    
    %% Load existing baselines
    BaselinesFile = filename:join(ResultsDir, "baselines.json"),
    Baselines = load_baselines(BaselinesFile),
    
    %% Test configuration
    TestConfig = #{
        results_dir => ResultsDir,
        baselines_file => BaselinesFile,
        baselines => Baselines,
        test_start_time => erlang:system_time(microsecond),
        message_sizes => ?MESSAGE_SIZES,
        connection_scales => ?CONNECTION_SCALES
    },
    
    ct:pal("Benchmark suite initialized. Results will be stored in: ~s", [ResultsDir]),
    [{test_config, TestConfig} | Config].

end_per_suite(Config) ->
    TestConfig = proplists:get_value(test_config, Config),
    ResultsDir = maps:get(results_dir, TestConfig),
    
    %% Generate final benchmark report
    generate_final_report(Config),
    
    %% Stop services
    erlmcp_performance_analysis:stop(),
    
    ct:pal("Benchmark suite completed. Check results in: ~s", [ResultsDir]),
    ok.

init_per_group(GroupName, Config) ->
    ct:pal("Starting benchmark group: ~p", [GroupName]),
    
    %% Start OpenTelemetry span for group
    SpanName = list_to_binary("benchmark_group_" ++ atom_to_list(GroupName)),
    SpanCtx = otel_tracer:start_span(SpanName),
    otel_tracer:set_current_span(SpanCtx),
    
    [{group_span, SpanCtx}, {group_name, GroupName} | Config].

end_per_group(GroupName, Config) ->
    %% End OpenTelemetry span
    case proplists:get_value(group_span, Config) of
        undefined -> ok;
        SpanCtx -> 
            otel_tracer:set_current_span(SpanCtx),
            otel_tracer:end_span()
    end,
    
    ct:pal("Completed benchmark group: ~p", [GroupName]),
    ok.

init_per_testcase(TestCase, Config) ->
    ct:pal("Starting benchmark test: ~p", [TestCase]),
    
    %% Start OpenTelemetry span for test
    SpanName = list_to_binary("benchmark_test_" ++ atom_to_list(TestCase)),
    SpanCtx = otel_tracer:start_span(SpanName),
    otel_tracer:set_current_span(SpanCtx),
    
    %% Reset performance metrics
    erlmcp_performance_analysis ! {reset_metrics},
    
    %% Enable scheduler statistics for CPU monitoring
    erlang:system_flag(scheduler_wall_time, true),
    
    [{test_span, SpanCtx}, {test_case, TestCase} | Config].

end_per_testcase(TestCase, Config) ->
    %% End OpenTelemetry span
    case proplists:get_value(test_span, Config) of
        undefined -> ok;
        SpanCtx -> 
            otel_tracer:set_current_span(SpanCtx),
            otel_tracer:end_span()
    end,
    
    ct:pal("Completed benchmark test: ~p", [TestCase]),
    ok.

%%====================================================================
%% Throughput Benchmarks
%%====================================================================

throughput_stdio_small_messages(Config) ->
    run_throughput_benchmark(Config, stdio, 128, 10000, "STDIO Small Messages").

throughput_stdio_medium_messages(Config) ->
    run_throughput_benchmark(Config, stdio, 1024, 5000, "STDIO Medium Messages").

throughput_stdio_large_messages(Config) ->
    run_throughput_benchmark(Config, stdio, 8192, 1000, "STDIO Large Messages").

throughput_tcp_small_messages(Config) ->
    run_throughput_benchmark(Config, tcp, 128, 5000, "TCP Small Messages").

throughput_tcp_medium_messages(Config) ->
    run_throughput_benchmark(Config, tcp, 1024, 2000, "TCP Medium Messages").

throughput_tcp_large_messages(Config) ->
    run_throughput_benchmark(Config, tcp, 8192, 500, "TCP Large Messages").

throughput_http_small_messages(Config) ->
    run_throughput_benchmark(Config, http, 128, 1000, "HTTP Small Messages").

throughput_http_medium_messages(Config) ->
    run_throughput_benchmark(Config, http, 1024, 500, "HTTP Medium Messages").

throughput_http_large_messages(Config) ->
    run_throughput_benchmark(Config, http, 8192, 100, "HTTP Large Messages").

throughput_concurrent_connections(Config) ->
    TestConfig = proplists:get_value(test_config, Config),
    MessageSize = 1024,
    MessagesPerConnection = 1000,
    
    %% Test different connection counts
    ConnectionCounts = [1, 5, 10, 20, 50],
    
    Results = lists:map(fun(Connections) ->
        TestName = io_lib:format("Concurrent ~p connections", [Connections]),
        Result = run_concurrent_throughput_benchmark(
            Config, stdio, MessageSize, MessagesPerConnection, Connections, TestName
        ),
        {Connections, Result}
    end, ConnectionCounts),
    
    %% Analyze scaling characteristics
    analyze_concurrent_scaling(Results, Config),
    
    %% Verify reasonable scaling
    [{1, Result1}, {10, Result10} | _] = Results,
    Throughput1 = Result1#benchmark_result.throughput_bps,
    Throughput10 = Result10#benchmark_result.throughput_bps,
    
    %% Expect some scaling improvement (at least 3x with 10x connections)
    ?assert(Throughput10 > Throughput1 * 3),
    
    ok.

throughput_message_size_scaling(Config) ->
    TestConfig = proplists:get_value(test_config, Config),
    MessageSizes = [64, 256, 1024, 4096, 16384, 65536],
    MessageCount = 1000,
    
    %% Test STDIO transport across message sizes
    Results = lists:map(fun(Size) ->
        TestName = io_lib:format("Message size ~p bytes", [Size]),
        Result = run_throughput_benchmark(Config, stdio, Size, MessageCount, TestName),
        {Size, Result}
    end, MessageSizes),
    
    %% Analyze message size impact
    analyze_message_size_scaling(Results, Config),
    
    %% Verify throughput generally increases with message size (efficiency)
    [{64, Small}, {1024, Medium}, {16384, Large} | _] = Results,
    
    ?assert(Medium#benchmark_result.throughput_bps > Small#benchmark_result.throughput_bps),
    ?assert(Large#benchmark_result.throughput_bps > Medium#benchmark_result.throughput_bps),
    
    ok.

%%====================================================================
%% Latency Benchmarks
%%====================================================================

latency_end_to_end_stdio(Config) ->
    run_latency_benchmark(Config, stdio, 1024, 1000, "STDIO End-to-End Latency").

latency_end_to_end_tcp(Config) ->
    run_latency_benchmark(Config, tcp, 1024, 1000, "TCP End-to-End Latency").

latency_end_to_end_http(Config) ->
    run_latency_benchmark(Config, http, 1024, 500, "HTTP End-to-End Latency").

latency_percentiles_analysis(Config) ->
    %% Comprehensive latency analysis across transports
    Transports = [stdio, tcp, http],
    MessageCount = 2000,
    MessageSize = 1024,
    
    Results = lists:map(fun(Transport) ->
        TestName = io_lib:format("~p Percentile Analysis", [Transport]),
        Result = run_detailed_latency_benchmark(Config, Transport, MessageSize, MessageCount, TestName),
        {Transport, Result}
    end, Transports),
    
    %% Verify latency expectations: stdio < tcp < http
    [{stdio, StdioResult}, {tcp, TcpResult}, {http, HttpResult}] = Results,
    
    ?assert(StdioResult#benchmark_result.latency_p95 < TcpResult#benchmark_result.latency_p95),
    ?assert(TcpResult#benchmark_result.latency_p95 < HttpResult#benchmark_result.latency_p95),
    
    %% Verify reasonable P99/P50 ratios (< 10x for well-behaved systems)
    lists:foreach(fun({Transport, Result}) ->
        P50 = Result#benchmark_result.latency_p50,
        P99 = Result#benchmark_result.latency_p99,
        Ratio = case P50 of
            0 -> 0;
            _ -> P99 / P50
        end,
        ct:pal("~p latency tail ratio (P99/P50): ~.2f", [Transport, Ratio]),
        ?assert(Ratio < 20.0) % Allow some tail latency but not excessive
    end, Results),
    
    ok.

latency_under_load_stdio(Config) ->
    run_latency_under_load_benchmark(Config, stdio, 1024, 10, "STDIO Latency Under Load").

latency_under_load_tcp(Config) ->
    run_latency_under_load_benchmark(Config, tcp, 1024, 5, "TCP Latency Under Load").

latency_under_load_http(Config) ->
    run_latency_under_load_benchmark(Config, http, 1024, 2, "HTTP Latency Under Load").

latency_connection_establishment(Config) ->
    %% Measure connection establishment time for each transport
    Attempts = 100,
    
    Results = lists:map(fun(Transport) ->
        TestName = io_lib:format("~p Connection Establishment", [Transport]),
        
        Durations = lists:map(fun(_) ->
            {Time, _} = timer:tc(fun() -> 
                setup_and_cleanup_transport(Transport)
            end),
            Time
        end, lists:seq(1, Attempts)),
        
        create_benchmark_result(TestName, Transport, #{
            connection_establishment_times => Durations,
            operations_count => Attempts
        })
    end, [tcp, http]), % Skip stdio as it doesn't have connection establishment
    
    %% Verify reasonable connection times
    lists:foreach(fun(Result) ->
        case Result#benchmark_result.latency_p95 of
            P95 when P95 > 0 ->
                % Connection establishment should be < 5 seconds P95
                ?assert(P95 < 5000000),
                ct:pal("~s P95 connection time: ~.2f ms", 
                       [Result#benchmark_result.test_name, P95/1000])
        end
    end, Results),
    
    ok.

%%====================================================================
%% Resource Benchmarks
%%====================================================================

memory_usage_per_connection(Config) ->
    %% Measure memory usage scaling with connection count
    ConnectionCounts = [1, 10, 50, 100, 500],
    MessageSize = 1024,
    TestDuration = 10000, % 10 seconds
    
    Results = lists:map(fun(Connections) ->
        TestName = io_lib:format("Memory usage ~p connections", [Connections]),
        
        InitialMemory = erlang:memory(total),
        
        %% Start concurrent connections
        Processes = start_concurrent_memory_test(stdio, Connections, MessageSize, TestDuration),
        
        %% Monitor peak memory usage
        PeakMemory = monitor_peak_memory_usage(TestDuration + 2000, InitialMemory),
        
        %% Wait for cleanup
        wait_for_processes(Processes),
        timer:sleep(2000), % Allow GC
        
        FinalMemory = erlang:memory(total),
        MemoryIncrease = PeakMemory - InitialMemory,
        
        create_benchmark_result(TestName, stdio, #{
            connections_count => Connections,
            memory_increase_bytes => MemoryIncrease,
            memory_per_connection => MemoryIncrease / Connections,
            operations_count => Connections
        })
    end, ConnectionCounts),
    
    %% Analyze memory scaling
    analyze_memory_scaling(Results, Config),
    
    %% Verify linear scaling (within reason)
    [{1, Result1}, {100, Result100} | _] = 
        [{C, R} || R <- Results, C <- [R#benchmark_result.connections_count], C =:= 1 orelse C =:= 100],
    
    MemoryPerConn1 = Result1#benchmark_result.memory_peak_mb * 1024 * 1024,
    MemoryPerConn100 = Result100#benchmark_result.memory_peak_mb * 1024 * 1024 / 100,
    
    %% Memory per connection should not grow excessively (< 10x)
    Ratio = MemoryPerConn100 / max(1, MemoryPerConn1),
    ct:pal("Memory per connection ratio (100 vs 1): ~.2f", [Ratio]),
    ?assert(Ratio < 10.0),
    
    ok.

memory_usage_message_size_impact(Config) ->
    %% Test memory usage with different message sizes
    MessageSizes = ?MESSAGE_SIZES,
    ConnectionCount = 10,
    TestDuration = 5000,
    
    Results = lists:map(fun(MessageSize) ->
        TestName = io_lib:format("Memory usage ~p byte messages", [MessageSize]),
        
        InitialMemory = erlang:memory(total),
        Processes = start_concurrent_memory_test(stdio, ConnectionCount, MessageSize, TestDuration),
        PeakMemory = monitor_peak_memory_usage(TestDuration + 1000, InitialMemory),
        
        wait_for_processes(Processes),
        MemoryIncrease = PeakMemory - InitialMemory,
        
        create_benchmark_result(TestName, stdio, #{
            message_size => MessageSize,
            memory_increase_bytes => MemoryIncrease,
            memory_per_message => MemoryIncrease / (ConnectionCount * 100), % Approx messages
            operations_count => ConnectionCount * 100
        })
    end, MessageSizes),
    
    %% Verify memory usage correlates with message size
    SmallMsg = hd([R || R <- Results, R#benchmark_result.message_size =< 128]),
    LargeMsg = hd([R || R <- Results, R#benchmark_result.message_size >= 1048576]),
    
    ?assert(LargeMsg#benchmark_result.memory_peak_mb > SmallMsg#benchmark_result.memory_peak_mb),
    
    ok.

cpu_utilization_patterns(Config) ->
    %% Monitor CPU utilization during different workloads
    TestCases = [
        {stdio, 1024, 1000, "STDIO CPU baseline"},
        {stdio, 1024, 10000, "STDIO CPU high load"},
        {tcp, 1024, 1000, "TCP CPU baseline"},
        {http, 1024, 500, "HTTP CPU baseline"}
    ],
    
    Results = lists:map(fun({Transport, MessageSize, MessageCount, TestName}) ->
        %% Enable CPU monitoring
        erlang:system_flag(scheduler_wall_time, true),
        InitialStats = erlang:statistics(scheduler_wall_time),
        
        %% Run workload
        {Duration, _} = timer:tc(fun() ->
            run_throughput_workload(Transport, MessageSize, MessageCount)
        end),
        
        %% Calculate CPU usage
        FinalStats = erlang:statistics(scheduler_wall_time),
        CpuUsage = calculate_cpu_usage(InitialStats, FinalStats),
        
        create_benchmark_result(TestName, Transport, #{
            duration_us => Duration,
            cpu_usage_percent => CpuUsage,
            operations_count => MessageCount,
            message_size => MessageSize
        })
    end, TestCases),
    
    %% Verify CPU usage is reasonable (< 90% for sustainable load)
    lists:foreach(fun(Result) ->
        CpuUsage = Result#benchmark_result.cpu_avg_percent,
        ct:pal("~s CPU usage: ~.1f%%", [Result#benchmark_result.test_name, CpuUsage]),
        ?assert(CpuUsage < 90.0)
    end, Results),
    
    ok.

file_descriptor_scaling(Config) ->
    %% Test file descriptor usage with connection scaling
    %% Note: This primarily applies to TCP and HTTP transports
    
    ConnectionCounts = [1, 10, 50, 100],
    
    Results = lists:map(fun(Connections) ->
        TestName = io_lib:format("FD usage ~p TCP connections", [Connections]),
        
        InitialFDs = count_file_descriptors(),
        
        %% Start TCP connections
        Processes = start_tcp_connections(Connections),
        timer:sleep(1000), % Allow connections to establish
        
        PeakFDs = count_file_descriptors(),
        
        %% Cleanup
        cleanup_processes(Processes),
        timer:sleep(2000), % Allow cleanup
        
        FinalFDs = count_file_descriptors(),
        FDIncrease = PeakFDs - InitialFDs,
        
        create_benchmark_result(TestName, tcp, #{
            connections_count => Connections,
            fd_increase => FDIncrease,
            fd_per_connection => FDIncrease / max(1, Connections),
            operations_count => Connections
        })
    end, ConnectionCounts),
    
    %% Verify FD usage scales reasonably
    case Results of
        [Result1, Result10 | _] ->
            FD1 = get_metric_value(Result1, fd_increase, 0),
            FD10 = get_metric_value(Result10, fd_increase, 0),
            
            %% FD usage should scale roughly linearly (within 2x factor)
            ExpectedFD10 = FD1 * 10,
            ?assert(FD10 =< ExpectedFD10 * 2),
            ct:pal("FD scaling: 1 conn = ~p FDs, 10 conns = ~p FDs", [FD1, FD10]);
        _ ->
            ok
    end,
    
    ok.

process_count_scaling(Config) ->
    %% Monitor process count during scaling tests
    ConnectionCounts = [1, 10, 50, 100],
    
    Results = lists:map(fun(Connections) ->
        TestName = io_lib:format("Process count ~p connections", [Connections]),
        
        InitialProcesses = erlang:system_info(process_count),
        
        %% Start workload
        WorkerProcesses = start_concurrent_workload(stdio, Connections, 1000, 1024),
        
        PeakProcesses = erlang:system_info(process_count),
        
        %% Cleanup
        cleanup_processes(WorkerProcesses),
        timer:sleep(1000),
        
        FinalProcesses = erlang:system_info(process_count),
        ProcessIncrease = PeakProcesses - InitialProcesses,
        
        create_benchmark_result(TestName, stdio, #{
            connections_count => Connections,
            process_increase => ProcessIncrease,
            processes_per_connection => ProcessIncrease / max(1, Connections),
            operations_count => Connections
        })
    end, ConnectionCounts),
    
    %% Verify process cleanup
    case lists:last(Results) of
        LastResult ->
            ProcessLimit = erlang:system_info(process_limit),
            MaxProcesses = get_metric_value(LastResult, process_increase, 0),
            
            %% Should not exceed 10% of process limit
            ?assert(MaxProcesses < ProcessLimit * 0.1),
            ct:pal("Peak process increase: ~p (limit: ~p)", [MaxProcesses, ProcessLimit])
    end,
    
    ok.

garbage_collection_impact(Config) ->
    %% Measure GC impact on performance
    MessageSizes = [1024, 8192, 65536],
    MessageCount = 5000,
    
    Results = lists:map(fun(MessageSize) ->
        TestName = io_lib:format("GC impact ~p byte messages", [MessageSize]),
        
        %% Force initial GC
        erlang:garbage_collect(),
        InitialGCStats = erlang:statistics(garbage_collection),
        
        %% Run memory-intensive workload
        {Duration, _} = timer:tc(fun() ->
            run_throughput_workload(stdio, MessageSize, MessageCount)
        end),
        
        FinalGCStats = erlang:statistics(garbage_collection),
        
        {InitialGCs, InitialWordsReclaimed} = InitialGCStats,
        {FinalGCs, FinalWordsReclaimed} = FinalGCStats,
        
        GCCount = FinalGCs - InitialGCs,
        WordsReclaimed = FinalWordsReclaimed - InitialWordsReclaimed,
        
        create_benchmark_result(TestName, stdio, #{
            duration_us => Duration,
            gc_count => GCCount,
            words_reclaimed => WordsReclaimed,
            gc_per_operation => GCCount / max(1, MessageCount),
            operations_count => MessageCount,
            message_size => MessageSize
        })
    end, MessageSizes),
    
    %% Verify GC impact is reasonable
    lists:foreach(fun(Result) ->
        GCCount = get_metric_value(Result, gc_count, 0),
        OpsCount = Result#benchmark_result.operations_count,
        GCRate = GCCount / max(1, OpsCount),
        
        ct:pal("~s: ~p GCs for ~p operations (~.4f GC/op)", 
               [Result#benchmark_result.test_name, GCCount, OpsCount, GCRate]),
        
        %% GC rate should be reasonable (< 10% of operations)
        ?assert(GCRate < 0.1)
    end, Results),
    
    ok.

%%====================================================================
%% Scalability Benchmarks
%%====================================================================

linear_scaling_test(Config) ->
    %% Test linear scaling characteristics
    ConnectionCounts = [1, 2, 5, 10, 20, 50],
    MessageSize = 1024,
    MessagesPerConnection = 1000,
    
    Results = lists:map(fun(Connections) ->
        TestName = io_lib:format("Linear scaling ~p connections", [Connections]),
        
        run_concurrent_throughput_benchmark(
            Config, stdio, MessageSize, MessagesPerConnection, Connections, TestName
        )
    end, ConnectionCounts),
    
    %% Analyze scaling efficiency
    BaseResult = hd(Results),
    BaseThroughput = BaseResult#benchmark_result.throughput_bps,
    
    ScalingEfficiencies = lists:map(fun(Result) ->
        Connections = Result#benchmark_result.connections_count,
        Throughput = Result#benchmark_result.throughput_bps,
        ExpectedThroughput = BaseThroughput * Connections,
        Efficiency = Throughput / ExpectedThroughput,
        
        ct:pal("~p connections: ~.2f%% scaling efficiency", 
               [Connections, Efficiency * 100]),
        
        {Connections, Efficiency}
    end, Results),
    
    %% Verify reasonable scaling (at least 50% efficiency up to 10 connections)
    {10, Efficiency10} = lists:keyfind(10, 1, ScalingEfficiencies),
    ?assert(Efficiency10 > 0.5),
    
    ok.

breakpoint_detection(Config) ->
    %% Find the point where performance starts degrading significantly
    ConnectionSequence = [1, 5, 10, 20, 50, 100, 200, 500, 1000, 2000],
    MessageSize = 1024,
    MessagesPerConnection = 500,
    
    Results = lists:foldl(fun(Connections, Acc) ->
        try
            TestName = io_lib:format("Breakpoint test ~p connections", [Connections]),
            Result = run_concurrent_throughput_benchmark(
                Config, stdio, MessageSize, MessagesPerConnection, Connections, TestName
            ),
            [{Connections, Result} | Acc]
        catch
            _:Error ->
                ct:pal("Failed at ~p connections: ~p", [Connections, Error]),
                [{Connections, {error, Error}} | Acc]
        end
    end, [], ConnectionSequence),
    
    %% Find breakpoint (where throughput drops significantly or errors occur)
    SuccessfulResults = [{C, R} || {C, R} <- Results, is_record(R, benchmark_result)],
    
    case find_performance_breakpoint(SuccessfulResults) of
        {breakpoint, Connections, Reason} ->
            ct:pal("Performance breakpoint detected at ~p connections: ~p", 
                   [Connections, Reason]);
        no_breakpoint ->
            ct:pal("No clear breakpoint detected within test range")
    end,
    
    %% Verify we can handle at least 50 connections
    case lists:keyfind(50, 1, SuccessfulResults) of
        {50, Result50} ->
            ?assert(Result50#benchmark_result.success_rate > 80.0);
        false ->
            ct:fail("Failed to handle 50 concurrent connections")
    end,
    
    ok.

degradation_curve_analysis(Config) ->
    %% Analyze how performance degrades with increasing load
    LoadPoints = [10, 50, 100, 200, 500, 1000],
    MessageSize = 1024,
    MessageCount = 1000,
    
    Results = lists:map(fun(Load) ->
        TestName = io_lib:format("Degradation analysis load ~p", [Load]),
        
        run_concurrent_throughput_benchmark(
            Config, stdio, MessageSize, MessageCount, Load, TestName
        )
    end, LoadPoints),
    
    %% Calculate degradation metrics
    BaseResult = hd(Results),
    BaseLatency = BaseResult#benchmark_result.latency_p95,
    BaseThroughput = BaseResult#benchmark_result.throughput_bps,
    
    DegradationPoints = lists:map(fun(Result) ->
        Load = Result#benchmark_result.connections_count,
        Latency = Result#benchmark_result.latency_p95,
        Throughput = Result#benchmark_result.throughput_bps,
        
        LatencyDegradation = (Latency - BaseLatency) / max(1, BaseLatency),
        ThroughputDegradation = (BaseThroughput - Throughput) / max(1, BaseThroughput),
        
        ct:pal("Load ~p: Latency +~.1f%%, Throughput -~.1f%%", 
               [Load, LatencyDegradation * 100, ThroughputDegradation * 100]),
        
        {Load, LatencyDegradation, ThroughputDegradation}
    end, Results),
    
    %% Verify graceful degradation (not cliff-like)
    MaxLatencyDegradation = lists:max([LD || {_, LD, _} <- DegradationPoints]),
    MaxThroughputDegradation = lists:max([TD || {_, _, TD} <- DegradationPoints]),
    
    %% Should not have extreme degradation (< 500% latency increase)
    ?assert(MaxLatencyDegradation < 5.0),
    
    ok.

connection_limit_discovery(Config) ->
    %% Discover practical connection limits
    StartConnections = 100,
    StepSize = 100,
    MaxConnections = 2000,
    
    discover_connection_limit(Config, StartConnections, StepSize, MaxConnections).

throughput_plateau_identification(Config) ->
    %% Find throughput saturation point
    MessageSizes = [128, 512, 1024, 2048, 4096, 8192, 16384, 32768],
    MessageCount = 1000,
    
    Results = lists:map(fun(MessageSize) ->
        TestName = io_lib:format("Plateau test ~p bytes", [MessageSize]),
        run_throughput_benchmark(Config, stdio, MessageSize, MessageCount, TestName)
    end, MessageSizes),
    
    %% Find throughput plateau
    Throughputs = [R#benchmark_result.throughput_bps || R <- Results],
    case find_throughput_plateau(Throughputs, MessageSizes) of
        {plateau, MessageSize, Throughput} ->
            ct:pal("Throughput plateau at ~p bytes: ~.2f MB/s", 
                   [MessageSize, Throughput / (1024*1024)]);
        no_plateau ->
            ct:pal("No clear throughput plateau found")
    end,
    
    ok.

%%====================================================================
%% Regression Detection
%%====================================================================

detect_throughput_regression(Config) ->
    TestConfig = proplists:get_value(test_config, Config),
    Baselines = maps:get(baselines, TestConfig),
    
    %% Run current throughput tests
    CurrentResults = [
        run_throughput_benchmark(Config, stdio, 1024, 5000, "Regression STDIO"),
        run_throughput_benchmark(Config, tcp, 1024, 2000, "Regression TCP"),
        run_throughput_benchmark(Config, http, 1024, 500, "Regression HTTP")
    ],
    
    %% Compare against baselines
    Regressions = lists:foldl(fun(Result, Acc) ->
        TestKey = create_baseline_key(Result),
        case maps:get(TestKey, Baselines, undefined) of
            undefined ->
                ct:pal("No baseline for ~s", [TestKey]),
                Acc;
            BaselineThroughput ->
                CurrentThroughput = Result#benchmark_result.throughput_bps,
                Degradation = (BaselineThroughput - CurrentThroughput) / BaselineThroughput,
                
                if
                    Degradation > ?THROUGHPUT_REGRESSION_THRESHOLD ->
                        Regression = #{
                            test => Result#benchmark_result.test_name,
                            metric => throughput,
                            baseline => BaselineThroughput,
                            current => CurrentThroughput,
                            degradation_percent => Degradation * 100
                        },
                        [Regression | Acc];
                    true ->
                        Acc
                end
        end
    end, [], CurrentResults),
    
    case Regressions of
        [] ->
            ct:pal("No throughput regressions detected"),
            ok;
        _ ->
            lists:foreach(fun(Reg) ->
                ct:pal("REGRESSION: ~s throughput degraded by ~.1f%% (~.2f -> ~.2f MB/s)",
                       [maps:get(test, Reg), maps:get(degradation_percent, Reg),
                        maps:get(baseline, Reg) / (1024*1024),
                        maps:get(current, Reg) / (1024*1024)])
            end, Regressions),
            
            %% Mark test results
            lists:foreach(fun(Result) ->
                save_result_with_regression_flag(Result#benchmark_result{regression_detected = true}, Config)
            end, CurrentResults),
            
            %% Fail if regressions found
            ct:fail("Throughput regressions detected")
    end.

detect_latency_regression(Config) ->
    TestConfig = proplists:get_value(test_config, Config),
    Baselines = maps:get(baselines, TestConfig),
    
    CurrentResults = [
        run_latency_benchmark(Config, stdio, 1024, 1000, "Latency Regression STDIO"),
        run_latency_benchmark(Config, tcp, 1024, 1000, "Latency Regression TCP"),
        run_latency_benchmark(Config, http, 1024, 500, "Latency Regression HTTP")
    ],
    
    Regressions = detect_metric_regression(
        CurrentResults, Baselines, latency_p95, ?LATENCY_REGRESSION_THRESHOLD
    ),
    
    handle_regression_results(Regressions, "latency", Config).

detect_memory_regression(Config) ->
    TestConfig = proplists:get_value(test_config, Config),
    Baselines = maps:get(baselines, TestConfig),
    
    %% Run memory-intensive test
    CurrentResult = run_memory_benchmark(Config, stdio, 10, 8192, "Memory Regression"),
    
    TestKey = create_baseline_key(CurrentResult),
    case maps:get(TestKey, Baselines, undefined) of
        undefined ->
            ct:pal("No memory baseline for comparison");
        BaselineMemory ->
            CurrentMemory = CurrentResult#benchmark_result.memory_peak_mb,
            Increase = (CurrentMemory - BaselineMemory) / BaselineMemory,
            
            if
                Increase > ?MEMORY_REGRESSION_THRESHOLD ->
                    ct:pal("MEMORY REGRESSION: ~.1f%% increase (~.2f -> ~.2f MB)",
                           [Increase * 100, BaselineMemory, CurrentMemory]),
                    save_result_with_regression_flag(
                        CurrentResult#benchmark_result{regression_detected = true}, Config
                    ),
                    ct:fail("Memory regression detected");
                true ->
                    ct:pal("Memory usage within acceptable range: ~.2f MB", [CurrentMemory])
            end
    end,
    
    ok.

detect_scalability_regression(Config) ->
    %% Test scaling from 1 to 50 connections
    Results = lists:map(fun(Connections) ->
        TestName = io_lib:format("Scalability regression ~p connections", [Connections]),
        run_concurrent_throughput_benchmark(
            Config, stdio, 1024, 500, Connections, TestName
        )
    end, [1, 10, 50]),
    
    %% Calculate scaling efficiency
    [Result1, Result10, Result50] = Results,
    
    Throughput1 = Result1#benchmark_result.throughput_bps,
    Throughput10 = Result10#benchmark_result.throughput_bps,
    Throughput50 = Result50#benchmark_result.throughput_bps,
    
    Efficiency10 = (Throughput10 / Throughput1) / 10,
    Efficiency50 = (Throughput50 / Throughput1) / 50,
    
    ct:pal("Scaling efficiency: 10x = ~.1f%%, 50x = ~.1f%%", 
           [Efficiency10 * 100, Efficiency50 * 100]),
    
    %% Check for scaling regressions
    TestConfig = proplists:get_value(test_config, Config),
    Baselines = maps:get(baselines, TestConfig),
    
    EfficiencyKey = "scalability_efficiency_50x",
    case maps:get(EfficiencyKey, Baselines, undefined) of
        undefined ->
            ct:pal("No scalability baseline for comparison");
        BaselineEfficiency ->
            if
                Efficiency50 < BaselineEfficiency * (1 - ?THROUGHPUT_REGRESSION_THRESHOLD) ->
                    ct:pal("SCALABILITY REGRESSION: Efficiency dropped from ~.1f%% to ~.1f%%",
                           [BaselineEfficiency * 100, Efficiency50 * 100]),
                    ct:fail("Scalability regression detected");
                true ->
                    ct:pal("Scalability within acceptable range")
            end
    end,
    
    ok.

%%====================================================================
%% Baseline Comparison
%%====================================================================

compare_against_baseline(Config) ->
    TestConfig = proplists:get_value(test_config, Config),
    Baselines = maps:get(baselines, TestConfig),
    
    %% Run comprehensive comparison tests
    ComparisonTests = [
        {stdio, 1024, 5000, "Baseline Comparison STDIO"},
        {tcp, 1024, 2000, "Baseline Comparison TCP"},
        {http, 1024, 500, "Baseline Comparison HTTP"}
    ],
    
    ComparisonResults = lists:map(fun({Transport, MessageSize, MessageCount, TestName}) ->
        Result = run_throughput_benchmark(Config, Transport, MessageSize, MessageCount, TestName),
        
        %% Add baseline comparison
        TestKey = create_baseline_key(Result),
        BaselineComparison = case maps:get(TestKey, Baselines, undefined) of
            undefined -> #{status => no_baseline};
            BaselineValue ->
                CurrentValue = Result#benchmark_result.throughput_bps,
                Change = (CurrentValue - BaselineValue) / BaselineValue,
                #{
                    baseline_value => BaselineValue,
                    current_value => CurrentValue,
                    change_percent => Change * 100,
                    status => if
                        abs(Change) < 0.05 -> stable;
                        Change > 0 -> improved;
                        true -> degraded
                    end
                }
        end,
        
        Result#benchmark_result{baseline_comparison = BaselineComparison}
    end, ComparisonTests),
    
    %% Report comparison results
    lists:foreach(fun(Result) ->
        Comparison = Result#benchmark_result.baseline_comparison,
        case maps:get(status, Comparison) of
            no_baseline ->
                ct:pal("~s: No baseline available", [Result#benchmark_result.test_name]);
            stable ->
                ct:pal("~s: Performance STABLE (~+.1f%%)", 
                       [Result#benchmark_result.test_name, maps:get(change_percent, Comparison)]);
            improved ->
                ct:pal("~s: Performance IMPROVED (+~.1f%%)", 
                       [Result#benchmark_result.test_name, maps:get(change_percent, Comparison)]);
            degraded ->
                ct:pal("~s: Performance DEGRADED (-~.1f%%)", 
                       [Result#benchmark_result.test_name, abs(maps:get(change_percent, Comparison))])
        end
    end, ComparisonResults),
    
    %% Save results
    lists:foreach(fun(Result) ->
        save_benchmark_result(Result, Config)
    end, ComparisonResults),
    
    ok.

update_baseline_metrics(Config) ->
    %% Run fresh baseline tests and update baseline file
    TestConfig = proplists:get_value(test_config, Config),
    
    BaselineTests = [
        {stdio, 128, 10000, "Baseline STDIO Small"},
        {stdio, 1024, 5000, "Baseline STDIO Medium"},
        {stdio, 8192, 1000, "Baseline STDIO Large"},
        {tcp, 128, 5000, "Baseline TCP Small"},
        {tcp, 1024, 2000, "Baseline TCP Medium"},
        {tcp, 8192, 500, "Baseline TCP Large"},
        {http, 128, 1000, "Baseline HTTP Small"},
        {http, 1024, 500, "Baseline HTTP Medium"},
        {http, 8192, 100, "Baseline HTTP Large"}
    ],
    
    NewBaselines = lists:foldl(fun({Transport, MessageSize, MessageCount, TestName}, Acc) ->
        Result = run_throughput_benchmark(Config, Transport, MessageSize, MessageCount, TestName),
        TestKey = create_baseline_key(Result),
        maps:put(TestKey, Result#benchmark_result.throughput_bps, Acc)
    end, #{}, BaselineTests),
    
    %% Add scalability baseline
    ScalabilityResult = run_concurrent_throughput_benchmark(
        Config, stdio, 1024, 500, 50, "Baseline Scalability"
    ),
    SingleConnResult = run_concurrent_throughput_benchmark(
        Config, stdio, 1024, 500, 1, "Baseline Single Connection"
    ),
    
    ScalabilityEfficiency = (ScalabilityResult#benchmark_result.throughput_bps / 
                            SingleConnResult#benchmark_result.throughput_bps) / 50,
    
    FinalBaselines = maps:put("scalability_efficiency_50x", ScalabilityEfficiency, NewBaselines),
    
    %% Save updated baselines
    BaselinesFile = maps:get(baselines_file, TestConfig),
    save_baselines(BaselinesFile, FinalBaselines),
    
    ct:pal("Updated baselines with ~p metrics", [maps:size(FinalBaselines)]),
    
    ok.

generate_trend_analysis(Config) ->
    %% Load historical results and generate trend analysis
    TestConfig = proplists:get_value(test_config, Config),
    ResultsDir = maps:get(results_dir, TestConfig),
    
    %% Find all historical result files
    ResultFiles = filelib:wildcard(filename:join([ResultsDir, "benchmark_*.json"])),
    
    %% Load and analyze trends
    TrendData = lists:foldl(fun(File, Acc) ->
        case load_benchmark_results(File) of
            {ok, Results} ->
                lists:foldl(fun(Result, TAcc) ->
                    TestKey = create_baseline_key(Result),
                    Timestamp = Result#benchmark_result.timestamp,
                    Throughput = Result#benchmark_result.throughput_bps,
                    
                    Current = maps:get(TestKey, TAcc, []),
                    maps:put(TestKey, [{Timestamp, Throughput} | Current], TAcc)
                end, Acc, Results);
            {error, _} ->
                Acc
        end
    end, #{}, ResultFiles),
    
    %% Analyze trends for each test
    TrendAnalysis = maps:map(fun(TestKey, DataPoints) ->
        SortedPoints = lists:sort(DataPoints),
        case length(SortedPoints) of
            N when N < 3 ->
                #{trend => insufficient_data, data_points => N};
            _ ->
                analyze_performance_trend(SortedPoints)
        end
    end, TrendData),
    
    %% Report trends
    maps:foreach(fun(TestKey, Analysis) ->
        case maps:get(trend, Analysis) of
            insufficient_data ->
                ct:pal("~s: Insufficient data for trend analysis", [TestKey]);
            Trend ->
                Slope = maps:get(slope, Analysis, 0),
                Confidence = maps:get(confidence, Analysis, 0),
                ct:pal("~s: ~p trend (slope: ~.2e, confidence: ~.2f)", 
                       [TestKey, Trend, Slope, Confidence])
        end
    end, TrendAnalysis),
    
    %% Save trend analysis
    TrendFile = filename:join(maps:get(results_dir, TestConfig), "trend_analysis.json"),
    save_json_file(TrendFile, TrendAnalysis),
    
    ok.

%%====================================================================
%% Sustained Load Tests
%%====================================================================

sustained_load_stdio_5min(Config) ->
    run_sustained_load_test(Config, stdio, ?SUSTAINED_LOAD_DURATION, "STDIO 5-minute sustained load").

sustained_load_tcp_5min(Config) ->
    run_sustained_load_test(Config, tcp, ?SUSTAINED_LOAD_DURATION, "TCP 5-minute sustained load").

sustained_load_http_5min(Config) ->
    run_sustained_load_test(Config, http, ?SUSTAINED_LOAD_DURATION, "HTTP 5-minute sustained load").

sustained_load_mixed_transport(Config) ->
    %% Run mixed transport sustained load test
    Duration = ?SUSTAINED_LOAD_DURATION,
    
    %% Start concurrent sustained loads
    Processes = [
        spawn_link(fun() -> sustained_load_worker(stdio, Duration div 3, 1024) end),
        spawn_link(fun() -> sustained_load_worker(tcp, Duration div 3, 1024) end),
        spawn_link(fun() -> sustained_load_worker(http, Duration div 6, 1024) end)
    ],
    
    %% Monitor system during mixed load
    StartTime = erlang:system_time(microsecond),
    MonitorPid = spawn_link(fun() -> 
        sustained_load_monitor(Duration, "Mixed Transport Sustained Load")
    end),
    
    %% Wait for completion
    wait_for_processes(Processes),
    EndTime = erlang:system_time(microsecond),
    
    exit(MonitorPid, shutdown),
    
    Duration = EndTime - StartTime,
    
    %% Create summary result
    Result = create_benchmark_result("Mixed Transport Sustained Load", mixed, #{
        duration_us => Duration,
        operations_count => 3, % Number of concurrent transports
        sustained_load => true
    }),
    
    save_benchmark_result(Result, Config),
    
    ok.

%%====================================================================
%% Helper Functions - Benchmark Execution
%%====================================================================

run_throughput_benchmark(Config, Transport, MessageSize, MessageCount, TestName) ->
    ct:pal("Running throughput benchmark: ~s (~p transport, ~p messages, ~p bytes)", 
           [TestName, Transport, MessageCount, MessageSize]),
    
    %% Start OpenTelemetry span
    SpanCtx = otel_tracer:start_span(list_to_binary("throughput_benchmark")),
    otel_tracer:set_current_span(SpanCtx),
    
    otel_tracer:set_attributes([
        {transport_type, Transport},
        {message_size, MessageSize},
        {message_count, MessageCount},
        {test_name, TestName}
    ]),
    
    try
        %% Setup monitoring
        InitialMemory = erlang:memory(total),
        erlang:system_flag(scheduler_wall_time, true),
        InitialCPUStats = erlang:statistics(scheduler_wall_time),
        
        %% Run benchmark
        StartTime = erlang:system_time(microsecond),
        
        {ok, ThroughputBps} = erlmcp_performance_analysis:run_throughput_test(
            Transport, MessageCount, MessageSize
        ),
        
        EndTime = erlang:system_time(microsecond),
        Duration = EndTime - StartTime,
        
        %% Collect metrics
        FinalMemory = erlang:memory(total),
        FinalCPUStats = erlang:statistics(scheduler_wall_time),
        
        MemoryIncrease = FinalMemory - InitialMemory,
        CPUUsage = calculate_cpu_usage(InitialCPUStats, FinalCPUStats),
        
        %% Get latency metrics from last test
        {ok, {P50, P95, P99}} = erlmcp_performance_analysis:run_latency_test(
            Transport, min(100, MessageCount div 10), MessageSize
        ),
        
        otel_tracer:set_attributes([
            {throughput_bps, ThroughputBps},
            {duration_us, Duration},
            {latency_p95_us, P95},
            {memory_increase_bytes, MemoryIncrease},
            {cpu_usage_percent, CPUUsage}
        ]),
        
        Result = #benchmark_result{
            test_name = TestName,
            transport_type = Transport,
            timestamp = StartTime,
            duration_us = Duration,
            throughput_bps = ThroughputBps,
            latency_p50 = P50,
            latency_p95 = P95,
            latency_p99 = P99,
            latency_p999 = P99 + (P99 - P95), % Estimate
            memory_peak_mb = MemoryIncrease / (1024 * 1024),
            cpu_avg_percent = CPUUsage,
            operations_count = MessageCount,
            error_count = 0,
            success_rate = 100.0,
            connections_count = 1,
            message_size = MessageSize,
            regression_detected = false,
            baseline_comparison = #{}
        },
        
        %% Save result
        save_benchmark_result(Result, Config),
        
        ct:pal("Throughput benchmark completed: ~.2f MB/s, P95 latency: ~.2f ms", 
               [ThroughputBps / (1024*1024), P95/1000]),
        
        Result
        
    catch
        error:Reason:Stacktrace ->
            otel_tracer:set_status({error, Reason}),
            ct:pal("Throughput benchmark failed: ~p~nStacktrace: ~p", [Reason, Stacktrace]),
            error({benchmark_failed, Reason})
    after
        otel_tracer:end_span()
    end.

run_latency_benchmark(Config, Transport, MessageSize, MessageCount, TestName) ->
    ct:pal("Running latency benchmark: ~s", [TestName]),
    
    SpanCtx = otel_tracer:start_span(list_to_binary("latency_benchmark")),
    otel_tracer:set_current_span(SpanCtx),
    
    try
        StartTime = erlang:system_time(microsecond),
        
        {ok, {P50, P95, P99}} = erlmcp_performance_analysis:run_latency_test(
            Transport, MessageCount, MessageSize
        ),
        
        EndTime = erlang:system_time(microsecond),
        Duration = EndTime - StartTime,
        
        % Estimate P99.9 from P99 and P95
        P999 = P99 + 2 * (P99 - P95),
        
        Result = #benchmark_result{
            test_name = TestName,
            transport_type = Transport,
            timestamp = StartTime,
            duration_us = Duration,
            throughput_bps = 0.0, % Not measured in latency test
            latency_p50 = P50,
            latency_p95 = P95,
            latency_p99 = P99,
            latency_p999 = P999,
            memory_peak_mb = 0.0,
            cpu_avg_percent = 0.0,
            operations_count = MessageCount,
            error_count = 0,
            success_rate = 100.0,
            connections_count = 1,
            message_size = MessageSize,
            regression_detected = false,
            baseline_comparison = #{}
        },
        
        otel_tracer:set_attributes([
            {latency_p50_us, P50},
            {latency_p95_us, P95},
            {latency_p99_us, P99},
            {latency_p999_us, P999}
        ]),
        
        save_benchmark_result(Result, Config),
        
        ct:pal("Latency benchmark completed - P50: ~.2f ms, P95: ~.2f ms, P99: ~.2f ms", 
               [P50/1000, P95/1000, P99/1000]),
        
        Result
        
    after
        otel_tracer:end_span()
    end.

run_detailed_latency_benchmark(Config, Transport, MessageSize, MessageCount, TestName) ->
    %% Enhanced latency benchmark with detailed percentile analysis
    ct:pal("Running detailed latency benchmark: ~s", [TestName]),
    
    SpanCtx = otel_tracer:start_span(list_to_binary("detailed_latency_benchmark")),
    otel_tracer:set_current_span(SpanCtx),
    
    try
        %% Collect individual latency measurements
        Latencies = collect_individual_latencies(Transport, MessageSize, MessageCount),
        
        %% Calculate detailed percentiles
        SortedLatencies = lists:sort(Latencies),
        P50 = calculate_percentile(SortedLatencies, 50),
        P75 = calculate_percentile(SortedLatencies, 75),
        P90 = calculate_percentile(SortedLatencies, 90),
        P95 = calculate_percentile(SortedLatencies, 95),
        P99 = calculate_percentile(SortedLatencies, 99),
        P999 = calculate_percentile(SortedLatencies, 99.9),
        
        %% Calculate additional metrics
        Mean = lists:sum(SortedLatencies) / length(SortedLatencies),
        Min = lists:min(SortedLatencies),
        Max = lists:max(SortedLatencies),
        
        StartTime = erlang:system_time(microsecond),
        
        Result = #benchmark_result{
            test_name = TestName,
            transport_type = Transport,
            timestamp = StartTime,
            duration_us = 0,
            throughput_bps = 0.0,
            latency_p50 = P50,
            latency_p95 = P95,
            latency_p99 = P99,
            latency_p999 = P999,
            memory_peak_mb = 0.0,
            cpu_avg_percent = 0.0,
            operations_count = MessageCount,
            error_count = 0,
            success_rate = 100.0,
            connections_count = 1,
            message_size = MessageSize,
            regression_detected = false,
            baseline_comparison = #{}
        },
        
        otel_tracer:set_attributes([
            {latency_mean_us, Mean},
            {latency_min_us, Min},
            {latency_max_us, Max},
            {latency_p75_us, P75},
            {latency_p90_us, P90}
        ]),
        
        save_benchmark_result(Result, Config),
        
        ct:pal("Detailed latency benchmark completed - Mean: ~.2f ms, P99.9: ~.2f ms", 
               [Mean/1000, P999/1000]),
        
        Result
        
    after
        otel_tracer:end_span()
    end.

run_concurrent_throughput_benchmark(Config, Transport, MessageSize, MessageCount, Connections, TestName) ->
    ct:pal("Running concurrent throughput benchmark: ~s (~p connections)", [TestName, Connections]),
    
    SpanCtx = otel_tracer:start_span(list_to_binary("concurrent_throughput_benchmark")),
    otel_tracer:set_current_span(SpanCtx),
    
    try
        InitialMemory = erlang:memory(total),
        StartTime = erlang:system_time(microsecond),
        
        {ok, Results} = erlmcp_performance_analysis:run_concurrent_test(
            Transport, Connections, MessageCount, MessageSize
        ),
        
        EndTime = erlang:system_time(microsecond),
        Duration = EndTime - StartTime,
        FinalMemory = erlang:memory(total),
        
        TotalOperations = maps:get(total_operations, Results),
        TotalErrors = maps:get(total_errors, Results),
        Throughput = maps:get(throughput, Results),
        AvgLatency = maps:get(avg_latency_us, Results, 0),
        SuccessRate = maps:get(success_rate, Results),
        
        MemoryIncrease = FinalMemory - InitialMemory,
        
        Result = #benchmark_result{
            test_name = TestName,
            transport_type = Transport,
            timestamp = StartTime,
            duration_us = Duration,
            throughput_bps = Throughput * MessageSize, % Convert ops/s to bytes/s
            latency_p50 = AvgLatency,
            latency_p95 = AvgLatency * 2, % Rough estimate
            latency_p99 = AvgLatency * 3,
            latency_p999 = AvgLatency * 5,
            memory_peak_mb = MemoryIncrease / (1024 * 1024),
            cpu_avg_percent = 0.0,
            operations_count = TotalOperations,
            error_count = TotalErrors,
            success_rate = SuccessRate,
            connections_count = Connections,
            message_size = MessageSize,
            regression_detected = false,
            baseline_comparison = #{}
        },
        
        otel_tracer:set_attributes([
            {concurrent_connections, Connections},
            {total_operations, TotalOperations},
            {success_rate, SuccessRate}
        ]),
        
        save_benchmark_result(Result, Config),
        
        ct:pal("Concurrent benchmark completed: ~.2f ops/s, ~.1f%% success rate", 
               [Throughput, SuccessRate]),
        
        Result
        
    after
        otel_tracer:end_span()
    end.

run_latency_under_load_benchmark(Config, Transport, MessageSize, Connections, TestName) ->
    ct:pal("Running latency under load benchmark: ~s", [TestName]),
    
    %% Start background load
    LoadProcesses = start_background_load(Transport, MessageSize, Connections),
    
    %% Wait for load to stabilize
    timer:sleep(2000),
    
    %% Measure latency under load
    {ok, {P50, P95, P99}} = erlmcp_performance_analysis:run_latency_test(
        Transport, 500, MessageSize
    ),
    
    %% Stop background load
    cleanup_processes(LoadProcesses),
    
    StartTime = erlang:system_time(microsecond),
    
    Result = #benchmark_result{
        test_name = TestName,
        transport_type = Transport,
        timestamp = StartTime,
        duration_us = 0,
        throughput_bps = 0.0,
        latency_p50 = P50,
        latency_p95 = P95,
        latency_p99 = P99,
        latency_p999 = P99 + 2 * (P99 - P95),
        memory_peak_mb = 0.0,
        cpu_avg_percent = 0.0,
        operations_count = 500,
        error_count = 0,
        success_rate = 100.0,
        connections_count = Connections,
        message_size = MessageSize,
        regression_detected = false,
        baseline_comparison = #{}
    },
    
    save_benchmark_result(Result, Config),
    
    ct:pal("Latency under load benchmark completed - P95: ~.2f ms with ~p concurrent connections", 
           [P95/1000, Connections]),
    
    Result.

run_sustained_load_test(Config, Transport, Duration, TestName) ->
    ct:pal("Running sustained load test: ~s (~.1f minutes)", [TestName, Duration/60000]),
    
    SpanCtx = otel_tracer:start_span(list_to_binary("sustained_load_test")),
    otel_tracer:set_current_span(SpanCtx),
    
    try
        StartTime = erlang:system_time(microsecond),
        
        %% Run sustained test
        {ok, SustainedResult} = erlmcp_performance_analysis:run_stress_test(
            Transport, Duration, 5 % 5 concurrent connections
        ),
        
        EndTime = erlang:system_time(microsecond),
        ActualDuration = EndTime - StartTime,
        
        TotalOperations = maps:get(total_operations, SustainedResult),
        TotalErrors = maps:get(total_errors, SustainedResult),
        SuccessRate = maps:get(success_rate, SustainedResult),
        OpsPerSec = maps:get(operations_per_second, SustainedResult),
        
        Result = #benchmark_result{
            test_name = TestName,
            transport_type = Transport,
            timestamp = StartTime,
            duration_us = ActualDuration,
            throughput_bps = OpsPerSec * 1024, % Assume 1KB messages
            latency_p50 = 0,
            latency_p95 = 0,
            latency_p99 = 0,
            latency_p999 = 0,
            memory_peak_mb = 0.0,
            cpu_avg_percent = 0.0,
            operations_count = TotalOperations,
            error_count = TotalErrors,
            success_rate = SuccessRate,
            connections_count = 5,
            message_size = 1024,
            regression_detected = false,
            baseline_comparison = #{}
        },
        
        otel_tracer:set_attributes([
            {sustained_duration_minutes, ActualDuration / 60000000},
            {sustained_ops_per_sec, OpsPerSec},
            {sustained_success_rate, SuccessRate}
        ]),
        
        save_benchmark_result(Result, Config),
        
        ct:pal("Sustained load test completed: ~.2f ops/s over ~.1f minutes, ~.1f%% success", 
               [OpsPerSec, ActualDuration/60000000, SuccessRate]),
        
        %% Verify sustained performance
        ?assert(SuccessRate > 90.0),
        ?assert(OpsPerSec > 10), % Minimum sustainable rate
        
        Result
        
    after
        otel_tracer:end_span()
    end.

%%====================================================================
%% Helper Functions - Data Collection
%%====================================================================

collect_individual_latencies(Transport, MessageSize, Count) ->
    TestData = erlmcp_performance_analysis:generate_test_data(MessageSize),
    {ok, TransportPid, ServerId} = erlmcp_performance_analysis:setup_test_transport(Transport),
    
    try
        Latencies = lists:map(fun(_) ->
            StartTime = erlang:system_time(microsecond),
            
            case Transport of
                stdio -> erlmcp_transport_stdio_new:send(TransportPid, TestData);
                tcp -> erlmcp_transport_tcp:send(TransportPid, TestData);
                http -> gen_server:call(TransportPid, {send, TestData})
            end,
            
            EndTime = erlang:system_time(microsecond),
            EndTime - StartTime
        end, lists:seq(1, Count)),
        
        Latencies
    after
        erlmcp_performance_analysis:cleanup_test_transport(TransportPid, ServerId)
    end.

calculate_percentile([], _) -> 0;
calculate_percentile(SortedList, Percentile) ->
    N = length(SortedList),
    Index = max(1, round(N * Percentile / 100)),
    lists:nth(min(Index, N), SortedList).

calculate_cpu_usage(InitialStats, FinalStats) ->
    case {InitialStats, FinalStats} of
        {undefined, _} -> 0.0;
        {_, undefined} -> 0.0;
        _ ->
            TotalActive = lists:sum([ActiveF - ActiveI || 
                {{_, ActiveI, _}, {_, ActiveF, _}} <- 
                lists:zip(InitialStats, FinalStats)]),
            TotalTime = lists:sum([TotalF - TotalI || 
                {{_, _, TotalI}, {_, _, TotalF}} <- 
                lists:zip(InitialStats, FinalStats)]),
            
            case TotalTime of
                0 -> 0.0;
                _ -> (TotalActive / TotalTime) * 100.0
            end
    end.

monitor_peak_memory_usage(Duration, Baseline) ->
    EndTime = erlang:system_time(millisecond) + Duration,
    monitor_peak_memory_loop(EndTime, Baseline).

monitor_peak_memory_loop(EndTime, PeakMemory) ->
    case erlang:system_time(millisecond) >= EndTime of
        true -> PeakMemory;
        false ->
            CurrentMemory = erlang:memory(total),
            NewPeak = max(PeakMemory, CurrentMemory),
            timer:sleep(100),
            monitor_peak_memory_loop(EndTime, NewPeak)
    end.

count_file_descriptors() ->
    %% Use lsof to count file descriptors (Unix/Linux systems)
    try
        Pid = os:getpid(),
        Cmd = io_lib:format("lsof -p ~s | wc -l", [Pid]),
        Output = string:strip(os:cmd(Cmd), both, $\n),
        list_to_integer(Output) - 1 % Subtract header line
    catch
        _:_ -> 0 % Fallback if lsof not available
    end.

%%====================================================================
%% Helper Functions - Process Management
%%====================================================================

start_concurrent_memory_test(Transport, Connections, MessageSize, Duration) ->
    lists:map(fun(_) ->
        spawn_link(fun() -> 
            memory_test_worker(Transport, MessageSize, Duration)
        end)
    end, lists:seq(1, Connections)).

start_concurrent_workload(Transport, Connections, MessageCount, MessageSize) ->
    lists:map(fun(_) ->
        spawn_link(fun() ->
            workload_worker(Transport, MessageCount, MessageSize)
        end)
    end, lists:seq(1, Connections)).

start_tcp_connections(Count) ->
    lists:map(fun(_) ->
        spawn_link(fun() ->
            tcp_connection_worker()
        end)
    end, lists:seq(1, Count)).

start_background_load(Transport, MessageSize, Connections) ->
    lists:map(fun(_) ->
        spawn_link(fun() ->
            background_load_worker(Transport, MessageSize)
        end)
    end, lists:seq(1, Connections)).

wait_for_processes(Processes) ->
    lists:foreach(fun(Pid) ->
        case is_process_alive(Pid) of
            true ->
                receive
                    {'EXIT', Pid, _} -> ok
                after 10000 ->
                    exit(Pid, kill)
                end;
            false ->
                ok
        end
    end, Processes).

cleanup_processes(Processes) ->
    lists:foreach(fun(Pid) ->
        case is_process_alive(Pid) of
            true -> exit(Pid, shutdown);
            false -> ok
        end
    end, Processes).

memory_test_worker(Transport, MessageSize, Duration) ->
    EndTime = erlang:system_time(millisecond) + Duration,
    TestData = erlmcp_performance_analysis:generate_test_data(MessageSize),
    {ok, TransportPid, ServerId} = erlmcp_performance_analysis:setup_test_transport(Transport),
    
    try
        memory_test_loop(TransportPid, Transport, TestData, EndTime)
    after
        erlmcp_performance_analysis:cleanup_test_transport(TransportPid, ServerId)
    end.

memory_test_loop(TransportPid, Transport, TestData, EndTime) ->
    case erlang:system_time(millisecond) >= EndTime of
        true -> ok;
        false ->
            case Transport of
                stdio -> erlmcp_transport_stdio_new:send(TransportPid, TestData);
                tcp -> erlmcp_transport_tcp:send(TransportPid, TestData);
                http -> gen_server:call(TransportPid, {send, TestData})
            end,
            timer:sleep(10),
            memory_test_loop(TransportPid, Transport, TestData, EndTime)
    end.

workload_worker(Transport, MessageCount, MessageSize) ->
    TestData = erlmcp_performance_analysis:generate_test_data(MessageSize),
    {ok, TransportPid, ServerId} = erlmcp_performance_analysis:setup_test_transport(Transport),
    
    try
        lists:foreach(fun(_) ->
            case Transport of
                stdio -> erlmcp_transport_stdio_new:send(TransportPid, TestData);
                tcp -> erlmcp_transport_tcp:send(TransportPid, TestData);
                http -> gen_server:call(TransportPid, {send, TestData})
            end
        end, lists:seq(1, MessageCount))
    after
        erlmcp_performance_analysis:cleanup_test_transport(TransportPid, ServerId)
    end.

tcp_connection_worker() ->
    %% Simulate TCP connection
    case gen_tcp:connect("127.0.0.1", 8080, [binary, {active, false}], 5000) of
        {ok, Socket} ->
            timer:sleep(1000), % Keep connection alive briefly
            gen_tcp:close(Socket);
        {error, _} ->
            ok % Connection failed, which is expected in test environment
    end.

background_load_worker(Transport, MessageSize) ->
    TestData = erlmcp_performance_analysis:generate_test_data(MessageSize),
    {ok, TransportPid, ServerId} = erlmcp_performance_analysis:setup_test_transport(Transport),
    
    try
        background_load_loop(TransportPid, Transport, TestData)
    after
        erlmcp_performance_analysis:cleanup_test_transport(TransportPid, ServerId)
    end.

background_load_loop(TransportPid, Transport, TestData) ->
    case Transport of
        stdio -> erlmcp_transport_stdio_new:send(TransportPid, TestData);
        tcp -> erlmcp_transport_tcp:send(TransportPid, TestData);
        http -> gen_server:call(TransportPid, {send, TestData})
    end,
    timer:sleep(100), % Continuous but not overwhelming load
    background_load_loop(TransportPid, Transport, TestData).

sustained_load_worker(Transport, Duration, MessageSize) ->
    EndTime = erlang:system_time(millisecond) + Duration,
    TestData = erlmcp_performance_analysis:generate_test_data(MessageSize),
    
    sustained_load_loop(Transport, TestData, EndTime, 0, 0).

sustained_load_loop(Transport, TestData, EndTime, Operations, Errors) ->
    case erlang:system_time(millisecond) >= EndTime of
        true -> 
            ct:pal("Sustained load worker completed: ~p operations, ~p errors", 
                   [Operations, Errors]);
        false ->
            try
                {ok, TransportPid, ServerId} = erlmcp_performance_analysis:setup_test_transport(Transport),
                case Transport of
                    stdio -> erlmcp_transport_stdio_new:send(TransportPid, TestData);
                    tcp -> erlmcp_transport_tcp:send(TransportPid, TestData);
                    http -> gen_server:call(TransportPid, {send, TestData})
                end,
                erlmcp_performance_analysis:cleanup_test_transport(TransportPid, ServerId),
                timer:sleep(50), % Throttle to sustainable rate
                sustained_load_loop(Transport, TestData, EndTime, Operations + 1, Errors)
            catch
                _:_ ->
                    timer:sleep(100), % Back off on errors
                    sustained_load_loop(Transport, TestData, EndTime, Operations, Errors + 1)
            end
    end.

sustained_load_monitor(Duration, TestName) ->
    EndTime = erlang:system_time(millisecond) + Duration,
    
    sustained_monitor_loop(EndTime, TestName, []).

sustained_monitor_loop(EndTime, TestName, Samples) ->
    case erlang:system_time(millisecond) >= EndTime of
        true -> 
            ct:pal("~s monitoring completed with ~p samples", [TestName, length(Samples)]);
        false ->
            Memory = erlang:memory(total),
            Processes = erlang:system_info(process_count),
            
            Sample = {erlang:system_time(millisecond), Memory, Processes},
            timer:sleep(5000), % Sample every 5 seconds
            
            sustained_monitor_loop(EndTime, TestName, [Sample | Samples])
    end.

run_throughput_workload(Transport, MessageSize, MessageCount) ->
    TestData = erlmcp_performance_analysis:generate_test_data(MessageSize),
    {ok, TransportPid, ServerId} = erlmcp_performance_analysis:setup_test_transport(Transport),
    
    try
        lists:foreach(fun(_) ->
            case Transport of
                stdio -> erlmcp_transport_stdio_new:send(TransportPid, TestData);
                tcp -> erlmcp_transport_tcp:send(TransportPid, TestData);
                http -> gen_server:call(TransportPid, {send, TestData})
            end
        end, lists:seq(1, MessageCount))
    after
        erlmcp_performance_analysis:cleanup_test_transport(TransportPid, ServerId)
    end.

run_memory_benchmark(Config, Transport, Connections, MessageSize, TestName) ->
    InitialMemory = erlang:memory(total),
    
    %% Start memory-intensive test
    Processes = start_concurrent_memory_test(Transport, Connections, MessageSize, 10000),
    
    %% Monitor peak memory
    PeakMemory = monitor_peak_memory_usage(12000, InitialMemory),
    
    wait_for_processes(Processes),
    
    MemoryIncrease = PeakMemory - InitialMemory,
    
    create_benchmark_result(TestName, Transport, #{
        connections_count => Connections,
        memory_increase_bytes => MemoryIncrease,
        message_size => MessageSize,
        operations_count => Connections * 100
    }).

setup_and_cleanup_transport(Transport) ->
    {ok, TransportPid, ServerId} = erlmcp_performance_analysis:setup_test_transport(Transport),
    erlmcp_performance_analysis:cleanup_test_transport(TransportPid, ServerId),
    ok.

%%====================================================================
%% Helper Functions - Analysis
%%====================================================================

analyze_concurrent_scaling(Results, Config) ->
    ct:pal("Concurrent scaling analysis:"),
    
    lists:foreach(fun({Connections, Result}) ->
        Throughput = Result#benchmark_result.throughput_bps,
        SuccessRate = Result#benchmark_result.success_rate,
        
        ct:pal("  ~p connections: ~.2f MB/s, ~.1f%% success", 
               [Connections, Throughput / (1024*1024), SuccessRate])
    end, Results).

analyze_message_size_scaling(Results, Config) ->
    ct:pal("Message size scaling analysis:"),
    
    lists:foreach(fun({Size, Result}) ->
        Throughput = Result#benchmark_result.throughput_bps,
        ct:pal("  ~p bytes: ~.2f MB/s", [Size, Throughput / (1024*1024)])
    end, Results).

analyze_memory_scaling(Results, Config) ->
    ct:pal("Memory scaling analysis:"),
    
    lists:foreach(fun(Result) ->
        Connections = Result#benchmark_result.connections_count,
        MemoryMB = Result#benchmark_result.memory_peak_mb,
        MemoryPerConn = MemoryMB / max(1, Connections),
        
        ct:pal("  ~p connections: ~.2f MB total, ~.2f MB per connection", 
               [Connections, MemoryMB, MemoryPerConn])
    end, Results).

find_performance_breakpoint(Results) ->
    %% Sort by connection count
    SortedResults = lists:sort(fun({C1, _}, {C2, _}) -> C1 =< C2 end, Results),
    
    %% Look for significant drops in success rate or throughput
    find_breakpoint_in_results(SortedResults, undefined).

find_breakpoint_in_results([], _) -> no_breakpoint;
find_breakpoint_in_results([{Connections, Result} | Rest], PrevResult) ->
    case PrevResult of
        undefined -> 
            find_breakpoint_in_results(Rest, Result);
        _ ->
            SuccessRate = Result#benchmark_result.success_rate,
            PrevSuccessRate = PrevResult#benchmark_result.success_rate,
            
            Throughput = Result#benchmark_result.throughput_bps,
            PrevThroughput = PrevResult#benchmark_result.throughput_bps,
            
            %% Check for significant degradation
            SuccessDrop = PrevSuccessRate - SuccessRate,
            ThroughputDrop = (PrevThroughput - Throughput) / max(1, PrevThroughput),
            
            if
                SuccessRate < 50.0 ->
                    {breakpoint, Connections, low_success_rate};
                SuccessDrop > 20.0 ->
                    {breakpoint, Connections, success_rate_drop};
                ThroughputDrop > 0.5 ->
                    {breakpoint, Connections, throughput_collapse};
                true ->
                    find_breakpoint_in_results(Rest, Result)
            end
    end.

find_throughput_plateau(Throughputs, MessageSizes) ->
    %% Look for point where throughput stops increasing significantly
    IndexedThroughputs = lists:zip(MessageSizes, Throughputs),
    find_plateau_in_throughputs(IndexedThroughputs, undefined).

find_plateau_in_throughputs([], _) -> no_plateau;
find_plateau_in_throughputs([{Size, Throughput} | Rest], PrevThroughput) ->
    case PrevThroughput of
        undefined ->
            find_plateau_in_throughputs(Rest, Throughput);
        _ ->
            Improvement = (Throughput - PrevThroughput) / PrevThroughput,
            if
                Improvement < 0.1 -> % Less than 10% improvement
                    {plateau, Size, Throughput};
                true ->
                    find_plateau_in_throughputs(Rest, Throughput)
            end
    end.

discover_connection_limit(Config, StartConnections, StepSize, MaxConnections) ->
    discover_connection_limit_loop(Config, StartConnections, StepSize, MaxConnections, []).

discover_connection_limit_loop(Config, Connections, StepSize, MaxConnections, Results) 
  when Connections > MaxConnections ->
    ct:pal("Connection limit discovery completed. Max tested: ~p", [MaxConnections]),
    Results;
discover_connection_limit_loop(Config, Connections, StepSize, MaxConnections, Results) ->
    TestName = io_lib:format("Connection limit test ~p", [Connections]),
    
    try
        Result = run_concurrent_throughput_benchmark(
            Config, stdio, 1024, 100, Connections, TestName
        ),
        
        SuccessRate = Result#benchmark_result.success_rate,
        
        if
            SuccessRate < 10.0 ->
                ct:pal("Connection limit found at ~p connections (~.1f%% success)", 
                       [Connections, SuccessRate]),
                [{limit_reached, Connections} | Results];
            true ->
                discover_connection_limit_loop(
                    Config, Connections + StepSize, StepSize, MaxConnections, 
                    [{Connections, Result} | Results]
                )
        end
    catch
        _:Error ->
            ct:pal("Connection limit reached due to error at ~p connections: ~p", 
                   [Connections, Error]),
            [{error_limit, Connections} | Results]
    end.

analyze_performance_trend(DataPoints) ->
    %% Simple linear regression to detect trends
    N = length(DataPoints),
    {Times, Values} = lists:unzip(DataPoints),
    
    %% Normalize timestamps to avoid large numbers
    MinTime = lists:min(Times),
    NormalizedTimes = [T - MinTime || T <- Times],
    
    %% Calculate means
    MeanTime = lists:sum(NormalizedTimes) / N,
    MeanValue = lists:sum(Values) / N,
    
    %% Calculate slope (correlation coefficient simplified)
    Numerator = lists:sum([
        (T - MeanTime) * (V - MeanValue) || 
        {T, V} <- lists:zip(NormalizedTimes, Values)
    ]),
    Denominator = lists:sum([
        (T - MeanTime) * (T - MeanTime) || 
        T <- NormalizedTimes
    ]),
    
    case Denominator of
        0 -> 
            #{trend => stable, slope => 0, confidence => 0};
        _ ->
            Slope = Numerator / Denominator,
            
            %% Calculate R-squared (simplified)
            PredictedValues = [MeanValue + Slope * (T - MeanTime) || T <- NormalizedTimes],
            SSTot = lists:sum([(V - MeanValue) * (V - MeanValue) || V <- Values]),
            SSRes = lists:sum([
                (V - P) * (V - P) || 
                {V, P} <- lists:zip(Values, PredictedValues)
            ]),
            
            RSquared = case SSTot of
                0 -> 1;
                _ -> 1 - (SSRes / SSTot)
            end,
            
            TrendType = if
                abs(Slope) < MeanValue * 0.01 -> stable; % Less than 1% change per unit time
                Slope > 0 -> improving;
                true -> degrading
            end,
            
            #{
                trend => TrendType,
                slope => Slope,
                confidence => RSquared,
                data_points => N
            }
    end.

%%====================================================================
%% Helper Functions - Regression Detection
%%====================================================================

detect_metric_regression(Results, Baselines, Metric, Threshold) ->
    lists:foldl(fun(Result, Acc) ->
        TestKey = create_baseline_key(Result),
        case maps:get(TestKey, Baselines, undefined) of
            undefined -> Acc;
            BaselineValue ->
                CurrentValue = get_benchmark_metric(Result, Metric),
                Change = (CurrentValue - BaselineValue) / BaselineValue,
                
                if
                    Change > Threshold ->
                        Regression = #{
                            test => Result#benchmark_result.test_name,
                            metric => Metric,
                            baseline => BaselineValue,
                            current => CurrentValue,
                            change_percent => Change * 100
                        },
                        [Regression | Acc];
                    true -> Acc
                end
        end
    end, [], Results).

handle_regression_results([], _MetricName, _Config) ->
    ct:pal("No regressions detected"),
    ok;
handle_regression_results(Regressions, MetricName, Config) ->
    lists:foreach(fun(Reg) ->
        ct:pal("REGRESSION: ~s ~s regression: ~.1f%% change",
               [maps:get(test, Reg), MetricName, maps:get(change_percent, Reg)])
    end, Regressions),
    ct:fail(io_lib:format("~s regressions detected", [MetricName])).

get_benchmark_metric(Result, latency_p95) -> Result#benchmark_result.latency_p95;
get_benchmark_metric(Result, throughput) -> Result#benchmark_result.throughput_bps;
get_benchmark_metric(Result, memory_peak) -> Result#benchmark_result.memory_peak_mb.

%%====================================================================
%% Helper Functions - Data Management
%%====================================================================

create_benchmark_result(TestName, Transport, Metrics) ->
    #benchmark_result{
        test_name = TestName,
        transport_type = Transport,
        timestamp = erlang:system_time(microsecond),
        duration_us = maps:get(duration_us, Metrics, 0),
        throughput_bps = maps:get(throughput_bps, Metrics, 0.0),
        latency_p50 = maps:get(latency_p50, Metrics, 0),
        latency_p95 = maps:get(latency_p95, Metrics, 0),
        latency_p99 = maps:get(latency_p99, Metrics, 0),
        latency_p999 = maps:get(latency_p999, Metrics, 0),
        memory_peak_mb = maps:get(memory_increase_bytes, Metrics, 0) / (1024 * 1024),
        cpu_avg_percent = maps:get(cpu_usage_percent, Metrics, 0.0),
        operations_count = maps:get(operations_count, Metrics, 0),
        error_count = maps:get(error_count, Metrics, 0),
        success_rate = maps:get(success_rate, Metrics, 100.0),
        connections_count = maps:get(connections_count, Metrics, 1),
        message_size = maps:get(message_size, Metrics, 0),
        regression_detected = false,
        baseline_comparison = #{}
    }.

save_benchmark_result(Result, Config) ->
    TestConfig = proplists:get_value(test_config, Config),
    ResultsDir = maps:get(results_dir, TestConfig),
    
    %% Create filename with timestamp
    Timestamp = integer_to_list(Result#benchmark_result.timestamp),
    Filename = "benchmark_" ++ Timestamp ++ ".json",
    FilePath = filename:join(ResultsDir, Filename),
    
    %% Convert result to JSON-serializable format
    ResultMap = benchmark_result_to_map(Result),
    
    save_json_file(FilePath, ResultMap),
    
    %% Also append to daily summary file
    {Date, _} = calendar:now_to_datetime({Result#benchmark_result.timestamp div 1000000, 0, 0}),
    DateStr = io_lib:format("~4..0w-~2..0w-~2..0w", tuple_to_list(Date)),
    SummaryFile = filename:join(ResultsDir, "summary_" ++ DateStr ++ ".json"),
    
    append_to_summary_file(SummaryFile, ResultMap).

save_result_with_regression_flag(Result, Config) ->
    save_benchmark_result(Result#benchmark_result{regression_detected = true}, Config).

create_baseline_key(Result) ->
    TransportStr = atom_to_list(Result#benchmark_result.transport_type),
    SizeStr = integer_to_list(Result#benchmark_result.message_size),
    ConnStr = integer_to_list(Result#benchmark_result.connections_count),
    
    TransportStr ++ "_" ++ SizeStr ++ "_" ++ ConnStr.

benchmark_result_to_map(Result) ->
    #{
        test_name => Result#benchmark_result.test_name,
        transport_type => Result#benchmark_result.transport_type,
        timestamp => Result#benchmark_result.timestamp,
        duration_us => Result#benchmark_result.duration_us,
        throughput_bps => Result#benchmark_result.throughput_bps,
        latency_p50 => Result#benchmark_result.latency_p50,
        latency_p95 => Result#benchmark_result.latency_p95,
        latency_p99 => Result#benchmark_result.latency_p99,
        latency_p999 => Result#benchmark_result.latency_p999,
        memory_peak_mb => Result#benchmark_result.memory_peak_mb,
        cpu_avg_percent => Result#benchmark_result.cpu_avg_percent,
        operations_count => Result#benchmark_result.operations_count,
        error_count => Result#benchmark_result.error_count,
        success_rate => Result#benchmark_result.success_rate,
        connections_count => Result#benchmark_result.connections_count,
        message_size => Result#benchmark_result.message_size,
        regression_detected => Result#benchmark_result.regression_detected,
        baseline_comparison => Result#benchmark_result.baseline_comparison
    }.

load_baselines(FilePath) ->
    case file:read_file(FilePath) of
        {ok, Data} ->
            try
                jsx:decode(Data, [return_maps])
            catch
                _:_ -> #{}
            end;
        {error, _} ->
            ct:pal("No existing baselines found at ~s", [FilePath]),
            #{}
    end.

save_baselines(FilePath, Baselines) ->
    Data = jsx:encode(Baselines),
    file:write_file(FilePath, Data).

load_benchmark_results(FilePath) ->
    case file:read_file(FilePath) of
        {ok, Data} ->
            try
                JsonData = jsx:decode(Data, [return_maps]),
                Results = case is_list(JsonData) of
                    true -> [map_to_benchmark_result(R) || R <- JsonData];
                    false -> [map_to_benchmark_result(JsonData)]
                end,
                {ok, Results}
            catch
                _:Error -> {error, Error}
            end;
        Error -> Error
    end.

map_to_benchmark_result(Map) ->
    #benchmark_result{
        test_name = maps:get(test_name, Map, ""),
        transport_type = maps:get(transport_type, Map, stdio),
        timestamp = maps:get(timestamp, Map, 0),
        duration_us = maps:get(duration_us, Map, 0),
        throughput_bps = maps:get(throughput_bps, Map, 0.0),
        latency_p50 = maps:get(latency_p50, Map, 0),
        latency_p95 = maps:get(latency_p95, Map, 0),
        latency_p99 = maps:get(latency_p99, Map, 0),
        latency_p999 = maps:get(latency_p999, Map, 0),
        memory_peak_mb = maps:get(memory_peak_mb, Map, 0.0),
        cpu_avg_percent = maps:get(cpu_avg_percent, Map, 0.0),
        operations_count = maps:get(operations_count, Map, 0),
        error_count = maps:get(error_count, Map, 0),
        success_rate = maps:get(success_rate, Map, 100.0),
        connections_count = maps:get(connections_count, Map, 1),
        message_size = maps:get(message_size, Map, 0),
        regression_detected = maps:get(regression_detected, Map, false),
        baseline_comparison = maps:get(baseline_comparison, Map, #{})
    }.

save_json_file(FilePath, Data) ->
    JsonData = jsx:encode(Data),
    file:write_file(FilePath, JsonData).

append_to_summary_file(FilePath, ResultMap) ->
    %% Read existing summary
    ExistingSummary = case file:read_file(FilePath) of
        {ok, Data} ->
            try jsx:decode(Data, [return_maps]) catch _:_ -> [] end;
        {error, _} -> []
    end,
    
    %% Append new result
    UpdatedSummary = case is_list(ExistingSummary) of
        true -> [ResultMap | ExistingSummary];
        false -> [ResultMap, ExistingSummary]
    end,
    
    save_json_file(FilePath, UpdatedSummary).

get_metric_value(Result, MetricName, Default) ->
    case MetricName of
        fd_increase -> Default; % Would be stored in custom metrics
        process_increase -> Default;
        gc_count -> Default;
        _ -> Default
    end.

generate_final_report(Config) ->
    TestConfig = proplists:get_value(test_config, Config),
    ResultsDir = maps:get(results_dir, TestConfig),
    
    %% Find all result files from this test run
    TestStartTime = maps:get(test_start_time, TestConfig),
    
    %% Generate comprehensive report
    ReportFile = filename:join(ResultsDir, "benchmark_report.html"),
    
    ReportContent = [
        "<html><head><title>ErlMCP Benchmark Report</title></head><body>",
        "<h1>ErlMCP Performance Benchmark Report</h1>",
        "<p>Generated: ", iso8601_timestamp(), "</p>",
        "<h2>Test Summary</h2>",
        generate_summary_section(Config),
        "<h2>Performance Metrics</h2>",
        generate_metrics_section(Config),
        "<h2>Regression Analysis</h2>",
        generate_regression_section(Config),
        "<h2>Recommendations</h2>",
        generate_recommendations_section(Config),
        "</body></html>"
    ],
    
    file:write_file(ReportFile, iolist_to_binary(ReportContent)),
    
    ct:pal("Final benchmark report generated: ~s", [ReportFile]).

generate_summary_section(_Config) ->
    "<p>Comprehensive performance benchmarking completed for all transport types.</p>".

generate_metrics_section(_Config) ->
    "<p>Detailed metrics collected including throughput, latency percentiles, and resource usage.</p>".

generate_regression_section(_Config) ->
    "<p>Regression analysis performed against historical baselines.</p>".

generate_recommendations_section(_Config) ->
    [
        "<ul>",
        "<li>STDIO transport provides best performance for local communication</li>",
        "<li>TCP transport scales well with concurrent connections</li>", 
        "<li>HTTP transport suitable for request-response patterns</li>",
        "<li>Monitor memory usage during sustained loads</li>",
        "<li>Consider connection pooling for high-throughput scenarios</li>",
        "</ul>"
    ].

iso8601_timestamp() ->
    {{Y, M, D}, {H, Min, S}} = calendar:universal_time(),
    io_lib:format("~4..0w-~2..0w-~2..0wT~2..0w:~2..0w:~2..0wZ", 
                  [Y, M, D, H, Min, S]).