-module(erlmcp_test_dashboard).

%% API exports
-export([
    start/0,
    stop/0,
    generate_html_report/1,
    generate_json_report/1,
    create_live_dashboard/0,
    update_dashboard/1,
    export_metrics/1
]).

%% Internal exports
-export([
    dashboard_server/1,
    metrics_collector/0,
    chart_generator/2
]).

-include_lib("kernel/include/logger.hrl").

-record(dashboard_state, {
    test_results = [],
    live_metrics = #{},
    charts = #{},
    subscriptions = [],
    update_interval = 5000
}).

-record(chart_config, {
    type,           % line, bar, pie, gauge
    title,
    data_source,
    refresh_rate = 30000,
    options = #{}
}).

%%% ============================================================================
%%% API Functions
%%% ============================================================================

start() ->
    Pid = spawn_link(?MODULE, dashboard_server, [#dashboard_state{}]),
    register(erlmcp_dashboard, Pid),
    
    % Start metrics collector
    spawn_link(?MODULE, metrics_collector, []),
    
    ?LOG_INFO("Test Dashboard started"),
    {ok, Pid}.

stop() ->
    case whereis(erlmcp_dashboard) of
        undefined -> ok;
        Pid -> 
            Pid ! stop,
            unregister(erlmcp_dashboard)
    end.

generate_html_report(TestResults) ->
    Html = generate_comprehensive_html(TestResults),
    ReportFile = filename:join([code:priv_dir(erlmcp), "reports", "test_report.html"]),
    
    % Ensure reports directory exists
    filelib:ensure_dir(ReportFile),
    
    case file:write_file(ReportFile, Html) of
        ok -> 
            ?LOG_INFO("HTML report generated: ~s", [ReportFile]),
            {ok, ReportFile};
        {error, Reason} ->
            ?LOG_ERROR("Failed to write HTML report: ~p", [Reason]),
            {error, Reason}
    end.

generate_json_report(TestResults) ->
    JsonData = generate_json_data(TestResults),
    ReportFile = filename:join([code:priv_dir(erlmcp), "reports", "test_report.json"]),
    
    filelib:ensure_dir(ReportFile),
    
    case file:write_file(ReportFile, jsx:encode(JsonData)) of
        ok -> 
            ?LOG_INFO("JSON report generated: ~s", [ReportFile]),
            {ok, ReportFile};
        {error, Reason} ->
            ?LOG_ERROR("Failed to write JSON report: ~p", [Reason]),
            {error, Reason}
    end.

create_live_dashboard() ->
    erlmcp_dashboard ! create_live_dashboard,
    receive
        {dashboard_created, Port} -> {ok, Port};
        {dashboard_error, Reason} -> {error, Reason}
    after 5000 -> {error, timeout}
    end.

update_dashboard(NewResults) ->
    erlmcp_dashboard ! {update_results, NewResults},
    ok.

export_metrics(Format) ->
    erlmcp_dashboard ! {export_metrics, Format, self()},
    receive
        {metrics_exported, Data} -> {ok, Data};
        {export_error, Reason} -> {error, Reason}
    after 5000 -> {error, timeout}
    end.

%%% ============================================================================
%%% Dashboard Server
%%% ============================================================================

dashboard_server(State) ->
    receive
        {update_results, NewResults} ->
            NewState = update_test_results(NewResults, State),
            dashboard_server(NewState);
            
        create_live_dashboard ->
            case start_live_dashboard(State) of
                {ok, Port} -> 
                    self() ! {dashboard_created, Port},
                    dashboard_server(State);
                {error, Reason} ->
                    self() ! {dashboard_error, Reason},
                    dashboard_server(State)
            end;
            
        {export_metrics, Format, From} ->
            case export_dashboard_metrics(Format, State) of
                {ok, Data} -> From ! {metrics_exported, Data};
                {error, Reason} -> From ! {export_error, Reason}
            end,
            dashboard_server(State);
            
        {metrics_update, Metrics} ->
            NewState = State#dashboard_state{live_metrics = Metrics},
            broadcast_to_subscribers(metrics_updated, Metrics, NewState),
            dashboard_server(NewState);
            
        {subscribe, Pid, Type} ->
            NewSubs = [{Pid, Type} | State#dashboard_state.subscriptions],
            NewState = State#dashboard_state{subscriptions = NewSubs},
            dashboard_server(NewState);
            
        {unsubscribe, Pid} ->
            NewSubs = lists:keydelete(Pid, 1, State#dashboard_state.subscriptions),
            NewState = State#dashboard_state{subscriptions = NewSubs},
            dashboard_server(NewState);
            
        refresh_charts ->
            NewCharts = refresh_all_charts(State),
            NewState = State#dashboard_state{charts = NewCharts},
            broadcast_to_subscribers(charts_updated, NewCharts, NewState),
            
            % Schedule next refresh
            timer:send_after(State#dashboard_state.update_interval, refresh_charts),
            dashboard_server(NewState);
            
        stop ->
            ?LOG_INFO("Dashboard server stopping"),
            ok;
            
        _Other ->
            dashboard_server(State)
    end.

%%% ============================================================================
%%% Live Dashboard Implementation
%%% ============================================================================

start_live_dashboard(State) ->
    try
        % Start HTTP server for live dashboard
        Port = 8080,
        
        Dispatch = cowboy_router:compile([
            {'_', [
                {"/", dashboard_handler, State},
                {"/api/metrics", metrics_api_handler, State},
                {"/api/results", results_api_handler, State},
                {"/ws", websocket_handler, State},
                {"/static/[...]", cowboy_static, {priv_dir, erlmcp, "static"}}
            ]}
        ]),
        
        {ok, _} = cowboy:start_clear(dashboard_http_listener,
            [{port, Port}],
            #{env => #{dispatch => Dispatch}}
        ),
        
        ?LOG_INFO("Live dashboard started on port ~p", [Port]),
        {ok, Port}
        
    catch
        Class:Error:Stacktrace ->
            ?LOG_ERROR("Failed to start live dashboard: ~p:~p~n~p", 
                      [Class, Error, Stacktrace]),
            {error, {Class, Error}}
    end.

%%% ============================================================================
%%% HTML Report Generation
%%% ============================================================================

generate_comprehensive_html(TestResults) ->
    [
        html_header(),
        html_navigation(),
        html_overview_section(TestResults),
        html_charts_section(TestResults),
        html_detailed_results_section(TestResults),
        html_trends_section(TestResults),
        html_performance_section(TestResults),
        html_footer()
    ].

html_header() ->
    [
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>ErlMCP Test Dashboard</title>\n",
        "    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css\">\n",
        "    <script src=\"https://cdn.jsdelivr.net/npm/chart.js\"></script>\n",
        "    <style>\n",
        html_custom_styles(),
        "    </style>\n",
        "</head>\n",
        "<body>\n"
    ].

html_custom_styles() ->
    [
        "        .dashboard-header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 2rem 0; }\n",
        "        .metric-card { transition: transform 0.2s; }\n",
        "        .metric-card:hover { transform: translateY(-2px); }\n",
        "        .status-passed { color: #28a745; }\n",
        "        .status-failed { color: #dc3545; }\n",
        "        .status-running { color: #ffc107; }\n",
        "        .chart-container { height: 400px; margin: 2rem 0; }\n",
        "        .results-table { font-size: 0.9rem; }\n",
        "        .trend-positive { color: #28a745; }\n",
        "        .trend-negative { color: #dc3545; }\n",
        "        .loading { display: inline-block; width: 20px; height: 20px; border: 3px solid #f3f3f3; border-top: 3px solid #3498db; border-radius: 50%; animation: spin 1s linear infinite; }\n",
        "        @keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }\n"
    ].

html_navigation() ->
    [
        "<nav class=\"navbar navbar-expand-lg navbar-dark bg-dark\">\n",
        "    <div class=\"container\">\n",
        "        <a class=\"navbar-brand\" href=\"#\">ErlMCP Test Dashboard</a>\n",
        "        <div class=\"navbar-nav ms-auto\">\n",
        "            <a class=\"nav-link\" href=\"#overview\">Overview</a>\n",
        "            <a class=\"nav-link\" href=\"#charts\">Charts</a>\n",
        "            <a class=\"nav-link\" href=\"#results\">Results</a>\n",
        "            <a class=\"nav-link\" href=\"#trends\">Trends</a>\n",
        "        </div>\n",
        "    </div>\n",
        "</nav>\n"
    ].

html_overview_section(TestResults) ->
    Summary = calculate_summary(TestResults),
    
    [
        "<section id=\"overview\" class=\"dashboard-header\">\n",
        "    <div class=\"container\">\n",
        "        <div class=\"row\">\n",
        "            <div class=\"col-md-3\">\n",
        "                <div class=\"text-center\">\n",
        "                    <h2 class=\"display-4\">", integer_to_list(maps:get(total_tests, Summary, 0)), "</h2>\n",
        "                    <p class=\"lead\">Total Tests</p>\n",
        "                </div>\n",
        "            </div>\n",
        "            <div class=\"col-md-3\">\n",
        "                <div class=\"text-center\">\n",
        "                    <h2 class=\"display-4 status-passed\">", integer_to_list(maps:get(passed_tests, Summary, 0)), "</h2>\n",
        "                    <p class=\"lead\">Passed</p>\n",
        "                </div>\n",
        "            </div>\n",
        "            <div class=\"col-md-3\">\n",
        "                <div class=\"text-center\">\n",
        "                    <h2 class=\"display-4 status-failed\">", integer_to_list(maps:get(failed_tests, Summary, 0)), "</h2>\n",
        "                    <p class=\"lead\">Failed</p>\n",
        "                </div>\n",
        "            </div>\n",
        "            <div class=\"col-md-3\">\n",
        "                <div class=\"text-center\">\n",
        "                    <h2 class=\"display-4\">", format_percentage(maps:get(success_rate, Summary, 0)), "%</h2>\n",
        "                    <p class=\"lead\">Success Rate</p>\n",
        "                </div>\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "</section>\n"
    ].

html_charts_section(TestResults) ->
    [
        "<section id=\"charts\" class=\"py-5\">\n",
        "    <div class=\"container\">\n",
        "        <h2 class=\"mb-4\">Performance Metrics</h2>\n",
        "        <div class=\"row\">\n",
        "            <div class=\"col-md-6\">\n",
        "                <div class=\"card\">\n",
        "                    <div class=\"card-header\">\n",
        "                        <h5>Test Success Rate by Suite</h5>\n",
        "                    </div>\n",
        "                    <div class=\"card-body\">\n",
        "                        <canvas id=\"successRateChart\" class=\"chart-container\"></canvas>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "            <div class=\"col-md-6\">\n",
        "                <div class=\"card\">\n",
        "                    <div class=\"card-header\">\n",
        "                        <h5>Execution Time Distribution</h5>\n",
        "                    </div>\n",
        "                    <div class=\"card-body\">\n",
        "                        <canvas id=\"executionTimeChart\" class=\"chart-container\"></canvas>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "        </div>\n",
        "        <div class=\"row mt-4\">\n",
        "            <div class=\"col-12\">\n",
        "                <div class=\"card\">\n",
        "                    <div class=\"card-header\">\n",
        "                        <h5>Performance Trends</h5>\n",
        "                    </div>\n",
        "                    <div class=\"card-body\">\n",
        "                        <canvas id=\"trendsChart\" class=\"chart-container\"></canvas>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "</section>\n",
        generate_chart_scripts(TestResults)
    ].

html_detailed_results_section(TestResults) ->
    [
        "<section id=\"results\" class=\"py-5 bg-light\">\n",
        "    <div class=\"container\">\n",
        "        <h2 class=\"mb-4\">Detailed Test Results</h2>\n",
        "        <div class=\"table-responsive\">\n",
        "            <table class=\"table table-striped results-table\">\n",
        "                <thead class=\"table-dark\">\n",
        "                    <tr>\n",
        "                        <th>Suite</th>\n",
        "                        <th>Test Name</th>\n",
        "                        <th>Status</th>\n",
        "                        <th>Duration (ms)</th>\n",
        "                        <th>Timestamp</th>\n",
        "                        <th>Details</th>\n",
        "                    </tr>\n",
        "                </thead>\n",
        "                <tbody>\n",
        generate_results_table_rows(TestResults),
        "                </tbody>\n",
        "            </table>\n",
        "        </div>\n",
        "    </div>\n",
        "</section>\n"
    ].

html_trends_section(TestResults) ->
    TrendsData = analyze_test_trends(TestResults),
    
    [
        "<section id=\"trends\" class=\"py-5\">\n",
        "    <div class=\"container\">\n",
        "        <h2 class=\"mb-4\">Trend Analysis</h2>\n",
        "        <div class=\"row\">\n",
        generate_trend_cards(TrendsData),
        "        </div>\n",
        "    </div>\n",
        "</section>\n"
    ].

html_performance_section(TestResults) ->
    PerfData = analyze_performance_metrics(TestResults),
    
    [
        "<section id=\"performance\" class=\"py-5 bg-light\">\n",
        "    <div class=\"container\">\n",
        "        <h2 class=\"mb-4\">Performance Analysis</h2>\n",
        "        <div class=\"row\">\n",
        generate_performance_cards(PerfData),
        "        </div>\n",
        "    </div>\n",
        "</section>\n"
    ].

html_footer() ->
    [
        "    <footer class=\"bg-dark text-light py-3\">\n",
        "        <div class=\"container text-center\">\n",
        "            <p>&copy; 2024 ErlMCP Test Dashboard. Generated at ", format_timestamp(calendar:universal_time()), "</p>\n",
        "        </div>\n",
        "    </footer>\n",
        "    <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js\"></script>\n",
        "</body>\n",
        "</html>\n"
    ].

%%% ============================================================================
%%% Chart Generation
%%% ============================================================================

generate_chart_scripts(TestResults) ->
    SuccessRateData = prepare_success_rate_data(TestResults),
    ExecutionTimeData = prepare_execution_time_data(TestResults),
    TrendsData = prepare_trends_data(TestResults),
    
    [
        "<script>\n",
        "// Success Rate Chart\n",
        "const successRateCtx = document.getElementById('successRateChart').getContext('2d');\n",
        "new Chart(successRateCtx, {\n",
        "    type: 'doughnut',\n",
        "    data: ", jsx:encode(SuccessRateData), ",\n",
        "    options: {\n",
        "        responsive: true,\n",
        "        maintainAspectRatio: false,\n",
        "        plugins: {\n",
        "            legend: { position: 'bottom' }\n",
        "        }\n",
        "    }\n",
        "});\n\n",
        
        "// Execution Time Chart\n",
        "const executionTimeCtx = document.getElementById('executionTimeChart').getContext('2d');\n",
        "new Chart(executionTimeCtx, {\n",
        "    type: 'bar',\n",
        "    data: ", jsx:encode(ExecutionTimeData), ",\n",
        "    options: {\n",
        "        responsive: true,\n",
        "        maintainAspectRatio: false,\n",
        "        scales: {\n",
        "            y: { beginAtZero: true }\n",
        "        }\n",
        "    }\n",
        "});\n\n",
        
        "// Trends Chart\n",
        "const trendsCtx = document.getElementById('trendsChart').getContext('2d');\n",
        "new Chart(trendsCtx, {\n",
        "    type: 'line',\n",
        "    data: ", jsx:encode(TrendsData), ",\n",
        "    options: {\n",
        "        responsive: true,\n",
        "        maintainAspectRatio: false,\n",
        "        scales: {\n",
        "            x: { type: 'time' },\n",
        "            y: { beginAtZero: true }\n",
        "        }\n",
        "    }\n",
        "});\n",
        "</script>\n"
    ].

prepare_success_rate_data(TestResults) ->
    SuiteSummaries = group_results_by_suite(TestResults),
    
    Labels = maps:keys(SuiteSummaries),
    Data = [maps:get(success_rate, Summary, 0) * 100 || Summary <- maps:values(SuiteSummaries)],
    
    #{
        labels => Labels,
        datasets => [#{
            data => Data,
            backgroundColor => [
                <<"#FF6384">>,
                <<"#36A2EB">>,
                <<"#FFCE56">>,
                <<"#4BC0C0">>,
                <<"#9966FF">>,
                <<"#FF9F40">>
            ]
        }]
    }.

prepare_execution_time_data(TestResults) ->
    SuiteSummaries = group_results_by_suite(TestResults),
    
    Labels = maps:keys(SuiteSummaries),
    Data = [maps:get(avg_duration, Summary, 0) || Summary <- maps:values(SuiteSummaries)],
    
    #{
        labels => Labels,
        datasets => [#{
            label => <<"Average Execution Time (ms)">>,
            data => Data,
            backgroundColor => <<"#36A2EB">>,
            borderColor => <<"#36A2EB">>,
            borderWidth => 1
        }]
    }.

prepare_trends_data(_TestResults) ->
    % Placeholder for trends data - would typically come from historical data
    #{
        labels => [<<"Week 1">>, <<"Week 2">>, <<"Week 3">>, <<"Week 4">>],
        datasets => [#{
            label => <<"Success Rate %">>,
            data => [92, 95, 89, 96],
            borderColor => <<"#28a745">>,
            tension => 0.4
        }]
    }.

%%% ============================================================================
%%% JSON Report Generation
%%% ============================================================================

generate_json_data(TestResults) ->
    Summary = calculate_summary(TestResults),
    SuiteSummaries = group_results_by_suite(TestResults),
    Trends = analyze_test_trends(TestResults),
    Performance = analyze_performance_metrics(TestResults),
    
    #{
        report_info => #{
            generated_at => calendar:universal_time(),
            total_results => length(TestResults),
            report_version => <<"1.0">>
        },
        summary => Summary,
        suite_summaries => SuiteSummaries,
        trends => Trends,
        performance => Performance,
        detailed_results => [format_result_for_json(R) || R <- TestResults],
        metadata => #{
            system_info => get_system_info(),
            test_environment => get_test_environment()
        }
    }.

format_result_for_json(Result) when is_record(Result, test_result) ->
    #{
        suite => Result#test_result.suite,
        test_name => Result#test_result.test_name,
        status => Result#test_result.status,
        duration => Result#test_result.duration,
        timestamp => Result#test_result.timestamp,
        details => Result#test_result.details
    };
format_result_for_json(Result) when is_map(Result) ->
    Result.

%%% ============================================================================
%%% Metrics Collection
%%% ============================================================================

metrics_collector() ->
    timer:sleep(5000), % Initial delay
    
    Metrics = collect_current_metrics(),
    erlmcp_dashboard ! {metrics_update, Metrics},
    
    timer:sleep(30000), % Update every 30 seconds
    metrics_collector().

collect_current_metrics() ->
    #{
        timestamp => calendar:universal_time(),
        system => #{
            memory_usage => erlang:memory(),
            process_count => erlang:system_info(process_count),
            run_queue => erlang:statistics(run_queue),
            scheduler_utilization => erlang:statistics(scheduler_wall_time)
        },
        test_execution => #{
            active_tests => count_active_tests(),
            completed_tests_today => count_completed_tests_today(),
            avg_test_duration => calculate_avg_test_duration_today()
        }
    }.

count_active_tests() ->
    % Placeholder - would query active test processes
    0.

count_completed_tests_today() ->
    % Placeholder - would query test results from today
    0.

calculate_avg_test_duration_today() ->
    % Placeholder - would calculate from today's results
    0.0.

%%% ============================================================================
%%% Utility Functions
%%% ============================================================================

calculate_summary(TestResults) ->
    ValidResults = [R || R <- TestResults, is_record(R, test_result)],
    Total = length(ValidResults),
    
    case Total of
        0 ->
            #{
                total_tests => 0,
                passed_tests => 0,
                failed_tests => 0,
                success_rate => 0,
                avg_duration => 0
            };
        _ ->
            Passed = length([R || R <- ValidResults, R#test_result.status =:= passed]),
            Failed = Total - Passed,
            SuccessRate = Passed / Total,
            
            Durations = [R#test_result.duration || R <- ValidResults, 
                        is_number(R#test_result.duration)],
            AvgDuration = case Durations of
                [] -> 0;
                _ -> lists:sum(Durations) / length(Durations)
            end,
            
            #{
                total_tests => Total,
                passed_tests => Passed,
                failed_tests => Failed,
                success_rate => SuccessRate,
                avg_duration => AvgDuration
            }
    end.

group_results_by_suite(TestResults) ->
    ValidResults = [R || R <- TestResults, is_record(R, test_result)],
    
    GroupedResults = lists:foldl(fun(Result, Acc) ->
        Suite = Result#test_result.suite,
        SuiteResults = maps:get(Suite, Acc, []),
        maps:put(Suite, [Result | SuiteResults], Acc)
    end, #{}, ValidResults),
    
    maps:map(fun(_Suite, Results) ->
        calculate_summary(Results)
    end, GroupedResults).

analyze_test_trends(_TestResults) ->
    % Placeholder for trend analysis
    #{
        overall_trend => stable,
        success_rate_trend => #{direction => stable, change => 0},
        performance_trend => #{direction => improving, change => 5.2},
        reliability_trend => #{direction => stable, change => 0}
    }.

analyze_performance_metrics(_TestResults) ->
    % Placeholder for performance analysis
    #{
        throughput => #{current => 10000, target => 15000, status => below_target},
        latency => #{avg => 2.5, p95 => 5.0, p99 => 10.0, status => good},
        resource_usage => #{cpu => 45, memory => 256, status => optimal},
        bottlenecks => []
    }.

generate_results_table_rows(TestResults) ->
    ValidResults = [R || R <- TestResults, is_record(R, test_result)],
    
    [generate_result_row(Result) || Result <- ValidResults].

generate_result_row(Result) ->
    Status = atom_to_list(Result#test_result.status),
    StatusClass = case Result#test_result.status of
        passed -> "status-passed";
        failed -> "status-failed";
        _ -> "status-running"
    end,
    
    [
        "                    <tr>\n",
        "                        <td>", atom_to_list(Result#test_result.suite), "</td>\n",
        "                        <td>", atom_to_list(Result#test_result.test_name), "</td>\n",
        "                        <td><span class=\"", StatusClass, "\">", Status, "</span></td>\n",
        "                        <td>", format_duration(Result#test_result.duration), "</td>\n",
        "                        <td>", format_timestamp(Result#test_result.timestamp), "</td>\n",
        "                        <td>", format_details(Result#test_result.details), "</td>\n",
        "                    </tr>\n"
    ].

generate_trend_cards(TrendsData) ->
    [
        "            <div class=\"col-md-4\">\n",
        "                <div class=\"card metric-card\">\n",
        "                    <div class=\"card-body\">\n",
        "                        <h5>Success Rate Trend</h5>\n",
        "                        <p class=\"trend-positive\">Stable</p>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "            <div class=\"col-md-4\">\n",
        "                <div class=\"card metric-card\">\n",
        "                    <div class=\"card-body\">\n",
        "                        <h5>Performance Trend</h5>\n",
        "                        <p class=\"trend-positive\">Improving +5.2%</p>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "            <div class=\"col-md-4\">\n",
        "                <div class=\"card metric-card\">\n",
        "                    <div class=\"card-body\">\n",
        "                        <h5>Reliability Trend</h5>\n",
        "                        <p class=\"trend-positive\">Stable</p>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n"
    ].

generate_performance_cards(PerfData) ->
    [
        "            <div class=\"col-md-4\">\n",
        "                <div class=\"card metric-card\">\n",
        "                    <div class=\"card-body\">\n",
        "                        <h5>Throughput</h5>\n",
        "                        <p>10,000 msg/sec (Target: 15,000)</p>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "            <div class=\"col-md-4\">\n",
        "                <div class=\"card metric-card\">\n",
        "                    <div class=\"card-body\">\n",
        "                        <h5>Latency</h5>\n",
        "                        <p>Avg: 2.5ms, P95: 5.0ms</p>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "            <div class=\"col-md-4\">\n",
        "                <div class=\"card metric-card\">\n",
        "                    <div class=\"card-body\">\n",
        "                        <h5>Resource Usage</h5>\n",
        "                        <p>CPU: 45%, Memory: 256MB</p>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n"
    ].

update_test_results(NewResults, State) ->
    AllResults = NewResults ++ State#dashboard_state.test_results,
    State#dashboard_state{test_results = AllResults}.

refresh_all_charts(State) ->
    % Refresh chart data based on current test results
    TestResults = State#dashboard_state.test_results,
    
    #{
        success_rate => prepare_success_rate_data(TestResults),
        execution_time => prepare_execution_time_data(TestResults),
        trends => prepare_trends_data(TestResults)
    }.

broadcast_to_subscribers(Event, Data, State) ->
    Subscribers = State#dashboard_state.subscriptions,
    
    lists:foreach(fun({Pid, Type}) ->
        case Type of
            all -> Pid ! {dashboard_event, Event, Data};
            EventType when EventType =:= Event -> Pid ! {dashboard_event, Event, Data};
            _ -> ok
        end
    end, Subscribers).

export_dashboard_metrics(Format, State) ->
    TestResults = State#dashboard_state.test_results,
    LiveMetrics = State#dashboard_state.live_metrics,
    
    case Format of
        json ->
            Data = #{
                test_results => [format_result_for_json(R) || R <- TestResults],
                live_metrics => LiveMetrics,
                summary => calculate_summary(TestResults),
                exported_at => calendar:universal_time()
            },
            {ok, jsx:encode(Data)};
        csv ->
            {ok, export_to_csv(TestResults)};
        _ ->
            {error, unsupported_format}
    end.

export_to_csv(TestResults) ->
    Header = "Suite,Test,Status,Duration,Timestamp\n",
    Rows = [format_result_as_csv(R) || R <- TestResults, is_record(R, test_result)],
    [Header | Rows].

format_result_as_csv(Result) ->
    io_lib:format("~s,~s,~s,~.2f,~s~n", [
        atom_to_list(Result#test_result.suite),
        atom_to_list(Result#test_result.test_name),
        atom_to_list(Result#test_result.status),
        Result#test_result.duration,
        format_timestamp(Result#test_result.timestamp)
    ]).

format_duration(Duration) when is_number(Duration) ->
    io_lib:format("~.2f", [Duration]);
format_duration(_) ->
    "N/A".

format_percentage(Rate) when is_number(Rate) ->
    io_lib:format("~.1f", [Rate * 100]);
format_percentage(_) ->
    "0.0".

format_timestamp({{Y, M, D}, {H, Min, S}}) ->
    io_lib:format("~4..0w-~2..0w-~2..0w ~2..0w:~2..0w:~2..0w", [Y, M, D, H, Min, S]);
format_timestamp(_) ->
    "Unknown".

format_details(Details) when is_map(Details) ->
    jsx:encode(Details);
format_details(Details) when is_list(Details) ->
    io_lib:format("~p", [Details]);
format_details(Details) ->
    io_lib:format("~p", [Details]).

get_system_info() ->
    #{
        erlang_version => erlang:system_info(version),
        otp_release => erlang:system_info(otp_release),
        system_architecture => erlang:system_info(system_architecture),
        schedulers => erlang:system_info(schedulers),
        memory => erlang:memory()
    }.

get_test_environment() ->
    #{
        node => node(),
        started_at => calendar:universal_time(),
        environment => "test"
    }.