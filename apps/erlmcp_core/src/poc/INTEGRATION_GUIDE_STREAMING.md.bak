# Streaming Tool Results - Integration Guide

## Overview

This guide shows how to integrate the streaming POC patterns into erlmcp core for production use.

## Integration Strategy

### Phase 1: Core Infrastructure (Week 1-2)

#### 1.1 Update Type Definitions in erlmcp.hrl

```erlang
%% Add streaming-related types
-type stream_ref() :: reference().
-type chunk_callback() :: fun((chunk_data()) -> ok).
-type chunk_data() :: map().

%% Extend tool handler type
-type tool_handler() ::
    fun((map()) -> tool_result()) |                    % Blocking handler (existing)
    fun((map(), chunk_callback()) -> tool_result()).   % Streaming handler (new)
```

#### 1.2 Create erlmcp_streaming.erl - Core Streaming Module

This module follows the POC pattern but production-ready:
- Proper supervision
- Configurable backpressure strategies
- Flow control with acks
- Stream lifecycle management
- Metrics integration

#### 1.3 Add Streaming to Supervision Tree

Add erlmcp_streaming to erlmcp_core_sup as a permanent worker.

### Phase 2: Server Integration (Week 2-3)

Enhance erlmcp_server to support streaming tools:
- Detect handler arity (1 = blocking, 2 = streaming)
- Execute streaming handlers with chunk callback
- Forward chunks to transport layer
- Signal completion

### Phase 3: Client Integration (Week 3-4)

Enhance erlmcp_client to consume streaming results:
- Callback-based API
- Process-based subscription API
- Handle stream control messages

### Phase 4: Transport Integration (Week 4-5)

Update all transports to support streaming:
- WebSocket: Native bidirectional streaming
- SSE: Server->client streaming
- HTTP: Chunked transfer encoding
- TCP: Length-prefixed frames
- STDIO: Line-delimited JSON

### Phase 5: Testing (Week 5-6)

Comprehensive test coverage:
- Unit tests (≥80% coverage)
- Integration tests
- Performance benchmarks
- Chaos engineering tests

## Success Criteria

- ✅ All existing tests pass (100%)
- ✅ New streaming tests pass (≥80% coverage)
- ✅ Performance benchmarks show <10% regression
- ✅ Latency P95 < 1ms for streaming chunks
- ✅ Support ≥1000 concurrent streams
- ✅ Zero breaking changes to existing API
