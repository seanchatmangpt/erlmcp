%%%====================================================================
%%% tcp_real_bench.erl - Real TCP Transport Benchmark with Actual Sockets
%%%====================================================================
%%%
%%% PROOF-OF-CONCEPT for "100K connections" claims.
%%%
%%% This benchmark uses REAL TCP sockets (ranch/gen_tcp) to measure:
%%% - Throughput (msg/sec) across N concurrent connections
%%% - Latency percentiles (p50/p95/p99 in microseconds)
%%% - Resource usage (CPU%, RSS memory, heap per connection)
%%% - Network bandwidth (MiB/sec)
%%%
%%% Workloads:
%%% - small_burst: 100 connections, 1KB messages, 60s
%%% - sustained_10k: 10,000 connections, 1KB messages, 300s
%%% - max_capacity_100k: 100,000 connections, 1KB messages, 1800s
%%%
%%% JSON Output Format (full metrology):
%%% {
%%%   "workload_id": "tcp_sustained_10k_1kib",
%%%   "transport": "tcp",
%%%   "tls_enabled": false,
%%%   "connections": 10000,
%%%   "duration_s": 300,
%%%   "messages_sent": 5000000,
%%%   "throughput_msg_per_s": 16666.67,
%%%   "latency_p50_us": 450,
%%%   "latency_p95_us": 1200,
%%%   "latency_p99_us": 2500,
%%%   "bandwidth_mib_per_s": 16.3,
%%%   "cpu_percent_per_node": 45.2,
%%%   "memory_rss_mib_per_node": 2048,
%%%   "memory_heap_mib_per_conn": 0.05,
%%%   "precision": "microsecond",
%%%   "scope": "per_node",
%%%   "environment": "local_dev"
%%% }
%%%
%%% Usage:
%%%   rebar3 shell
%%%   tcp_real_bench:run_workload(small_burst).
%%%   tcp_real_bench:run_workload(sustained_10k).
%%%   tcp_real_bench:run_all().
%%%
%%%====================================================================

-module(tcp_real_bench).

-export([
    run_all/0,
    run_workload/1,
    run_workload/2,
    benchmark_tcp/1,
    validate_and_save/2,
    metrics_to_json/1,
    validate_metrics/1,
    percentile/2
]).

%% Internal exports
-export([client_loop/5, server_stats_loop/3]).

-record(workload, {
    id :: binary(),
    connections :: pos_integer(),
    duration_s :: pos_integer(),
    payload_size_bytes :: pos_integer(),
    tls_enabled :: boolean(),
    compression :: boolean(),
    target_msg_per_s :: pos_integer()
}).

-record(metrics, {
    workload_id :: binary(),
    transport = <<"tcp">> :: binary(),
    tls_enabled :: boolean(),
    connections :: pos_integer(),
    duration_s :: pos_integer(),
    messages_sent :: non_neg_integer(),
    throughput_msg_per_s :: float(),
    latency_p50_us :: float(),
    latency_p95_us :: float(),
    latency_p99_us :: float(),
    bandwidth_mib_per_s :: float(),
    cpu_percent_per_node :: float(),
    memory_rss_mib_per_node :: float(),
    memory_heap_mib_per_conn :: float(),
    precision = <<"microsecond">> :: binary(),
    scope = <<"per_node">> :: binary(),
    environment :: binary(),
    timestamp :: integer(),
    error_count :: non_neg_integer(),
    actual_duration_s :: float()
}).

%%====================================================================
%% Workload Definitions
%%====================================================================

workload_definitions() ->
    #{
        small_burst => #workload{
            id = <<"tcp_small_burst_1kib">>,
            connections = 100,
            duration_s = 60,
            payload_size_bytes = 1024,
            tls_enabled = false,
            compression = false,
            target_msg_per_s = 50000
        },
        sustained_10k => #workload{
            id = <<"tcp_sustained_10k_1kib">>,
            connections = 10000,
            duration_s = 300,
            payload_size_bytes = 1024,
            tls_enabled = false,
            compression = false,
            target_msg_per_s = 100000
        },
        sustained_10k_10kib => #workload{
            id = <<"tcp_sustained_10k_10kib">>,
            connections = 10000,
            duration_s = 300,
            payload_size_bytes = 10240,
            tls_enabled = false,
            compression = false,
            target_msg_per_s = 50000
        },
        sustained_10k_100kib => #workload{
            id = <<"tcp_sustained_10k_100kib">>,
            connections = 10000,
            duration_s = 300,
            payload_size_bytes = 102400,
            tls_enabled = false,
            compression = false,
            target_msg_per_s = 10000
        },
        max_capacity_100k => #workload{
            id = <<"tcp_max_capacity_100k_1kib">>,
            connections = 100000,
            duration_s = 1800,
            payload_size_bytes = 1024,
            tls_enabled = false,
            compression = false,
            target_msg_per_s = 200000
        }
    }.

%%====================================================================
%% Public API
%%====================================================================

%% @doc Run all workload benchmarks
-spec run_all() -> ok.
run_all() ->
    io:format("~n========================================~n"),
    io:format("TCP REAL TRANSPORT BENCHMARK SUITE~n"),
    io:format("========================================~n~n"),

    Workloads = [small_burst, sustained_10k, sustained_10k_10kib],

    Results = lists:map(fun(Workload) ->
        try
            run_workload(Workload)
        catch
            Class:Reason:Stack ->
                io:format("ERROR in ~p: ~p:~p~n~p~n", [Workload, Class, Reason, Stack]),
                {error, Workload, Reason}
        end
    end, Workloads),

    %% Print summary
    io:format("~n========================================~n"),
    io:format("BENCHMARK SUITE SUMMARY~n"),
    io:format("========================================~n"),
    lists:foreach(fun
        ({ok, Metrics}) ->
            Status = case Metrics#metrics.throughput_msg_per_s >=
                          get_target(Metrics#metrics.workload_id) of
                true -> "PASS";
                false -> "FAIL"
            end,
            io:format("~s: ~s (~.2f msg/s)~n",
                     [Metrics#metrics.workload_id, Status,
                      Metrics#metrics.throughput_msg_per_s]);
        ({error, Workload, Reason}) ->
            io:format("~p: ERROR (~p)~n", [Workload, Reason])
    end, Results),

    ok.

%% @doc Run a specific workload
-spec run_workload(atom()) -> {ok, #metrics{}} | {error, term()}.
run_workload(WorkloadName) ->
    run_workload(WorkloadName, #{}).

-spec run_workload(atom(), map()) -> {ok, #metrics{}} | {error, term()}.
run_workload(WorkloadName, Opts) ->
    Definitions = workload_definitions(),
    case maps:get(WorkloadName, Definitions, undefined) of
        undefined ->
            {error, {unknown_workload, WorkloadName}};
        Workload ->
            %% Override workload with options
            FinalWorkload = apply_options(Workload, Opts),

            io:format("~n========================================~n"),
            io:format("Running: ~s~n", [FinalWorkload#workload.id]),
            io:format("Connections: ~B~n", [FinalWorkload#workload.connections]),
            io:format("Duration: ~B seconds~n", [FinalWorkload#workload.duration_s]),
            io:format("Payload: ~B bytes~n", [FinalWorkload#workload.payload_size_bytes]),
            io:format("========================================~n~n"),

            Result = benchmark_tcp(FinalWorkload),

            case Result of
                {ok, Metrics} ->
                    validate_and_save(Metrics, WorkloadName),
                    {ok, Metrics};
                Error ->
                    Error
            end
    end.

%%====================================================================
%% Core Benchmark Implementation
%%====================================================================

%% @doc Benchmark TCP transport with real sockets
-spec benchmark_tcp(#workload{}) -> {ok, #metrics{}} | {error, term()}.
benchmark_tcp(Workload) ->
    %% Ensure erlmcp application is started
    application:ensure_all_started(erlmcp),

    %% Start TCP server using ranch via erlmcp_transport_tcp
    {ok, ServerPid} = erlmcp_transport_tcp:start_server(#{
        mode => server,
        port => 0,  % Let OS assign port
        server_id => bench_server,
        transport_id => bench_transport,
        max_connections => Workload#workload.connections * 2,
        num_acceptors => min(1000, Workload#workload.connections div 10)
    }),

    %% Get actual port
    {ok, ServerState} = gen_server:call(ServerPid, get_state),
    Port = element(8, ServerState),  % #state.port

    io:format("Server started on port ~B~n", [Port]),

    %% Create payload
    Payload = create_json_rpc_payload(Workload#workload.payload_size_bytes),

    %% Capture initial system metrics
    InitialMemory = erlang:memory(total),
    _InitialProcesses = erlang:system_info(process_count),
    {InitialGCCount, _, _} = erlang:statistics(garbage_collection),
    WallClockStart = erlang:system_time(millisecond),

    io:format("Starting ~B client connections...~n", [Workload#workload.connections]),

    %% Start client connections
    ClientPids = start_clients(Workload#workload.connections, Port, Payload),

    io:format("~B clients connected~n", [length(ClientPids)]),
    timer:sleep(1000),  % Let connections stabilize

    %% Start metrics collection
    MetricsCollectorPid = spawn_link(?MODULE, server_stats_loop,
                                     [self(), WallClockStart, []]),

    %% Calculate end time
    DurationMs = Workload#workload.duration_s * 1000,
    EndTime = erlang:monotonic_time(millisecond) + DurationMs,

    io:format("Benchmark running for ~B seconds...~n", [Workload#workload.duration_s]),

    %% Signal clients to start sending
    StartTime = erlang:monotonic_time(millisecond),
    lists:foreach(fun(Pid) -> Pid ! {start_benchmark, EndTime} end, ClientPids),

    %% Wait for duration
    timer:sleep(DurationMs),

    %% Stop clients and collect results
    ActualEndTime = erlang:monotonic_time(millisecond),
    ActualDurationMs = ActualEndTime - StartTime,

    io:format("Collecting results from ~B clients...~n", [length(ClientPids)]),

    ClientResults = collect_client_results(ClientPids, []),

    %% Stop metrics collector
    MetricsCollectorPid ! {stop, self()},
    _SystemMetrics = receive
        {system_metrics, SysMetrics} -> SysMetrics
    after 5000 ->
        []
    end,

    %% Cleanup
    lists:foreach(fun(Pid) ->
        catch exit(Pid, shutdown)
    end, ClientPids),
    gen_server:stop(ServerPid),

    %% Calculate final system metrics
    FinalMemory = erlang:memory(total),
    _FinalProcesses = erlang:system_info(process_count),
    {FinalGCCount, _, _} = erlang:statistics(garbage_collection),

    %% Aggregate results
    TotalMessages = lists:sum([Msgs || {Msgs, _} <- ClientResults]),
    AllLatencies = lists:flatten([Lats || {_, Lats} <- ClientResults]),
    ErrorCount = count_errors(ClientResults),

    %% Calculate statistics
    SortedLatencies = lists:sort([L || L <- AllLatencies, is_number(L)]),

    FinalMetrics = case SortedLatencies of
        [] ->
            #metrics{
                workload_id = Workload#workload.id,
                tls_enabled = Workload#workload.tls_enabled,
                connections = Workload#workload.connections,
                duration_s = Workload#workload.duration_s,
                messages_sent = 0,
                throughput_msg_per_s = 0.0,
                latency_p50_us = 0.0,
                latency_p95_us = 0.0,
                latency_p99_us = 0.0,
                bandwidth_mib_per_s = 0.0,
                cpu_percent_per_node = 0.0,
                memory_rss_mib_per_node = (FinalMemory - InitialMemory) / (1024 * 1024),
                memory_heap_mib_per_conn = 0.0,
                environment = detect_environment(),
                timestamp = erlang:system_time(second),
                error_count = ErrorCount,
                actual_duration_s = ActualDurationMs / 1000.0
            };
        _ ->
            P50 = percentile(SortedLatencies, 0.50),
            P95 = percentile(SortedLatencies, 0.95),
            P99 = percentile(SortedLatencies, 0.99),

            ActualDurationSec = ActualDurationMs / 1000.0,
            Throughput = TotalMessages / ActualDurationSec,

            TotalBytes = TotalMessages * Workload#workload.payload_size_bytes,
            BandwidthMiBPerSec = (TotalBytes / ActualDurationSec) / (1024 * 1024),

            MemoryUsedMiB = (FinalMemory - InitialMemory) / (1024 * 1024),
            HeapPerConnMiB = MemoryUsedMiB / Workload#workload.connections,

            %% Estimate CPU usage from GC activity
            GCDelta = FinalGCCount - InitialGCCount,
            EstimatedCPU = min(100.0, (GCDelta / ActualDurationSec) * 0.1),

            #metrics{
                workload_id = Workload#workload.id,
                tls_enabled = Workload#workload.tls_enabled,
                connections = Workload#workload.connections,
                duration_s = Workload#workload.duration_s,
                messages_sent = TotalMessages,
                throughput_msg_per_s = Throughput,
                latency_p50_us = P50,
                latency_p95_us = P95,
                latency_p99_us = P99,
                bandwidth_mib_per_s = BandwidthMiBPerSec,
                cpu_percent_per_node = EstimatedCPU,
                memory_rss_mib_per_node = MemoryUsedMiB,
                memory_heap_mib_per_conn = HeapPerConnMiB,
                environment = detect_environment(),
                timestamp = erlang:system_time(second),
                error_count = ErrorCount,
                actual_duration_s = ActualDurationSec
            }
    end,

    print_metrics(FinalMetrics),

    {ok, FinalMetrics}.

%%====================================================================
%% Client Connection Management
%%====================================================================

start_clients(NumClients, Port, Payload) ->
    start_clients(NumClients, Port, Payload, []).

start_clients(0, _Port, _Payload, Acc) ->
    Acc;
start_clients(N, Port, Payload, Acc) ->
    Pid = spawn_link(?MODULE, client_loop, [self(), Port, Payload, init, []]),
    start_clients(N - 1, Port, Payload, [Pid | Acc]).

%% Client loop: connect, wait for signal, send messages, report results
client_loop(ParentPid, Port, Payload, init, _Latencies) ->
    %% Connect to server
    case gen_tcp:connect("localhost", Port, [binary, {active, false},
                                              {packet, line}, {nodelay, true}], 5000) of
        {ok, Socket} ->
            ParentPid ! {client_ready, self()},
            client_loop(ParentPid, Port, Payload, {connected, Socket}, []);
        {error, Reason} ->
            ParentPid ! {client_error, self(), Reason},
            exit({connection_failed, Reason})
    end;

client_loop(ParentPid, _Port, Payload, {connected, Socket}, Latencies) ->
    receive
        {start_benchmark, EndTime} ->
            client_send_loop(ParentPid, Socket, Payload, EndTime, Latencies, 0);
        {get_results, From} ->
            From ! {client_results, self(), {0, Latencies}};
        stop ->
            gen_tcp:close(Socket),
            exit(normal)
    after 60000 ->
        gen_tcp:close(Socket),
        exit(timeout)
    end.

client_send_loop(ParentPid, Socket, Payload, EndTime, Latencies, MsgCount) ->
    CurrentTime = erlang:monotonic_time(millisecond),

    case CurrentTime >= EndTime of
        true ->
            %% Benchmark complete, wait for results request
            client_loop(ParentPid, undefined, Payload, results, {MsgCount, Latencies});
        false ->
            %% Send message and measure latency
            StartUs = erlang:monotonic_time(microsecond),

            case gen_tcp:send(Socket, [Payload, <<"\n">>]) of
                ok ->
                    %% Wait for echo or acknowledgment (optional)
                    %% For maximum throughput, we skip recv
                    EndUs = erlang:monotonic_time(microsecond),
                    LatencyUs = EndUs - StartUs,

                    NewLatencies = case length(Latencies) < 10000 of
                        true -> [LatencyUs | Latencies];  % Sample first 10K
                        false -> Latencies
                    end,

                    client_send_loop(ParentPid, Socket, Payload, EndTime,
                                    NewLatencies, MsgCount + 1);
                {error, _Reason} ->
                    %% Connection lost
                    client_loop(ParentPid, undefined, Payload, results,
                               {MsgCount, Latencies})
            end
    end;

client_send_loop(_ParentPid, _Socket, _Payload, _EndTime, Latencies, MsgCount) ->
    %% Results state
    receive
        {get_results, From} ->
            From ! {client_results, self(), {MsgCount, Latencies}};
        stop ->
            exit(normal)
    after 5000 ->
        exit(timeout)
    end.

collect_client_results(Pids, Acc) ->
    collect_client_results(Pids, Acc, 5000).

collect_client_results([], Acc, _Timeout) ->
    Acc;
collect_client_results([Pid | Rest], Acc, Timeout) ->
    Pid ! {get_results, self()},
    receive
        {client_results, Pid, Result} ->
            collect_client_results(Rest, [Result | Acc], Timeout)
    after Timeout ->
        collect_client_results(Rest, [{0, []} | Acc], Timeout)
    end.

%%====================================================================
%% System Metrics Collection
%%====================================================================

server_stats_loop(ParentPid, StartTime, Samples) ->
    receive
        {stop, From} ->
            From ! {system_metrics, lists:reverse(Samples)}
    after 1000 ->
        %% Collect snapshot every second
        Elapsed = (erlang:system_time(millisecond) - StartTime) / 1000.0,
        Sample = #{
            elapsed_s => Elapsed,
            memory_total_mib => erlang:memory(total) / (1024 * 1024),
            processes => erlang:system_info(process_count),
            schedulers_online => erlang:system_info(schedulers_online)
        },
        server_stats_loop(ParentPid, StartTime, [Sample | Samples])
    end.

%%====================================================================
%% Validation and Persistence
%%====================================================================

validate_and_save(Metrics, WorkloadName) ->
    %% Convert to JSON map
    JsonMap = metrics_to_json(Metrics),

    %% Validate with erlmcp_chaos_plan_validator structure
    case validate_metrics(JsonMap) of
        ok ->
            %% Save to file
            Timestamp = integer_to_list(erlang:system_time(second)),
            Filename = lists:flatten(io_lib:format(
                "bench/results/transport_real_tcp_~s_~s.json",
                [WorkloadName, Timestamp])),

            JsonBinary = jsx:encode(JsonMap, [{space, 1}, {indent, 2}]),

            case file:write_file(Filename, JsonBinary) of
                ok ->
                    io:format("~nResults saved to: ~s~n", [Filename]),
                    ok;
                {error, Reason} ->
                    io:format("ERROR: Failed to save results: ~p~n", [Reason]),
                    {error, {write_failed, Reason}}
            end;
        {error, Reason} ->
            io:format("ERROR: Validation failed: ~p~n", [Reason]),
            {error, {validation_failed, Reason}}
    end.

validate_metrics(JsonMap) ->
    %% Ensure all required fields are present
    Required = [
        <<"workload_id">>, <<"transport">>, <<"connections">>,
        <<"duration_s">>, <<"messages_sent">>, <<"throughput_msg_per_s">>,
        <<"latency_p50_us">>, <<"latency_p95_us">>, <<"latency_p99_us">>,
        <<"bandwidth_mib_per_s">>, <<"memory_rss_mib_per_node">>,
        <<"precision">>, <<"scope">>, <<"environment">>
    ],

    Missing = [K || K <- Required, not maps:is_key(K, JsonMap)],

    case Missing of
        [] -> ok;
        _ -> {error, {missing_fields, Missing}}
    end.

%%====================================================================
%% Helpers
%%====================================================================

create_json_rpc_payload(TargetBytes) ->
    %% Create a realistic JSON-RPC message that's approximately TargetBytes
    BaseMsg = #{
        <<"jsonrpc">> => <<"2.0">>,
        <<"method">> => <<"tools/call">>,
        <<"id">> => 1,
        <<"params">> => #{
            <<"name">> => <<"benchmark_tool">>,
            <<"arguments">> => #{}
        }
    },

    BaseJson = jsx:encode(BaseMsg),
    BaseSize = byte_size(BaseJson),

    %% Pad with data to reach target size
    case TargetBytes > BaseSize of
        true ->
            PaddingSize = TargetBytes - BaseSize - 20,
            Padding = binary:copy(<<"X">>, max(0, PaddingSize)),

            PaddedMsg = BaseMsg#{
                <<"params">> => #{
                    <<"name">> => <<"benchmark_tool">>,
                    <<"arguments">> => #{
                        <<"data">> => Padding
                    }
                }
            },
            jsx:encode(PaddedMsg);
        false ->
            BaseJson
    end.

percentile([], _P) -> 0.0;
percentile(SortedList, P) when P >= 0, P =< 1 ->
    N = length(SortedList),
    Index = max(1, round(N * P)),
    lists:nth(Index, SortedList).

count_errors(Results) ->
    length([true || {Msgs, _} <- Results, Msgs =:= 0]).

detect_environment() ->
    %% Simple environment detection
    case os:getenv("KUBERNETES_SERVICE_HOST") of
        false ->
            case os:getenv("AWS_REGION") of
                false -> <<"local_dev">>;
                _ -> <<"aws_unknown">>
            end;
        _ ->
            <<"kubernetes">>
    end.

get_target(WorkloadId) ->
    Definitions = workload_definitions(),
    case [W || {_, W} <- maps:to_list(Definitions), W#workload.id =:= WorkloadId] of
        [Workload | _] -> Workload#workload.target_msg_per_s;
        [] -> 0
    end.

apply_options(Workload, Opts) ->
    Workload#workload{
        connections = maps:get(connections, Opts, Workload#workload.connections),
        duration_s = maps:get(duration_s, Opts, Workload#workload.duration_s),
        payload_size_bytes = maps:get(payload_size_bytes, Opts,
                                     Workload#workload.payload_size_bytes)
    }.

metrics_to_json(Metrics) ->
    #{
        <<"workload_id">> => Metrics#metrics.workload_id,
        <<"transport">> => Metrics#metrics.transport,
        <<"tls_enabled">> => Metrics#metrics.tls_enabled,
        <<"connections">> => Metrics#metrics.connections,
        <<"duration_s">> => Metrics#metrics.duration_s,
        <<"messages_sent">> => Metrics#metrics.messages_sent,
        <<"throughput_msg_per_s">> => round(Metrics#metrics.throughput_msg_per_s * 100) / 100,
        <<"latency_p50_us">> => round(Metrics#metrics.latency_p50_us * 100) / 100,
        <<"latency_p95_us">> => round(Metrics#metrics.latency_p95_us * 100) / 100,
        <<"latency_p99_us">> => round(Metrics#metrics.latency_p99_us * 100) / 100,
        <<"bandwidth_mib_per_s">> => round(Metrics#metrics.bandwidth_mib_per_s * 100) / 100,
        <<"cpu_percent_per_node">> => round(Metrics#metrics.cpu_percent_per_node * 100) / 100,
        <<"memory_rss_mib_per_node">> => round(Metrics#metrics.memory_rss_mib_per_node * 100) / 100,
        <<"memory_heap_mib_per_conn">> => round(Metrics#metrics.memory_heap_mib_per_conn * 100) / 100,
        <<"precision">> => Metrics#metrics.precision,
        <<"scope">> => Metrics#metrics.scope,
        <<"environment">> => Metrics#metrics.environment,
        <<"timestamp">> => Metrics#metrics.timestamp,
        <<"error_count">> => Metrics#metrics.error_count,
        <<"actual_duration_s">> => round(Metrics#metrics.actual_duration_s * 100) / 100
    }.

print_metrics(Metrics) ->
    io:format("~n========================================~n"),
    io:format("BENCHMARK RESULTS~n"),
    io:format("========================================~n"),
    io:format("Workload:        ~s~n", [Metrics#metrics.workload_id]),
    io:format("Connections:     ~B~n", [Metrics#metrics.connections]),
    io:format("Duration:        ~.2f seconds~n", [Metrics#metrics.actual_duration_s]),
    io:format("Messages:        ~B~n", [Metrics#metrics.messages_sent]),
    io:format("~n"),
    io:format("Throughput:      ~.2f msg/sec~n", [Metrics#metrics.throughput_msg_per_s]),
    io:format("Bandwidth:       ~.2f MiB/sec~n", [Metrics#metrics.bandwidth_mib_per_s]),
    io:format("~n"),
    io:format("Latency (Âµs):~n"),
    io:format("  P50:           ~.2f~n", [Metrics#metrics.latency_p50_us]),
    io:format("  P95:           ~.2f~n", [Metrics#metrics.latency_p95_us]),
    io:format("  P99:           ~.2f~n", [Metrics#metrics.latency_p99_us]),
    io:format("~n"),
    io:format("Resources:~n"),
    io:format("  CPU (est):     ~.2f%%~n", [Metrics#metrics.cpu_percent_per_node]),
    io:format("  Memory (RSS):  ~.2f MiB~n", [Metrics#metrics.memory_rss_mib_per_node]),
    io:format("  Heap/conn:     ~.6f MiB~n", [Metrics#metrics.memory_heap_mib_per_conn]),
    io:format("~n"),
    io:format("Errors:          ~B~n", [Metrics#metrics.error_count]),
    io:format("Environment:     ~s~n", [Metrics#metrics.environment]),
    io:format("========================================~n~n").
