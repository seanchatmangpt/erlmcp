# erlmcp v1.5.0 Metrology Validation Architecture

**Version:** 1.5.0
**Status:** Design Review
**Agent:** erlang-architect
**Date:** 2026-01-27

---

## Executive Summary

This document defines the **complete metrology validation system** for erlmcp v1.5.0, ensuring automated detection of the **7 stop-the-line conditions** that have plagued performance artifact quality:

1. **Naked numbers** (no units)
2. **Ambiguous metrics** (msg/s vs req/s confusion)
3. **Undefined scope** (per-process vs per-node)
4. **Unanchored duration** (rate without time window)
5. **Mixed context** (session vs connection)
6. **Unlabeled memory** (heap only vs RSS)
7. **Zero sample size** (percentiles without N)

**Design Philosophy:**
- **Fail fast** at schema validation (JSON Schema + jesse)
- **Canonical definitions** in shapes/metrology.schema.json
- **CI gate integration** (block merge on violations)
- **Backwards compatibility** with existing v1.3.0/v1.4.0 artifacts via migration

---

## 1. Schema File Design

### 1.1 Location and Structure

**Primary Schema:**
`/Users/sac/erlmcp/shapes/metrology.schema.json`

**Schema Features:**
- JSON Schema Draft-07 for validation with jesse
- Canonical metric name registry (18 standard metrics)
- Required fields: `metric_name`, `value`, `unit`, `scope`, `transport`, `duration_seconds`
- Unit definitions with dimension + symbol (e.g., `{dimension: "time", symbol: "ms"}`)
- Quality gates: throughput_min, latency_p99_max, memory_max, error_rate_max
- Violation tracking: maps violations to 7 stop-the-line rules

**Example Valid Measurement:**
```json
{
  "metric_name": "throughput",
  "value": 150000,
  "unit": {"dimension": "rate", "symbol": "msg/s"},
  "scope": "per_node",
  "transport": "tcp",
  "duration_seconds": 60.0,
  "sample_size": 1000,
  "workload_details": {
    "concurrent_connections": 1000,
    "message_size_bytes": 4096,
    "request_pattern": "constant",
    "json_rpc_operations": ["tools/list", "tools/call"]
  },
  "notes": "Sustained load, 4KB payload, excludes warmup"
}
```

**Example Violation:**
```json
{
  "metric_name": "throughput",
  "value": 150000,
  // âŒ MISSING: unit, scope, duration_seconds
}
```
**Violation detected:**
```json
{
  "violations": [
    {
      "rule": "missing_unit",
      "severity": "ERROR",
      "message": "Metric 'throughput' has no unit field (violates stop-line condition #1)",
      "measurement_index": 0
    },
    {
      "rule": "undefined_scope",
      "severity": "ERROR",
      "message": "Metric 'throughput' has no scope field (violates stop-line condition #3)",
      "measurement_index": 0
    }
  ],
  "status": "FAIL"
}
```

### 1.2 Canonical Metric Registry

**Throughput & Latency:**
- `throughput` - requests/second or messages/second (MUST specify in unit.symbol)
- `latency_p50`, `latency_p95`, `latency_p99`, `latency_p99_9` - percentile latencies (MUST include sample_size)

**Memory:**
- `memory_heap` - Erlang process heap (per_process scope)
- `memory_process` - Full process memory (heap + stack + mailbox)
- `memory_ets` - ETS table memory
- `memory_total` - Node total memory (RSS equivalent, per_node scope)

**System:**
- `cpu_utilization` - CPU usage percentage
- `gc_pause_avg`, `gc_pause_max`, `gc_count` - Garbage collection metrics
- `connection_count` - Active connections
- `message_queue_len` - Process mailbox depth
- `error_rate`, `success_rate` - Operation success metrics
- `bandwidth` - Network throughput (Mbps or MiB/s)
- `payload_size` - Message size (wire bytes, pre-TLS)

**Scopes:**
- `per_process` - Single Erlang process (e.g., one gen_server)
- `per_connection` - One client connection's resources
- `per_node` - Entire Erlang node (VM)
- `per_cluster` - Distributed Erlang cluster aggregate

---

## 2. Validation Architecture

### 2.1 Validation Stages

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 1: Build-Time Validation (make compile)              â”‚
â”‚ - Validate benchmark plan specs in test/                   â”‚
â”‚ - Check shapes/metrology.schema.json is valid JSON Schema  â”‚
â”‚ - Fail fast: Exit code 1 on violation                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 2: Test Execution (make test / rebar3 ct)            â”‚
â”‚ - Tests generate evidence artifacts in JSON                â”‚
â”‚ - Each artifact conforms to metrology.schema.json          â”‚
â”‚ - Schema validation happens in teardown hook               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 3: CI Gate (.github/workflows/ci.yml)                â”‚
â”‚ - Run: rebar3 tcps metrology_validate                      â”‚
â”‚ - Validates all artifacts in dist/evidence/v1.5.0/         â”‚
â”‚ - Checks quality gates (throughput_min, latency_p99_max)   â”‚
â”‚ - Blocks merge if violations found                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 4: Release Validation (pre-release hook)             â”‚
â”‚ - Validate all evidence bundles                            â”‚
â”‚ - Generate metrology compliance report                     â”‚
â”‚ - Attach report to GitHub release                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Validation Components

#### A. Jesse Validator (Erlang)

**Module:** `src/erlmcp_metrology_validator.erl`
**Behavior:** gen_server (stateful, caches compiled schema)

**API:**
```erlang
%% Validate single measurement
-spec validate_measurement(Measurement :: map()) ->
    ok | {error, [violation()]}.

%% Validate complete artifact (plan or evidence)
-spec validate_artifact(FilePath :: binary()) ->
    {ok, ValidatedArtifact :: map()} | {error, [violation()]}.

%% Batch validate directory
-spec validate_directory(DirPath :: binary()) ->
    {ok, #{passed => integer(), failed => integer(), violations => [violation()]}} |
    {error, term()}.

%% Check quality gates
-spec check_quality_gates(Artifact :: map(), Gates :: map()) ->
    pass | {fail, [gate_violation()]}.
```

**Implementation Pattern:**
1. Load shapes/metrology.schema.json at startup
2. Compile schema with jesse for fast validation
3. Cache compiled schema in gen_server state
4. Validate on-demand (synchronous call)
5. Return structured violations with rule mapping

**Integration Points:**
- Called from rebar3 provider (`src/rebar3_tcps_metrology.erl`)
- Called from CT teardown hooks (`test/erlmcp_metrology_ct_hook.erl`)
- Called from CI script (`.github/workflows/validate_metrology.sh`)

#### B. Rebar3 Provider

**Module:** `src/rebar3_tcps_metrology.erl`
**Provider Name:** `tcps metrology_validate`

**Usage:**
```bash
# Validate all artifacts in dist/evidence/v1.5.0/
rebar3 tcps metrology_validate

# Validate specific artifact
rebar3 tcps metrology_validate --file=dist/evidence/v1.5.0/benchmarks/throughput_baseline.json

# Generate compliance report
rebar3 tcps metrology_validate --report=dist/evidence/v1.5.0/metrology_compliance_report.md
```

**Provider Responsibilities:**
1. Discover all .json files in dist/evidence/v1.5.0/benchmarks/ and dist/evidence/v1.5.0/chaos/
2. Call erlmcp_metrology_validator:validate_artifact/1 for each
3. Aggregate violations
4. Generate human-readable report
5. Exit with code 1 if any ERROR-severity violations found

#### C. Common Test Hook

**Module:** `test/erlmcp_metrology_ct_hook.erl`
**Hook Lifecycle:**

```erlang
%% CT hook callbacks
-export([id/1, init/2, pre_init_per_suite/3, post_end_per_testcase/5]).

%% Automatically registers with CT framework
-spec id(Opts :: list()) -> atom().
id(_Opts) -> ?MODULE.

%% Called once per suite
init(_Id, _Opts) ->
    {ok, #state{schema = load_schema()}}.

%% Validate artifacts after each test
post_end_per_testcase(_Suite, _TC, _Config, Return, State) ->
    % Check if test wrote metrology artifact
    case find_metrology_artifact(_TC) of
        {ok, Path} ->
            case erlmcp_metrology_validator:validate_artifact(Path) of
                ok -> {Return, State};
                {error, Violations} ->
                    ct:fail("Metrology validation failed: ~p", [Violations])
            end;
        not_found -> {Return, State}
    end.
```

**Installation:**
Add to `test/*.config`:
```erlang
{ct_hooks, [erlmcp_metrology_ct_hook]}.
```

### 2.3 CI Integration

**File:** `.github/workflows/ci.yml`

**New Step:**
```yaml
- name: Validate Metrology Artifacts
  run: |
    rebar3 compile
    rebar3 tcps metrology_validate
    if [ $? -ne 0 ]; then
      echo "âŒ Metrology validation failed"
      echo "See violations above. Fix before merging."
      exit 1
    fi
    echo "âœ… All metrology artifacts valid"
```

**Position:** After `rebar3 do eunit, ct, proper` but before deployment

**Failure Behavior:**
- CI job fails with red X
- PR blocked from merging
- Comment added to PR with violation summary

---

## 3. Canonical Definitions Structure

### 3.1 Disambiguation Dictionary

**Problem:** Terms like "connection", "session", "message", "request" are overloaded.

**Solution:** Canonical definitions in metrology.schema.json + docs/metrology-glossary.md

#### Connection vs Session
- **connection**: TCP or WebSocket transport-level link (scope: per_connection)
- **session**: MCP protocol session (1:1 with connection in stdio, N:1 in HTTP)
- **Rule**: Always specify scope (per_connection or per_session) + transport context

#### Message vs Request
- **message**: JSON-RPC frame on wire (one notification or request/response pair)
- **request**: Client-initiated JSON-RPC request (tools/call, resources/read, etc.)
- **Rule**: Use `msg/s` for all frames, `req/s` for client requests only

#### Throughput Disambiguation
- **throughput (msg/s)**: All JSON-RPC frames (requests + responses + notifications)
- **throughput (req/s)**: Client requests only (subset of msg/s)
- **Rule**: Specify in unit.symbol ("msg/s" or "req/s") + include json_rpc_operations in workload_details

#### Payload Size
- **payload_size**: Message size in bytes (wire format, pre-TLS, post-JSON-encoding)
- **Unit**: Always `B`, `KiB`, or `MiB`
- **Rule**: State if compression applied in notes field

#### Memory Decomposition
- **memory_heap**: Erlang process heap only (per_process scope)
- **memory_process**: Heap + stack + mailbox + internal structures (per_process)
- **memory_ets**: ETS tables (per_node or per_table)
- **memory_total**: OS-reported RSS (per_node scope, includes all Erlang + OS overhead)
- **Rule**: Never use generic "memory" - always specify component

### 3.2 Metrology Glossary (New File)

**File:** `docs/metrology-glossary.md`

**Contents:**
- A-Z definitions of all ambiguous terms
- Examples of correct vs incorrect usage
- Mapping to schema metric_name values
- Cross-references to MCP spec and Erlang/OTP docs

**Maintenance:** Updated whenever new metric added to canonical registry

---

## 4. Backwards Compatibility Strategy

### 4.1 Existing Artifacts (v1.3.0, v1.4.0)

**Challenge:** 200+ JSON files in dist/evidence/ don't conform to v1.5.0 schema.

**Strategy:** Migration script + parallel validation.

#### Migration Script

**File:** `scripts/migrate_metrology_artifacts.escript`

**Function:**
1. Discover all .json files in dist/evidence/v1.3.0/ and dist/evidence/v1.4.0/
2. Parse with jsx
3. Transform to v1.5.0 schema:
   - Add schema_version: "1.5.0"
   - Add artifact_type: "evidence"
   - Infer scope from context (heuristic: look for "per node", "per connection" in notes)
   - Add default transport if missing (guess from filename or test module)
   - Wrap naked numbers in unit objects
   - Add duration_seconds if rate metric present (default: 60)
4. Write migrated files to dist/evidence/v1.5.0/benchmarks/ (new directory)
5. Log warnings for unresolvable ambiguities

**Usage:**
```bash
./scripts/migrate_metrology_artifacts.escript \
  --input=dist/evidence/v1.4.0 \
  --output=dist/evidence/v1.5.0 \
  --strict=false  # Allows heuristics
```

**Validation:**
After migration, run:
```bash
rebar3 tcps metrology_validate --dir=dist/evidence/v1.5.0/
```

#### Parallel Structure (Transition Period)

```
dist/evidence/
â”œâ”€â”€ v1.3.0/                          # Legacy artifacts (unchanged)
â”‚   â”œâ”€â”€ benchmarks/metrics.json      # Old format (naked numbers)
â”‚   â””â”€â”€ chaos/registry_crash.json
â”œâ”€â”€ v1.4.0/                          # Legacy artifacts (unchanged)
â”‚   â”œâ”€â”€ benchmarks/throughput_report.md
â”‚   â””â”€â”€ erlmcp-1.4.0.sbom.cyclonedx
â””â”€â”€ v1.5.0/                          # NEW: v1.5.0 schema-compliant
    â”œâ”€â”€ benchmarks/
    â”‚   â”œâ”€â”€ throughput_baseline.json  # Migrated + validated
    â”‚   â”œâ”€â”€ latency_p99_1k.json
    â”‚   â””â”€â”€ scaling_10k_100k.json
    â”œâ”€â”€ chaos/
    â”‚   â”œâ”€â”€ registry_crash_plan.json  # Plan artifact (pre-test spec)
    â”‚   â””â”€â”€ registry_crash_evidence.json  # Evidence artifact (post-test results)
    â””â”€â”€ metrology_compliance_report.md  # Generated by rebar3 provider
```

**Timeline:**
- **v1.5.0 release:** Both formats coexist, CI validates v1.5.0/ only
- **v1.6.0 release:** v1.3.0 and v1.4.0 directories archived (read-only)
- **v2.0.0 release:** Legacy directories removed

### 4.2 Test Migration

**Challenge:** 80+ test suites generate ad-hoc JSON output.

**Strategy:** Incremental migration with CT hook enforcement.

#### Phase 1: Opt-In (v1.5.0)
- erlmcp_metrology_ct_hook only warns on violations
- Developers fix tests incrementally
- No CI blocking

#### Phase 2: Mandatory (v1.6.0)
- CT hook fails tests on violations
- All new tests must pass validation
- Legacy tests grandfathered until v2.0.0

#### Phase 3: Full Enforcement (v2.0.0)
- All tests must emit v1.5.0-compliant artifacts
- No exceptions

**Helper Function for Tests:**
```erlang
%% In test/metrology_helpers.erl
-spec emit_measurement(MetricName, Value, Opts) -> ok.
emit_measurement(MetricName, Value, Opts) ->
    Measurement = #{
        metric_name => MetricName,
        value => Value,
        unit => maps:get(unit, Opts),
        scope => maps:get(scope, Opts),
        transport => maps:get(transport, Opts),
        duration_seconds => maps:get(duration_seconds, Opts),
        sample_size => maps:get(sample_size, Opts, 1)
    },
    Path = artifact_path(),
    Artifact = load_or_create_artifact(Path),
    UpdatedArtifact = Artifact#{
        measurements => [Measurement | maps:get(measurements, Artifact, [])]
    },
    write_artifact(Path, UpdatedArtifact).
```

**Usage in Test:**
```erlang
test_throughput(_Config) ->
    {ok, ThroughputMsgSec} = run_benchmark(),
    metrology_helpers:emit_measurement(throughput, ThroughputMsgSec, #{
        unit => #{dimension => rate, symbol => 'msg/s'},
        scope => per_node,
        transport => tcp,
        duration_seconds => 60.0,
        sample_size => 1000,
        workload_details => #{
            concurrent_connections => 1000,
            message_size_bytes => 4096,
            request_pattern => constant
        }
    }).
```

---

## 5. Fail-Fast Quality Gates

### 5.1 Schema Validation (Stage 1)

**When:** make compile (rebar3 pre_compile hook)
**Validator:** jesse:validate/2 (JSON Schema Draft-07)
**Failure:** Exit code 1, compilation blocked

**Example Error:**
```
===> Error: Metrology schema validation failed
===> File: shapes/metrology.schema.json
===> Reason: Invalid JSON Schema: 'metric_name' enum contains duplicate value 'throughput'
===> Action: Fix shapes/metrology.schema.json and recompile
```

### 5.2 Artifact Validation (Stage 2)

**When:** Test teardown (CT hook: post_end_per_testcase)
**Validator:** erlmcp_metrology_validator:validate_artifact/1
**Failure:** Test fails with detailed violation report

**Example Error:**
```
===> Test case: throughput_SUITE:health_check_baseline - FAILED
===> Metrology validation errors (3):
    1. [ERROR] missing_unit: Metric 'throughput' has no unit field
       File: _build/test/artifacts/throughput_baseline.json
       Measurement index: 0
       Rule: Stop-line condition #1 (naked numbers)

    2. [ERROR] undefined_scope: Metric 'latency_p99' has no scope field
       File: _build/test/artifacts/throughput_baseline.json
       Measurement index: 1
       Rule: Stop-line condition #3 (undefined scope)

    3. [WARNING] ambiguous_metric: Metric 'throughput' uses 'msg/s' but workload_details
       includes only JSON-RPC requests (should be 'req/s')
       File: _build/test/artifacts/throughput_baseline.json
       Measurement index: 0
       Rule: Stop-line condition #2 (ambiguous metrics)

===> Fix by adding required fields to emitted measurements.
```

### 5.3 CI Gate (Stage 3)

**When:** CI workflow after all tests pass
**Validator:** rebar3 tcps metrology_validate
**Failure:** CI job fails, PR blocked

**GitHub Actions Output:**
```yaml
âŒ Metrology Validation Failed
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Validated: 47 artifacts
Passed: 43 artifacts
Failed: 4 artifacts

Violations (ERROR severity only):
  â€¢ dist/evidence/v1.5.0/benchmarks/throughput_baseline.json
    - missing_unit (index 0)
    - undefined_scope (index 1)

  â€¢ dist/evidence/v1.5.0/chaos/bulkhead_crash_evidence.json
    - unanchored_duration (index 2)
    - zero_sample_size (index 3)

ðŸ”§ To fix: Add required fields to measurements.
ðŸ“– See: docs/metrology-glossary.md for canonical definitions

â›” PR merge blocked until violations resolved.
```

### 5.4 Quality Gate Thresholds

**Defined in:** shapes/metrology.schema.json (quality_gates object)
**Checked by:** erlmcp_metrology_validator:check_quality_gates/2

**Example Gates (v1.5.0 defaults):**
```json
{
  "quality_gates": {
    "throughput_min_req_s": 95000,
    "latency_p99_max_ms": 5.0,
    "memory_max_mb": 512,
    "error_rate_max_percent": 0.01,
    "regression_tolerance_percent": 5.0
  }
}
```

**Validation Logic:**
1. Load quality_gates from artifact or use defaults
2. For each measurement:
   - If metric_name == "throughput", check value >= throughput_min_req_s
   - If metric_name == "latency_p99", check value <= latency_p99_max_ms
   - If metric_name matches "memory_*", check value <= memory_max_mb
   - If metric_name == "error_rate", check value <= error_rate_max_percent
3. Return pass | {fail, [gate_violation()]}

**Gate Violation Example:**
```json
{
  "gate": "latency_p99_max_ms",
  "threshold": 5.0,
  "actual": 6.2,
  "severity": "ERROR",
  "message": "p99 latency (6.2ms) exceeds threshold (5.0ms) by 24%"
}
```

---

## 6. Implementation Plan

### 6.1 Phase 1: Core Validation (Week 1)

**Deliverables:**
1. âœ… `shapes/metrology.schema.json` (DONE - this design)
2. `src/erlmcp_metrology_validator.erl` (gen_server with jesse integration)
3. `test/erlmcp_metrology_validator_tests.erl` (unit tests for all 7 violation types)
4. `docs/metrology-glossary.md` (canonical definitions)

**Acceptance Criteria:**
- Schema validates itself (jesse:validate/2)
- Validator detects all 7 stop-line conditions
- Unit tests pass (80%+ coverage)

### 6.2 Phase 2: CI Integration (Week 2)

**Deliverables:**
1. `src/rebar3_tcps_metrology.erl` (rebar3 provider)
2. `test/erlmcp_metrology_ct_hook.erl` (CT hook)
3. `.github/workflows/validate_metrology.sh` (CI script)
4. Update `.github/workflows/ci.yml` (add validation step)

**Acceptance Criteria:**
- `rebar3 tcps metrology_validate` runs successfully
- CT hook fails test on violation (opt-in mode)
- CI job blocks PR on failure

### 6.3 Phase 3: Migration (Week 3)

**Deliverables:**
1. `scripts/migrate_metrology_artifacts.escript` (migration tool)
2. `dist/evidence/v1.5.0/` directory structure
3. Migration log (warnings for unresolvable cases)
4. `dist/evidence/v1.5.0/metrology_compliance_report.md` (generated)

**Acceptance Criteria:**
- 90%+ of v1.4.0 artifacts migrate cleanly
- Remaining 10% have documented heuristics
- All v1.5.0/ artifacts validate successfully

### 6.4 Phase 4: Test Migration (Week 4-6)

**Deliverables:**
1. `test/metrology_helpers.erl` (helper functions)
2. Migrate 10 high-value test suites (throughput, latency, chaos)
3. Update test documentation with examples

**Acceptance Criteria:**
- 10 test suites emit v1.5.0-compliant artifacts
- Helper functions cover 80%+ of common patterns
- Documentation includes before/after examples

### 6.5 Phase 5: Full Enforcement (v1.6.0 release)

**Deliverables:**
1. Switch CT hook to fail-on-violation (remove opt-in flag)
2. Archive v1.3.0 and v1.4.0 directories (read-only)
3. Update CLAUDE.md and release notes

**Acceptance Criteria:**
- All new tests emit v1.5.0 artifacts
- No CI failures due to validation (cleanup complete)
- Documentation reflects mandatory enforcement

---

## 7. Supervision Tree Impact

**No new supervisors required.** Validation is synchronous and stateless (per request).

**Integration Points:**
- erlmcp_metrology_validator registered as named gen_server under erlmcp_sup
- Lifetime: permanent (application-level, survives individual test failures)
- Restart strategy: transient (crash doesn't affect tests)

**Child Spec:**
```erlang
#{
    id => erlmcp_metrology_validator,
    start => {erlmcp_metrology_validator, start_link, []},
    restart => transient,
    shutdown => 5000,
    type => worker,
    modules => [erlmcp_metrology_validator]
}
```

**Why not supervised per-test?**
- Schema compilation is expensive (2-5ms), shared state amortizes cost
- Validator is read-only after init (no mutable state risk)
- Crash would only affect single validation call (caller handles error)

---

## 8. Rationale & Tradeoffs

### 8.1 Why JSON Schema Instead of SHACL?

**Decision:** Use JSON Schema (Draft-07) with jesse validator.

**Rationale:**
1. Erlang ecosystem support: jesse is mature, battle-tested
2. Developer familiarity: JSON Schema more common than SHACL/RDF
3. Tooling: Better IDE support, validation libraries
4. Performance: jesse is fast (<1ms per artifact)

**Tradeoff:**
- SHACL would enable semantic reasoning (e.g., infer scope from transport)
- Accepted: Metrology is domain-specific, not general-purpose ontology

### 8.2 Why Fail-Fast vs Collect-All-Errors?

**Decision:** Fail on first ERROR-severity violation (fail-fast).

**Rationale:**
1. Clear feedback: Developer fixes one issue at a time
2. CI efficiency: No wasted time on downstream validations
3. Encourages discipline: Forces immediate correction

**Tradeoff:**
- Developers might prefer seeing all errors at once
- Accepted: Quality > convenience; violations are rare after initial migration

### 8.3 Why Centralized Validator vs Embedded?

**Decision:** Single erlmcp_metrology_validator gen_server.

**Rationale:**
1. Schema compilation caching (avoids repeated parsing)
2. Single source of truth (no version skew between test suites)
3. Easy to update (one module to change)

**Tradeoff:**
- Single point of failure (validator crash blocks all validation)
- Accepted: Transient restart strategy + comprehensive unit tests mitigate risk

### 8.4 Why 7 Stop-Line Conditions?

**Decision:** Enumerate specific violations (not generic "invalid schema").

**Rationale:**
1. Traceability: Each violation maps to documented problem
2. Education: Developers learn metrology discipline
3. Metrics: Track violation frequency (kaizen target)

**Tradeoff:**
- More complex violation handling (7 cases vs 1 generic)
- Accepted: Precision > simplicity for manufacturing discipline

---

## 9. Testing Strategy

### 9.1 Unit Tests (erlmcp_metrology_validator_tests.erl)

**Coverage:**
1. Schema self-validation (jesse:validate/2 on metrology.schema.json)
2. Positive cases: Valid artifacts pass
3. Negative cases: Each of 7 violation types triggers ERROR
4. Quality gates: Pass/fail for each gate type
5. Edge cases: Empty measurements array, missing required fields

**Pattern:**
```erlang
test_missing_unit_violation() ->
    Artifact = #{
        <<"schema_version">> => <<"1.5.0">>,
        <<"artifact_type">> => <<"evidence">>,
        <<"workload_id">> => <<"test_workload">>,
        <<"measurements">> => [
            #{<<"metric_name">> => <<"throughput">>, <<"value">> => 150000}
            % Missing: unit, scope, transport
        ]
    },
    {error, Violations} = erlmcp_metrology_validator:validate_artifact(Artifact),
    ?assertMatch([#{rule := missing_unit}|_], Violations).
```

### 9.2 Integration Tests (rebar3_tcps_metrology_tests.erl)

**Coverage:**
1. Provider discovers artifacts in directory
2. Provider generates compliance report
3. Provider exits with code 1 on failure
4. CT hook fails test on violation

**Pattern:**
```erlang
test_ci_gate_blocks_on_violation() ->
    % Setup: Create invalid artifact in temp directory
    TempDir = create_temp_artifact_with_violation(),

    % Run provider
    Result = os:cmd("rebar3 tcps metrology_validate --dir=" ++ TempDir),

    % Verify failure
    ?assertMatch({error, _}, Result),
    ?assert(string:str(Result, "missing_unit") > 0).
```

### 9.3 Migration Tests (migrate_metrology_artifacts_tests.erl)

**Coverage:**
1. Migrate v1.4.0 artifact to v1.5.0 format
2. Migrated artifact passes validation
3. Heuristics for ambiguous cases (logged warnings)
4. Rollback on unresolvable errors

---

## 10. Documentation Deliverables

### 10.1 For Developers

**File:** `docs/metrology-developer-guide.md`

**Contents:**
- Quick start: Adding measurements to your test
- Helper functions (metrology_helpers.erl API)
- Common violations and fixes
- Examples: Before/after migration

### 10.2 For CI/CD

**File:** `docs/metrology-ci-integration.md`

**Contents:**
- GitHub Actions integration (workflow YAML)
- Jenkins/GitLab CI examples
- Interpreting validation reports
- Troubleshooting CI failures

### 10.3 For Architects

**File:** `docs/metrology-glossary.md`

**Contents:**
- A-Z canonical definitions
- Disambiguation rules
- Scope and transport context rules
- Examples from MCP spec and Erlang/OTP

### 10.4 For Release Managers

**File:** `docs/metrology-compliance-checklist.md`

**Contents:**
- Pre-release validation checklist
- Quality gate thresholds review
- Evidence bundle requirements
- Migration status tracking

---

## 11. Success Metrics

**Goal:** Eliminate metrology violations from v1.6.0 forward.

**Metrics:**
1. **Violation Rate:** Violations per 100 artifacts (target: <1)
2. **CI Pass Rate:** % of PRs passing metrology validation first try (target: >95%)
3. **Migration Coverage:** % of legacy artifacts migrated (target: >90%)
4. **Developer Adoption:** % of tests using metrology_helpers.erl (target: >80%)

**Tracking:** Dashboard in TCPS (erlmcp_dashboard_http)

---

## 12. Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|-----------|
| Schema too strict, blocks valid cases | HIGH | Provide opt-out for edge cases (notes field) |
| Migration script misclassifies artifacts | MEDIUM | Manual review of 10% sample, publish heuristics |
| Performance regression (jesse validation) | LOW | Benchmark <1ms per artifact, cache compiled schema |
| Developer resistance (too much work) | MEDIUM | Provide helpers, clear documentation, staged rollout |
| Legacy artifacts never migrate | MEDIUM | Parallel structure (v1.3.0, v1.4.0 coexist until v2.0) |

---

## 13. Next Steps

1. **Review this document** with stakeholders (plan-designer, erlang-otp-developer)
2. **Approve schema** (shapes/metrology.schema.json)
3. **Implement Phase 1** (erlmcp_metrology_validator.erl + tests)
4. **Validate PoC** (migrate 1 test suite, run CI gate)
5. **Iterate** based on feedback
6. **Roll out** Phases 2-5 per implementation plan

---

## Appendix A: Stop-Line Condition Examples

### #1: Naked Numbers (missing_unit)

âŒ **Bad:**
```json
{"metric_name": "throughput", "value": 150000}
```

âœ… **Good:**
```json
{
  "metric_name": "throughput",
  "value": 150000,
  "unit": {"dimension": "rate", "symbol": "msg/s"}
}
```

### #2: Ambiguous Metrics (ambiguous_metric)

âŒ **Bad:**
```json
{
  "metric_name": "throughput",
  "value": 150000,
  "unit": {"dimension": "rate", "symbol": "msg/s"}
  // Ambiguous: Is this all frames or just requests?
}
```

âœ… **Good:**
```json
{
  "metric_name": "throughput",
  "value": 150000,
  "unit": {"dimension": "rate", "symbol": "req/s"},
  "workload_details": {
    "json_rpc_operations": ["tools/list", "tools/call"]
  },
  "notes": "Client requests only, excludes responses/notifications"
}
```

### #3: Undefined Scope (undefined_scope)

âŒ **Bad:**
```json
{"metric_name": "memory_heap", "value": 24}
// Scope: Is this per-process or per-node?
```

âœ… **Good:**
```json
{
  "metric_name": "memory_heap",
  "value": 24,
  "unit": {"dimension": "bytes", "symbol": "MiB"},
  "scope": "per_process"
}
```

### #4: Unanchored Duration (unanchored_duration)

âŒ **Bad:**
```json
{
  "metric_name": "throughput",
  "value": 150000,
  "unit": {"dimension": "rate", "symbol": "msg/s"}
  // Missing: duration_seconds
}
```

âœ… **Good:**
```json
{
  "metric_name": "throughput",
  "value": 150000,
  "unit": {"dimension": "rate", "symbol": "msg/s"},
  "duration_seconds": 60.0,
  "notes": "Sustained load, excludes 5s warmup"
}
```

### #5: Mixed Context (mixed_context)

âŒ **Bad:**
```json
{
  "metric_name": "latency_p99",
  "value": 2.1,
  "unit": {"dimension": "time", "symbol": "ms"}
  // Ambiguous: Session latency or connection latency?
}
```

âœ… **Good:**
```json
{
  "metric_name": "latency_p99",
  "value": 2.1,
  "unit": {"dimension": "time", "symbol": "ms"},
  "scope": "per_connection",
  "transport": "tcp",
  "notes": "Measures transport-level RTT, excludes application logic"
}
```

### #6: Unlabeled Memory (unlabeled_memory)

âŒ **Bad:**
```json
{"metric_name": "memory", "value": 320}
// What memory? Heap? RSS? ETS?
```

âœ… **Good:**
```json
{
  "metric_name": "memory_total",
  "value": 320,
  "unit": {"dimension": "bytes", "symbol": "MiB"},
  "scope": "per_node",
  "notes": "OS-reported RSS, includes Erlang VM + all processes"
}
```

### #7: Zero Sample Size (zero_sample_size)

âŒ **Bad:**
```json
{
  "metric_name": "latency_p99",
  "value": 2.1,
  "unit": {"dimension": "time", "symbol": "ms"}
  // Missing: sample_size (how many samples in p99?)
}
```

âœ… **Good:**
```json
{
  "metric_name": "latency_p99",
  "value": 2.1,
  "unit": {"dimension": "time", "symbol": "ms"},
  "sample_size": 10000,
  "percentile": 99.0,
  "duration_seconds": 60.0
}
```

---

## Appendix B: Quality Gate Configuration

**File:** `config/metrology_gates.config`

```erlang
%% Quality gate thresholds for v1.5.0
[
    {metrology_gates, [
        %% Performance gates
        {throughput_min_req_s, 95000},
        {latency_p99_max_ms, 5.0},
        {latency_p95_max_ms, 3.0},

        %% Resource gates
        {memory_max_mb, 512},
        {cpu_max_percent, 80},

        %% Reliability gates
        {error_rate_max_percent, 0.01},
        {gc_pause_max_ms, 100},

        %% Regression gates
        {regression_tolerance_percent, 5.0}
    ]}
].
```

**Load in validator:**
```erlang
init([]) ->
    {ok, Gates} = application:get_env(metrology_gates),
    CompiledSchema = jesse:load_schema(schema_path()),
    {ok, #state{gates = Gates, schema = CompiledSchema}}.
```

---

**End of Architecture Design**

**Status:** Ready for review by plan-designer and erlang-otp-developer
**Next:** Implement Phase 1 (erlmcp_metrology_validator.erl)
