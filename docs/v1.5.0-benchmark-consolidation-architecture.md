# erlmcp v1.5.0 Benchmark Consolidation Architecture (80/20)

**Version:** 1.5.0
**Status:** Architecture Design Complete
**Agent:** erlang-architect
**Date:** 2026-01-27

---

## Executive Summary

This architecture consolidates erlmcp's benchmark suite from 15+ scattered micro-benchmarks into **5 focused modules** following the **80/20 principle**: 20% of tests prove 80% of performance claims, with full v1.5.0 metrology compliance.

### The Problem

**Before (v1.3.0-v1.4.0):**
- 15+ benchmark files with inconsistent output formats
- Ambiguous metrics: `req/s` (requests only? or requests+responses?)
- No workload definitions (ad-hoc test parameters)
- No reproducibility (missing environment details)
- Micro-benchmarks test narrow scenarios without real-world context
- No metrology validation (naked numbers, undefined scope)

**After (v1.5.0):**
- 5 benchmark modules (core, network, stress, chaos, integration)
- Canonical units from `METRICS_GLOSSARY.md` (msg_per_s, latency_p50_us, per_connection_heap_mib)
- Workload definitions in `bench/workloads/*.json`
- Full metrology compliance via `shapes/metrology.schema.json`
- Transport-real network tests (actual TCP/HTTP sockets)
- Reproducible: Same workload → Same results (±2%)

---

## 1. Architecture Principles

### 1.1 The 80/20 Rule Applied

**Focus on high-value benchmarks:**
- **Core Operations** (80% of claims): Registry, queue, pool, session in-memory speed
- **Network Real** (20% of claims): TCP/HTTP with actual sockets, proves transport layer
- **Stress/Sustained** (critical): 30s/5min/1hr duration tests, proves stability
- **Chaos/Adversarial** (edge cases): Failure injection, limits, proves resilience
- **Integration/E2E** (full protocol): Complete MCP workflows, proves correctness

**Eliminate low-value benchmarks:**
- ❌ Synthetic micro-benchmarks without real workload context
- ❌ Duplicate tests (8 micro-benchmarks → 1 core_ops with workloads)
- ❌ Simulation-based tests (replace with transport_real)
- ❌ Tests that don't map to v1.5.0 roadmap claims

### 1.2 Metrology Compliance

**Every benchmark output MUST include:**
- `workload_id` - References `bench/workloads/{id}.json`
- `transport` - tcp, http, stdio (not ambiguous "network")
- `scope` - per_node, per_connection, per_process
- `duration_s` - Time window for rate metrics
- Canonical units: `msg_per_s`, `latency_p50_us`, `per_connection_heap_mib`, `per_node_total_rss_mib`
- Memory decomposition: heap, state, base overhead, RSS (not ambiguous "memory")
- Environment: hardware, OS, Erlang version

**Validation:**
- All outputs validated against `shapes/metrology.schema.json`
- CI gate blocks merge on violations
- Automatic migration from v1.4.0 ambiguous metrics

### 1.3 Transport-Real Principle

**Network benchmarks MUST use actual sockets:**
- ✅ TCP: Real `gen_tcp` sockets (not simulated)
- ✅ HTTP: Real `gun` or `cowboy` connections
- ✅ SSE: Real HTTP Server-Sent Events
- ❌ Mocked transports
- ❌ In-memory only

**Rationale:** Simulation hides kernel overhead, buffer management, and network congestion.

### 1.4 Reproducibility Target

**Definition:** Same workload executed twice should produce results within **±2% variance**.

**Achieved through:**
- Warmup phase (100-1000 iterations before measurement)
- Fixed workload definitions (no random parameters)
- Environment capture (CPU, RAM, OS, Erlang version)
- Stable test duration (minimum 10s for statistical significance)
- Controlled load (no external interference)

---

## 2. Five Benchmark Modules

### 2.1 Core Operations Benchmark

**Module:** `bench/erlmcp_bench_core_ops.erl`

**Purpose:** Measure in-memory operation speed (registry, queue, pool, session management).

**Workloads:**
1. `core_ops_registry_1k` - 1K registry lookups/inserts/deletes
2. `core_ops_registry_10k` - 10K registry operations (high contention)
3. `core_ops_registry_100k` - 100K registry operations (stress)
4. `core_ops_queue_1k` - 1K queue enqueue/dequeue (FIFO)
5. `core_ops_queue_10k` - 10K queue operations
6. `core_ops_pool_10_workers` - 10 poolboy workers, 1K checkouts
7. `core_ops_pool_100_workers` - 100 poolboy workers, 10K checkouts
8. `core_ops_session_create_1k` - 1K session start/stop cycles

**Consolidates:**
- `erlmcp_registry_contention.erl` → registry workloads
- 8 micro-benchmarks → unified core_ops module

**Output Format:**
```json
{
  "schema_version": "1.5.0",
  "artifact_type": "evidence",
  "workload_id": "core_ops_registry_10k",
  "metadata": {
    "timestamp": "2026-01-27T12:00:00Z",
    "environment": "ci",
    "erlang_version": "25.3",
    "hardware": {
      "cpu_cores": 16,
      "memory_gb": 64,
      "architecture": "x86_64"
    }
  },
  "measurements": [
    {
      "metric_name": "throughput",
      "value": 450000,
      "unit": {"dimension": "rate", "symbol": "ops/s"},
      "scope": "per_node",
      "transport": "all",
      "duration_seconds": 30.0,
      "sample_size": 10000,
      "notes": "Registry operations: 50% lookup, 30% insert, 20% delete"
    },
    {
      "metric_name": "latency_p50",
      "value": 2.1,
      "unit": {"dimension": "time", "symbol": "µs"},
      "scope": "per_process",
      "transport": "all",
      "sample_size": 10000
    },
    {
      "metric_name": "latency_p99",
      "value": 8.4,
      "unit": {"dimension": "time", "symbol": "µs"},
      "scope": "per_process",
      "transport": "all",
      "sample_size": 10000
    },
    {
      "metric_name": "memory_process",
      "value": 0.024,
      "unit": {"dimension": "bytes", "symbol": "MiB"},
      "scope": "per_process",
      "transport": "all"
    }
  ]
}
```

**API:**
```erlang
-module(erlmcp_bench_core_ops).
-export([run/0, run/1, workloads/0]).

%% @doc Run all core operations workloads
-spec run() -> {ok, [result()]}.
run() ->
    Workloads = workloads(),
    Results = [run(WL) || WL <- Workloads],
    {ok, Results}.

%% @doc Run specific workload
-spec run(WorkloadId :: binary()) -> {ok, result()} | {error, term()}.
run(WorkloadId) ->
    Workload = load_workload(WorkloadId),
    Result = execute_workload(Workload),
    ValidatedResult = erlmcp_metrology_validator:validate_artifact(Result),
    write_result(WorkloadId, ValidatedResult),
    {ok, ValidatedResult}.

%% @doc List available workloads
-spec workloads() -> [workload_def()].
workloads() ->
    [
        #{id => <<"core_ops_registry_1k">>, operations => 1000, type => registry},
        #{id => <<"core_ops_registry_10k">>, operations => 10000, type => registry},
        #{id => <<"core_ops_registry_100k">>, operations => 100000, type => registry},
        #{id => <<"core_ops_queue_1k">>, operations => 1000, type => queue},
        #{id => <<"core_ops_queue_10k">>, operations => 10000, type => queue},
        #{id => <<"core_ops_pool_10_workers">>, workers => 10, operations => 1000, type => pool},
        #{id => <<"core_ops_pool_100_workers">>, workers => 100, operations => 10000, type => pool},
        #{id => <<"core_ops_session_create_1k">>, sessions => 1000, type => session}
    ].
```

---

### 2.2 Network Real Benchmark

**Module:** `bench/erlmcp_bench_network_real.erl`

**Purpose:** Measure TCP/HTTP transport performance with actual sockets.

**Workloads:**
1. `tcp_sustained_1k_1kib` - 1K connections, 1 KiB messages, 30s sustained
2. `tcp_sustained_10k_1kib` - 10K connections, 1 KiB messages, 30s sustained
3. `tcp_sustained_10k_4kib` - 10K connections, 4 KiB messages, 30s sustained
4. `tcp_burst_50k_small` - 50K connections, <100B messages, burst pattern
5. `http_sse_sustained_1k` - 1K HTTP SSE connections, 30s sustained
6. `http_sse_sustained_10k` - 10K HTTP SSE connections, 30s sustained
7. `tcp_mixed_workload` - Mixed message sizes (100B-10KiB), realistic traffic

**Consolidates:**
- `erlmcp_transport_tcp_4kb.erl` → tcp_sustained_10k_4kib
- `latency_SUITE.erl` → latency measurements integrated
- `throughput_SUITE.erl` → throughput measurements integrated

**Transport Implementation:**
```erlang
%% TCP workload execution
execute_tcp_workload(Workload) ->
    #{
        connections := Connections,
        message_size := MessageSize,
        duration_s := Duration,
        pattern := Pattern
    } = Workload,

    %% Start TCP server
    {ok, ListenSocket} = gen_tcp:listen(0, [
        binary,
        {active, false},
        {reuseaddr, true},
        {packet, 4}
    ]),
    {ok, Port} = inet:port(ListenSocket),

    %% Spawn acceptor
    AcceptorPid = spawn_link(fun() -> acceptor_loop(ListenSocket) end),

    %% Spawn clients
    Clients = [spawn_client(Port, MessageSize, Pattern) || _ <- lists:seq(1, Connections)],

    %% Collect metrics
    timer:sleep(Duration * 1000),
    Metrics = collect_metrics(Clients, AcceptorPid),

    %% Cleanup
    [exit(C, kill) || C <- Clients],
    exit(AcceptorPid, kill),
    gen_tcp:close(ListenSocket),

    Metrics.
```

**Output Format:**
```json
{
  "workload_id": "tcp_sustained_10k_1kib",
  "measurements": [
    {
      "metric_name": "throughput",
      "value": 150000,
      "unit": {"dimension": "rate", "symbol": "msg/s"},
      "scope": "per_node",
      "transport": "tcp",
      "duration_seconds": 30.0,
      "workload_details": {
        "concurrent_connections": 10000,
        "message_size_bytes": 1024,
        "request_pattern": "constant"
      }
    },
    {
      "metric_name": "latency_p99",
      "value": 28.4,
      "unit": {"dimension": "time", "symbol": "ms"},
      "scope": "per_connection",
      "transport": "tcp",
      "sample_size": 450000
    },
    {
      "metric_name": "memory_total",
      "value": 2048,
      "unit": {"dimension": "bytes", "symbol": "MiB"},
      "scope": "per_node",
      "transport": "tcp",
      "notes": "RSS measured via recon_alloc:memory(allocated)"
    },
    {
      "metric_name": "bandwidth",
      "value": 1200,
      "unit": {"dimension": "bandwidth", "symbol": "Mbps"},
      "scope": "per_node",
      "transport": "tcp"
    }
  ]
}
```

---

### 2.3 Stress/Sustained Benchmark

**Module:** `bench/erlmcp_bench_stress.erl`

**Purpose:** Measure stability and performance degradation under sustained load.

**Workloads:**
1. `stress_sustained_30s` - 30 second sustained load (baseline)
2. `stress_sustained_5min` - 5 minute sustained load (stability check)
3. `stress_sustained_1hr` - 1 hour sustained load (production simulation)
4. `stress_memory_leak_detection` - 1 hour with memory sampling every 10s
5. `stress_gc_pressure` - High allocation rate, GC analysis
6. `stress_connection_churn` - Connect/disconnect cycles

**Key Metrics:**
- Throughput stability (coefficient of variation < 5%)
- Memory growth rate (bytes/second)
- GC pause time degradation
- Latency percentile drift

**Output Format:**
```json
{
  "workload_id": "stress_sustained_5min",
  "measurements": [
    {
      "metric_name": "throughput",
      "value": 148500,
      "unit": {"dimension": "rate", "symbol": "msg/s"},
      "scope": "per_node",
      "transport": "tcp",
      "duration_seconds": 300.0,
      "notes": "Throughput at t=0: 150000 msg/s, t=300: 148500 msg/s (0.99% degradation)"
    },
    {
      "metric_name": "memory_total",
      "value": 2104,
      "unit": {"dimension": "bytes", "symbol": "MiB"},
      "scope": "per_node",
      "transport": "tcp",
      "notes": "Memory at t=0: 2048 MiB, t=300: 2104 MiB (growth: 186 KiB/s)"
    },
    {
      "metric_name": "gc_pause_avg",
      "value": 1.2,
      "unit": {"dimension": "time", "symbol": "ms"},
      "scope": "per_node",
      "transport": "tcp",
      "sample_size": 15000
    },
    {
      "metric_name": "gc_pause_max",
      "value": 45.8,
      "unit": {"dimension": "time", "symbol": "ms"},
      "scope": "per_node",
      "transport": "tcp"
    }
  ]
}
```

**Stability Analysis:**
```erlang
%% Analyze throughput stability over time
analyze_stability(Samples) ->
    Mean = lists:sum(Samples) / length(Samples),
    Variance = lists:sum([math:pow(S - Mean, 2) || S <- Samples]) / length(Samples),
    StdDev = math:sqrt(Variance),
    CV = (StdDev / Mean) * 100, % Coefficient of variation (%)

    #{
        mean => Mean,
        stddev => StdDev,
        cv_percent => CV,
        stable => CV < 5.0 % Threshold: <5% variation
    }.
```

---

### 2.4 Chaos/Adversarial Benchmark

**Module:** `bench/erlmcp_bench_chaos.erl`

**Purpose:** Test failure modes, edge cases, and system limits.

**Workloads:**
1. `chaos_node_failure` - Simulate node crash during load
2. `chaos_network_partition` - Test distributed Erlang partition handling
3. `chaos_process_crash` - Kill random gen_servers, measure recovery
4. `chaos_backpressure` - Overload system with 10x normal load
5. `chaos_malformed_messages` - Invalid JSON-RPC, schema violations
6. `chaos_large_payloads` - 10 MiB messages, buffer limits
7. `chaos_connection_limit` - Test max connections (file descriptor limits)
8. `chaos_cpu_starvation` - CPU pinned at 100%, measure degradation

**Failure Injection:**
```erlang
%% Inject random process crashes
inject_process_crashes(Duration) ->
    EndTime = erlang:system_time(second) + Duration,
    chaos_loop(EndTime, _CrashCount = 0).

chaos_loop(EndTime, CrashCount) ->
    case erlang:system_time(second) < EndTime of
        true ->
            %% Pick random erlmcp_server process
            Pids = erlang:processes(),
            MCP_Pids = [P || P <- Pids, is_mcp_server(P)],
            case MCP_Pids of
                [] ->
                    timer:sleep(100),
                    chaos_loop(EndTime, CrashCount);
                _ ->
                    Target = lists:nth(rand:uniform(length(MCP_Pids)), MCP_Pids),
                    exit(Target, chaos_test),
                    timer:sleep(1000), % 1 crash per second
                    chaos_loop(EndTime, CrashCount + 1)
            end;
        false ->
            {ok, CrashCount}
    end.
```

**Output Format:**
```json
{
  "workload_id": "chaos_process_crash",
  "measurements": [
    {
      "metric_name": "error_rate",
      "value": 0.02,
      "unit": {"dimension": "percentage", "symbol": "%"},
      "scope": "per_node",
      "transport": "tcp",
      "duration_seconds": 60.0,
      "notes": "Injected 60 process crashes, 0.02% request failure rate"
    },
    {
      "metric_name": "throughput",
      "value": 145000,
      "unit": {"dimension": "rate", "symbol": "msg/s"},
      "scope": "per_node",
      "transport": "tcp",
      "duration_seconds": 60.0,
      "notes": "Baseline: 150000 msg/s, under chaos: 145000 msg/s (3.3% degradation)"
    }
  ]
}
```

---

### 2.5 Integration/End-to-End Benchmark

**Module:** `bench/erlmcp_bench_integration.erl`

**Purpose:** Measure complete MCP protocol workflows (full request/response cycles).

**Workloads:**
1. `e2e_initialize_handshake` - Client initialization sequence
2. `e2e_tools_list_call` - List tools → Call tool → Response
3. `e2e_resources_list_read` - List resources → Read resource → Response
4. `e2e_prompts_list_get` - List prompts → Get prompt → Response
5. `e2e_subscription_workflow` - Subscribe → Notifications → Unsubscribe
6. `e2e_mixed_protocol_operations` - Realistic workload (50% tools, 30% resources, 20% prompts)

**Full Protocol Workflow:**
```erlang
%% End-to-end tools/call workflow
e2e_tools_workflow(ClientPid) ->
    %% 1. Initialize
    {ok, ServerCapabilities} = erlmcp_client:initialize(ClientPid, #{
        protocol_version => <<"2024-11-05">>,
        client_info => #{name => <<"benchmark_client">>, version => <<"1.5.0">>}
    }),

    %% 2. List tools
    {ok, Tools} = erlmcp_client:list_tools(ClientPid),

    %% 3. Call first tool
    [FirstTool | _] = Tools,
    {ok, Result} = erlmcp_client:call_tool(ClientPid, FirstTool#{args => #{}}),

    %% 4. Measure end-to-end latency
    {ok, Result}.
```

**Output Format:**
```json
{
  "workload_id": "e2e_tools_list_call",
  "measurements": [
    {
      "metric_name": "latency_p50",
      "value": 12.4,
      "unit": {"dimension": "time", "symbol": "ms"},
      "scope": "per_connection",
      "transport": "tcp",
      "sample_size": 1000,
      "workload_details": {
        "json_rpc_operations": ["initialize", "tools/list", "tools/call"],
        "message_size_bytes": 512
      },
      "notes": "End-to-end latency: client send → server process → client receive"
    },
    {
      "metric_name": "success_rate",
      "value": 99.98,
      "unit": {"dimension": "percentage", "symbol": "%"},
      "scope": "per_node",
      "transport": "tcp",
      "sample_size": 1000
    }
  ]
}
```

---

## 3. Benchmark Execution Flow

### 3.1 Standard Benchmark Module Structure

```erlang
-module(erlmcp_bench_<category>).
-export([run/0, run/1, workloads/0, workload_def/1]).

%% @doc Run all workloads in this category
-spec run() -> {ok, [result()]} | {error, term()}.
run() ->
    Workloads = workloads(),
    Results = lists:map(fun(WL) -> run(maps:get(id, WL)) end, Workloads),
    {ok, Results}.

%% @doc Run specific workload by ID
-spec run(WorkloadId :: binary()) -> {ok, result()} | {error, term()}.
run(WorkloadId) ->
    %% 1. Load workload definition
    WorkloadDef = workload_def(WorkloadId),

    %% 2. Warmup phase
    warmup(WorkloadDef),

    %% 3. Execute workload
    StartTime = erlang:system_time(microsecond),
    RawMetrics = execute_workload(WorkloadDef),
    EndTime = erlang:system_time(microsecond),
    Duration = (EndTime - StartTime) / 1_000_000, % seconds

    %% 4. Build metrology-compliant result
    Result = build_result(WorkloadId, RawMetrics, Duration),

    %% 5. Validate against schema
    case erlmcp_metrology_validator:validate_artifact(Result) of
        {ok, ValidatedResult} ->
            %% 6. Write result to bench/results/{workload_id}.json
            write_result(WorkloadId, ValidatedResult),
            {ok, ValidatedResult};
        {error, Violations} ->
            ct:fail("Metrology validation failed: ~p", [Violations])
    end.

%% @doc List available workloads
-spec workloads() -> [workload_def()].
workloads() ->
    [
        #{id => <<"category_workload_1">>, ...},
        #{id => <<"category_workload_2">>, ...}
    ].

%% @doc Load workload definition from bench/workloads/{id}.json
-spec workload_def(WorkloadId :: binary()) -> workload_def().
workload_def(WorkloadId) ->
    Path = iolist_to_binary([<<"bench/workloads/">>, WorkloadId, <<".json">>]),
    {ok, JSON} = file:read_file(Path),
    jsx:decode(JSON, [return_maps]).

%% Internal helpers
warmup(WorkloadDef) ->
    %% Run 100-1000 iterations to warm up JIT, caches, etc.
    ok.

execute_workload(WorkloadDef) ->
    %% Execute actual benchmark logic
    #{throughput => 150000, latency_samples => [...]}.

build_result(WorkloadId, RawMetrics, Duration) ->
    #{
        <<"schema_version">> => <<"1.5.0">>,
        <<"artifact_type">> => <<"evidence">>,
        <<"workload_id">> => WorkloadId,
        <<"metadata">> => build_metadata(),
        <<"measurements">> => build_measurements(RawMetrics, Duration)
    }.

build_metadata() ->
    #{
        <<"timestamp">> => iso8601_now(),
        <<"environment">> => <<"ci">>,
        <<"erlang_version">> => erlang:system_info(otp_release),
        <<"hardware">> => #{
            <<"cpu_cores">> => erlang:system_info(logical_processors),
            <<"memory_gb">> => erlang:memory(total) div (1024*1024*1024),
            <<"architecture">> => <<"x86_64">>
        }
    }.

build_measurements(#{throughput := Throughput, latency_samples := Latencies}, Duration) ->
    [
        #{
            <<"metric_name">> => <<"throughput">>,
            <<"value">> => Throughput,
            <<"unit">> => #{<<"dimension">> => <<"rate">>, <<"symbol">> => <<"msg/s">>},
            <<"scope">> => <<"per_node">>,
            <<"transport">> => <<"tcp">>,
            <<"duration_seconds">> => Duration,
            <<"sample_size">> => length(Latencies)
        },
        #{
            <<"metric_name">> => <<"latency_p99">>,
            <<"value">> => percentile(99, Latencies),
            <<"unit">> => #{<<"dimension">> => <<"time">>, <<"symbol">> => <<"ms">>},
            <<"scope">> => <<"per_connection">>,
            <<"transport">> => <<"tcp">>,
            <<"sample_size">> => length(Latencies)
        }
    ].

write_result(WorkloadId, Result) ->
    Timestamp = erlang:system_time(second),
    Filename = iolist_to_binary([
        <<"bench/results/">>,
        WorkloadId,
        <<"_">>,
        integer_to_binary(Timestamp),
        <<".json">>
    ]),
    JSON = jsx:encode(Result, [space, indent]),
    ok = file:write_file(Filename, JSON).
```

### 3.2 Workload Definition Format

**File:** `bench/workloads/{workload_id}.json`

```json
{
  "workload_id": "tcp_sustained_10k_1kib",
  "category": "network_real",
  "transport": "tcp",
  "parameters": {
    "concurrent_connections": 10000,
    "message_size_bytes": 1024,
    "duration_seconds": 30,
    "request_pattern": "constant",
    "json_rpc_operations": ["tools/list", "tools/call"],
    "warmup_iterations": 1000
  },
  "quality_gates": {
    "throughput_min_msg_s": 120000,
    "latency_p99_max_ms": 50,
    "memory_max_mib": 2500,
    "error_rate_max_percent": 0.1
  },
  "environment_requirements": {
    "min_cpu_cores": 8,
    "min_memory_gb": 32,
    "recommended_profile": "prod_hw_spec_01"
  }
}
```

---

## 4. File Organization

### 4.1 Directory Structure

```
bench/
├── erlmcp_bench_core_ops.erl        # Core operations (registry, queue, pool, session)
├── erlmcp_bench_network_real.erl    # TCP/HTTP socket tests
├── erlmcp_bench_stress.erl          # Sustained load (30s/5min/1hr)
├── erlmcp_bench_chaos.erl           # Failure injection, limits
├── erlmcp_bench_integration.erl     # End-to-end MCP workflows
├── workloads/                       # Workload definitions (JSON)
│   ├── core_ops_registry_1k.json
│   ├── core_ops_registry_10k.json
│   ├── core_ops_registry_100k.json
│   ├── tcp_sustained_1k_1kib.json
│   ├── tcp_sustained_10k_1kib.json
│   ├── tcp_sustained_10k_4kib.json
│   ├── stress_sustained_30s.json
│   ├── stress_sustained_5min.json
│   ├── stress_sustained_1hr.json
│   ├── chaos_process_crash.json
│   ├── e2e_tools_list_call.json
│   └── ...
├── results/                         # Timestamped JSON outputs
│   ├── 2026-01-27/
│   │   ├── core_ops_registry_10k_1738067200.json
│   │   ├── tcp_sustained_10k_1kib_1738067230.json
│   │   └── ...
│   └── ...
├── environments/                    # Hardware specs (referenced by workloads)
│   ├── dev_laptop.json
│   ├── prod_hw_spec_01.json
│   ├── prod_hw_spec_02.json
│   └── gov_fips_hardware.json
└── common/                          # Shared benchmark utilities
    ├── erlmcp_bench_common.erl      # Metrics collection helpers
    ├── erlmcp_bench_tcp_client.erl  # TCP client for network benchmarks
    └── erlmcp_bench_http_client.erl # HTTP client for SSE benchmarks
```

### 4.2 Consolidation Mapping

**Before (v1.4.0) → After (v1.5.0):**

| Old File | New Module | Workload ID |
|----------|------------|-------------|
| `erlmcp_registry_contention.erl` | `erlmcp_bench_core_ops.erl` | `core_ops_registry_10k` |
| `erlmcp_transport_tcp_4kb.erl` | `erlmcp_bench_network_real.erl` | `tcp_sustained_10k_4kib` |
| `latency_SUITE.erl` | `erlmcp_bench_network_real.erl` | Multiple workloads (latency integrated) |
| `throughput_SUITE.erl` | `erlmcp_bench_network_real.erl` | Multiple workloads (throughput integrated) |
| `benchmark_100k_SUITE.erl` | `erlmcp_bench_stress.erl` | `stress_sustained_1hr` |
| `benchmark_100k.erl` | DELETED | (merged into stress module) |

**Total Reduction:**
- Before: 15+ benchmark files
- After: 5 benchmark modules + shared utilities
- Code reduction: ~40% (from ~3500 LOC to ~2100 LOC)
- Maintainability: +80% (unified structure, workload-driven)

---

## 5. Metrology Integration

### 5.1 Schema Validation

**Every benchmark result is validated on execution:**

```erlang
%% In erlmcp_bench_<category>:run/1
Result = build_result(WorkloadId, RawMetrics, Duration),
case erlmcp_metrology_validator:validate_artifact(Result) of
    {ok, ValidatedResult} ->
        write_result(WorkloadId, ValidatedResult),
        {ok, ValidatedResult};
    {error, Violations} ->
        ct:pal("Metrology validation failed: ~p", [Violations]),
        ct:fail("Benchmark output violates metrology schema")
end.
```

### 5.2 CI Integration

**Makefile target:**
```makefile
bench-validate:
	rebar3 tcps metrology_validate --dir bench/results/

bench-ci: bench-validate
	rebar3 ct --suite=bench/erlmcp_bench_core_ops
	rebar3 ct --suite=bench/erlmcp_bench_network_real
	$(MAKE) bench-validate
```

**GitHub Actions workflow:**
```yaml
- name: Run benchmarks
  run: |
    make bench-ci

- name: Validate metrology compliance
  run: |
    rebar3 tcps metrology_validate --dir bench/results/

- name: Upload benchmark results
  uses: actions/upload-artifact@v3
  with:
    name: benchmark-results
    path: bench/results/
```

### 5.3 Quality Gates

**Quality gates defined in workload JSON:**
```json
{
  "quality_gates": {
    "throughput_min_msg_s": 120000,
    "latency_p99_max_ms": 50,
    "memory_max_mib": 2500,
    "error_rate_max_percent": 0.1
  }
}
```

**Checked automatically:**
```erlang
%% In erlmcp_metrology_validator:check_quality_gates/2
check_quality_gates(Result, WorkloadDef) ->
    Gates = maps:get(<<"quality_gates">>, WorkloadDef),
    Measurements = maps:get(<<"measurements">>, Result),

    Violations = lists:filtermap(fun(M) ->
        MetricName = maps:get(<<"metric_name">>, M),
        Value = maps:get(<<"value">>, M),

        case MetricName of
            <<"throughput">> ->
                MinThroughput = maps:get(<<"throughput_min_msg_s">>, Gates),
                if Value < MinThroughput ->
                    {true, #{rule => <<"throughput_below_min">>,
                             expected => MinThroughput,
                             actual => Value}};
                   true -> false
                end;
            <<"latency_p99">> ->
                MaxLatency = maps:get(<<"latency_p99_max_ms">>, Gates),
                if Value > MaxLatency ->
                    {true, #{rule => <<"latency_p99_above_max">>,
                             expected => MaxLatency,
                             actual => Value}};
                   true -> false
                end;
            _ -> false
        end
    end, Measurements),

    case Violations of
        [] -> pass;
        _ -> {fail, Violations}
    end.
```

---

## 6. Migration Strategy

### 6.1 Phase 1: Create New Modules (Week 1)

**Tasks:**
1. Create 5 new benchmark modules with module structure
2. Create workload definitions in `bench/workloads/`
3. Implement `erlmcp_bench_common.erl` utilities
4. Add metrology validation to each module

**Deliverables:**
- 5 new `.erl` files in `bench/`
- 20+ workload JSON files in `bench/workloads/`
- Common utilities module

### 6.2 Phase 2: Port Existing Tests (Week 2)

**Tasks:**
1. Port `erlmcp_registry_contention.erl` → `core_ops` workloads
2. Port `erlmcp_transport_tcp_4kb.erl` → `network_real` workloads
3. Port `latency_SUITE.erl` and `throughput_SUITE.erl` → `network_real` measurements
4. Port `benchmark_100k_SUITE.erl` → `stress` workloads
5. Create new `chaos` and `integration` workloads (not previously implemented)

**Deliverables:**
- All existing benchmark logic migrated
- Old files marked deprecated (comments)
- CI updated to use new modules

### 6.3 Phase 3: Deprecate Old Modules (Week 3)

**Tasks:**
1. Update CI to use new benchmark suite
2. Remove old benchmark files
3. Update documentation
4. Create migration guide

**Deliverables:**
- Old files deleted
- `docs/benchmark-migration-guide.md`
- Updated `rebar.config` to exclude old files

---

## 7. Performance Characteristics

### 7.1 Expected Results (Baseline)

**Core Operations:**
- Registry operations: 400K-500K ops/s
- Queue operations: 300K-400K ops/s
- Pool checkouts: 100K-200K ops/s
- Session create/destroy: 50K-100K ops/s

**Network Real (TCP, 10K connections):**
- Throughput: 120K-150K msg/s
- Latency p50: 5-8 ms
- Latency p99: 25-35 ms
- Memory (RSS): 1.8-2.2 GiB

**Stress (5 min sustained):**
- Throughput degradation: <2%
- Memory growth: <200 MiB over 5 minutes
- GC pause avg: <2 ms
- GC pause max: <50 ms

**Chaos (process crashes):**
- Error rate: <0.05% with 1 crash/second
- Throughput degradation: <5%
- Recovery time: <100 ms per crash

**Integration (end-to-end):**
- tools/list latency p50: 8-12 ms
- tools/call latency p50: 10-15 ms
- Success rate: >99.95%

### 7.2 Reproducibility Targets

**Target:** ±2% variance across runs

**Achieved through:**
- Fixed workload parameters (no randomness)
- Warmup phase (1000 iterations minimum)
- Statistical significance (minimum 10s duration, 1000 samples)
- Environment capture (CPU pinning, isolated test runs)

**Measurement:**
```erlang
%% Calculate coefficient of variation across 5 runs
calculate_reproducibility([Run1, Run2, Run3, Run4, Run5]) ->
    Throughputs = [maps:get(throughput, R) || R <- [Run1, Run2, Run3, Run4, Run5]],
    Mean = lists:sum(Throughputs) / 5,
    Variance = lists:sum([math:pow(T - Mean, 2) || T <- Throughputs]) / 5,
    StdDev = math:sqrt(Variance),
    CV = (StdDev / Mean) * 100,

    case CV < 2.0 of
        true -> {ok, CV};
        false -> {fail, CV, "Variance too high"}
    end.
```

---

## 8. Testing Strategy

### 8.1 Benchmark Test Phases

**Phase 1: Correctness (Unit Level)**
- Each workload runs successfully
- Output conforms to metrology schema
- Quality gates work correctly
- Validation catches violations

**Phase 2: Reproducibility (Integration Level)**
- Run each workload 5 times
- Verify CV < 2%
- Verify environment capture works
- Verify warmup phase stabilizes results

**Phase 3: Regression Detection (System Level)**
- Run baseline on known-good commit
- Run on new commit
- Compare results (threshold: ±5%)
- Alert on regressions

### 8.2 Test Execution

```bash
# Run all benchmarks
make bench-all

# Run specific category
rebar3 ct --suite=bench/erlmcp_bench_core_ops

# Run specific workload
rebar3 shell
> erlmcp_bench_network_real:run(<<"tcp_sustained_10k_1kib">>).

# Validate results
rebar3 tcps metrology_validate --dir bench/results/

# Compare against baseline
rebar3 tcps metrology_compare \
  --baseline bench/results/baseline/tcp_sustained_10k_1kib.json \
  --current bench/results/2026-01-27/tcp_sustained_10k_1kib_1738067230.json
```

---

## 9. Documentation Updates Required

### 9.1 New Documentation

1. **`docs/benchmark-architecture.md`** - This document
2. **`docs/benchmark-user-guide.md`** - How to run benchmarks
3. **`docs/benchmark-workload-guide.md`** - How to create new workloads
4. **`docs/benchmark-migration-guide.md`** - v1.4.0 → v1.5.0 migration
5. **`docs/metrology-integration.md`** - How metrology works in benchmarks

### 9.2 Updated Documentation

1. **`CLAUDE.md`** - Add benchmark commands
2. **`README.md`** - Update benchmark section
3. **`docs/architecture.md`** - Add benchmark architecture
4. **`Makefile`** - Add benchmark targets

---

## 10. Success Criteria

### 10.1 Functional Requirements

- [ ] 5 benchmark modules implemented
- [ ] 20+ workload definitions created
- [ ] All outputs validate against metrology schema
- [ ] Quality gates enforce performance targets
- [ ] CI integration blocks merge on violations
- [ ] Reproducibility: CV < 2% across 5 runs

### 10.2 Quality Requirements

- [ ] Code coverage: >80% for benchmark modules
- [ ] Dialyzer clean
- [ ] Documentation complete
- [ ] Migration guide tested on v1.4.0 benchmarks
- [ ] Performance baseline established

### 10.3 Performance Requirements

- [ ] Benchmark execution time: <5 minutes for full suite
- [ ] Results written within 1 second of completion
- [ ] Validation overhead: <100ms per artifact
- [ ] Storage: <100 MiB for 1000 benchmark runs

---

## 11. References

**Related Documents:**
- `docs/metrology/METRICS_GLOSSARY.md` - Canonical metric definitions
- `shapes/metrology.schema.json` - JSON Schema for validation
- `docs/v1.5.0-metrology-validation-architecture.md` - Validation system design
- `docs/architecture.md` - erlmcp system architecture

**External References:**
- Erlang `recon` library: https://ferd.github.io/recon/
- JSON Schema Draft-07: https://json-schema.org/draft-07/schema
- `jesse` validator: https://github.com/for-GET/jesse

---

## Appendix A: Example Benchmark Execution

**Run core operations benchmark:**
```erlang
1> erlmcp_bench_core_ops:run(<<"core_ops_registry_10k">>).

Warmup phase: 1000 iterations
Executing workload: core_ops_registry_10k (10000 operations)
Duration: 30.045 seconds

Metrics collected:
  Throughput: 448,234 ops/s
  Latency p50: 2.1 µs
  Latency p99: 8.4 µs
  Memory (process): 0.024 MiB

Validating against metrology schema... OK
Writing result to bench/results/2026-01-27/core_ops_registry_10k_1738067200.json

{ok, #{
    workload_id => <<"core_ops_registry_10k">>,
    status => <<"PASS">>,
    violations => []
}}.
```

---

**Document Status:** COMPLETE
**Last Updated:** 2026-01-27
**Next Steps:** Implementation via erlang-otp-developer agent
