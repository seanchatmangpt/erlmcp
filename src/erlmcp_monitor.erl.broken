%%%-------------------------------------------------------------------
%%% @doc ErlMCP Continuous Monitoring System
%%% 24/7 observability with real-time monitoring, health checks, and alerting.
%%% @end
%%%-------------------------------------------------------------------
-module(erlmcp_monitor).
-behaviour(gen_server).

%% API
-export([
    start_link/1,
    start_monitoring/1,
    stop_monitoring/0,
    add_alert_rule/2,
    remove_alert_rule/1,
    get_health_status/0,
    get_metrics/0,
    update_config/1
]).

%% gen_server callbacks
-export([
    init/1,
    handle_call/3,
    handle_cast/2,
    handle_info/2,
    terminate/2,
    code_change/3
]).

-include_lib("opentelemetry_api/include/otel_tracer.hrl").

%% Records
-record(state, {
    config :: map(),
    alert_rules :: map(),
    metrics :: map(),
    health_status :: map(),
    dashboard_pid :: pid() | undefined,
    timer_ref :: reference() | undefined
}).

-record(alert_rule, {
    id :: binary(),
    name :: binary(),
    condition :: function(),
    threshold :: number(),
    severity :: critical | warning | info,
    cooldown :: non_neg_integer(),
    last_triggered :: non_neg_integer() | undefined,
    enabled :: boolean()
}).

%% Types
-type health_status() :: #{
    overall => healthy | degraded | unhealthy,
    score => float(),
    components => map(),
    timestamp => non_neg_integer()
}.

-type alert_rule() :: #alert_rule{}.

%%%===================================================================
%%% API
%%%===================================================================

%% @doc Start the monitoring server
-spec start_link(Config :: map()) -> {ok, pid()} | {error, term()}.
start_link(Config) ->
    gen_server:start_link({local, ?MODULE}, ?MODULE, [Config], []).

%% @doc Start continuous monitoring
-spec start_monitoring(Config :: map()) -> ok | {error, term()}.
start_monitoring(Config) ->
    case whereis(?MODULE) of
        undefined ->
            supervisor:start_child(erlmcp_sup, {
                ?MODULE,
                {?MODULE, start_link, [Config]},
                permanent,
                5000,
                worker,
                [?MODULE]
            });
        _Pid ->
            gen_server:call(?MODULE, {start_monitoring, Config})
    end.

%% @doc Stop monitoring
-spec stop_monitoring() -> ok.
stop_monitoring() ->
    gen_server:call(?MODULE, stop_monitoring).

%% @doc Add alert rule
-spec add_alert_rule(RuleId :: binary(), Rule :: alert_rule()) -> ok | {error, term()}.
add_alert_rule(RuleId, Rule) ->
    gen_server:call(?MODULE, {add_alert_rule, RuleId, Rule}).

%% @doc Remove alert rule
-spec remove_alert_rule(RuleId :: binary()) -> ok | {error, term()}.
remove_alert_rule(RuleId) ->
    gen_server:call(?MODULE, {remove_alert_rule, RuleId}).

%% @doc Get current health status
-spec get_health_status() -> health_status() | {error, term()}.
get_health_status() ->
    gen_server:call(?MODULE, get_health_status).

%% @doc Get current metrics
-spec get_metrics() -> map() | {error, term()}.
get_metrics() ->
    gen_server:call(?MODULE, get_metrics).

%% @doc Update monitoring configuration
-spec update_config(Config :: map()) -> ok | {error, term()}.
update_config(Config) ->
    gen_server:call(?MODULE, {update_config, Config}).

%%%===================================================================
%%% gen_server callbacks
%%%===================================================================

init([Config]) ->
    process_flag(trap_exit, true),
    
    % Initialize OpenTelemetry tracer
    SpanCtx = ?start_span(<<"monitor_init">>),
    
    State = #state{
        config = maps:merge(default_config(), Config),
        alert_rules = #{},
        metrics = #{},
        health_status = #{},
        dashboard_pid = undefined,
        timer_ref = undefined
    },
    
    % Set up default alert rules
    DefaultRules = create_default_alert_rules(),
    NewState = State#state{alert_rules = DefaultRules},
    
    % Start monitoring timer
    TimerRef = erlang:start_timer(5000, self(), monitor_tick),
    FinalState = NewState#state{timer_ref = TimerRef},
    
    ?set_attributes(SpanCtx, [
        {<<"monitor.status">>, <<"initialized">>},
        {<<"monitor.rules_count">>, maps:size(DefaultRules)}
    ]),
    ?end_span(SpanCtx),
    
    {ok, FinalState}.

handle_call({start_monitoring, Config}, _From, State) ->
    SpanCtx = ?start_span(<<"start_monitoring">>),
    
    NewState = State#state{config = maps:merge(State#state.config, Config)},
    
    ?set_attributes(SpanCtx, [{<<"monitor.action">>, <<"start">>}]),
    ?end_span(SpanCtx),
    
    {reply, ok, NewState};

handle_call(stop_monitoring, _From, State) ->
    SpanCtx = ?start_span(<<"stop_monitoring">>),
    
    case State#state.timer_ref of
        undefined -> ok;
        TimerRef -> erlang:cancel_timer(TimerRef)
    end,
    
    ?set_attributes(SpanCtx, [{<<"monitor.action">>, <<"stop">>}]),
    ?end_span(SpanCtx),
    
    {reply, ok, State#state{timer_ref = undefined}};

handle_call({add_alert_rule, RuleId, Rule}, _From, State) ->
    SpanCtx = ?start_span(<<"add_alert_rule">>),
    
    Rules = maps:put(RuleId, Rule, State#state.alert_rules),
    NewState = State#state{alert_rules = Rules},
    
    ?set_attributes(SpanCtx, [
        {<<"alert.rule_id">>, RuleId},
        {<<"alert.severity">>, atom_to_binary(Rule#alert_rule.severity)}
    ]),
    ?end_span(SpanCtx),
    
    {reply, ok, NewState};

handle_call({remove_alert_rule, RuleId}, _From, State) ->
    SpanCtx = ?start_span(<<"remove_alert_rule">>),
    
    Rules = maps:remove(RuleId, State#state.alert_rules),
    NewState = State#state{alert_rules = Rules},
    
    ?set_attributes(SpanCtx, [{<<"alert.rule_id">>, RuleId}]),
    ?end_span(SpanCtx),
    
    {reply, ok, NewState};

handle_call(get_health_status, _From, State) ->
    {reply, State#state.health_status, State};

handle_call(get_metrics, _From, State) ->
    {reply, State#state.metrics, State};

handle_call({update_config, Config}, _From, State) ->
    NewConfig = maps:merge(State#state.config, Config),
    {reply, ok, State#state{config = NewConfig}}.

handle_cast(_Msg, State) ->
    {noreply, State}.

handle_info({timeout, _TimerRef, monitor_tick}, State) ->
    SpanCtx = ?start_span(<<"monitor_tick">>),
    
    % Perform health checks
    HealthStatus = perform_health_checks(State#state.config),
    
    % Collect metrics
    Metrics = collect_metrics(State#state.config),
    
    % Check alert rules
    check_alert_rules(State#state.alert_rules, HealthStatus, Metrics),
    
    % Update dashboard
    update_dashboard(HealthStatus, Metrics),
    
    ?set_attributes(SpanCtx, [
        {<<"health.overall_status">>, atom_to_binary(maps:get(overall, HealthStatus, unknown))},
        {<<"health.score">>, maps:get(score, HealthStatus, 0.0)},
        {<<"metrics.count">>, maps:size(Metrics)}
    ]),
    ?end_span(SpanCtx),
    
    % Schedule next tick
    Interval = maps:get(check_interval_ms, State#state.config, 5000),
    TimerRef = erlang:start_timer(Interval, self(), monitor_tick),
    
    NewState = State#state{
        health_status = HealthStatus,
        metrics = Metrics,
        timer_ref = TimerRef
    },
    
    {noreply, NewState};

handle_info(_Info, State) ->
    {noreply, State}.

terminate(_Reason, State) ->
    case State#state.timer_ref of
        undefined -> ok;
        TimerRef -> erlang:cancel_timer(TimerRef)
    end,
    ok.

code_change(_OldVsn, State, _Extra) ->
    {ok, State}.

%%%===================================================================
%%% Internal functions
%%%===================================================================

%% @doc Default monitoring configuration
default_config() ->
    #{
        check_interval_ms => 5000,
        alert_cooldown_ms => 60000,
        health_check_timeout_ms => 1000,
        metrics_retention_hours => 24,
        dashboard_enabled => true,
        alert_handlers => [console, log]
    }.

%% @doc Create default alert rules
create_default_alert_rules() ->
    #{
        <<"high_latency">> => #alert_rule{
            id = <<"high_latency">>,
            name = <<"High Response Latency">>,
            condition = fun(Metrics) ->
                Latency = maps:get(avg_response_time_ms, Metrics, 0),
                Latency > 1000
            end,
            threshold = 1000,
            severity = warning,
            cooldown = 300000, % 5 minutes
            enabled = true
        },
        
        <<"error_rate_high">> => #alert_rule{
            id = <<"error_rate_high">>,
            name = <<"High Error Rate">>,
            condition = fun(Metrics) ->
                ErrorRate = maps:get(error_rate_percent, Metrics, 0),
                ErrorRate > 5.0
            end,
            threshold = 5.0,
            severity = critical,
            cooldown = 180000, % 3 minutes
            enabled = true
        },
        
        <<"memory_usage_high">> => #alert_rule{
            id = <<"memory_usage_high">>,
            name = <<"High Memory Usage">>,
            condition = fun(Metrics) ->
                MemUsage = maps:get(memory_usage_percent, Metrics, 0),
                MemUsage > 85.0
            end,
            threshold = 85.0,
            severity = warning,
            cooldown = 600000, % 10 minutes
            enabled = true
        },
        
        <<"connection_failures">> => #alert_rule{
            id = <<"connection_failures">>,
            name = <<"Connection Failures">>,
            condition = fun(Metrics) ->
                FailureRate = maps:get(connection_failure_rate, Metrics, 0),
                FailureRate > 10.0
            end,
            threshold = 10.0,
            severity = critical,
            cooldown = 120000, % 2 minutes
            enabled = true
        },
        
        <<"service_unavailable">> => #alert_rule{
            id = <<"service_unavailable">>,
            name = <<"Service Unavailable">>,
            condition = fun(Health) ->
                Overall = maps:get(overall, Health, healthy),
                Overall =:= unhealthy
            end,
            threshold = 0,
            severity = critical,
            cooldown = 60000, % 1 minute
            enabled = true
        }
    }.

%% @doc Perform comprehensive health checks
perform_health_checks(Config) ->
    SpanCtx = ?start_span(<<"health_checks">>),
    
    Timeout = maps:get(health_check_timeout_ms, Config, 1000),
    
    Components = #{
        transports => check_transport_health(Timeout),
        registry => check_registry_health(Timeout),
        memory => check_memory_health(Timeout),
        connections => check_connection_health(Timeout),
        processes => check_process_health(Timeout),
        dependencies => check_dependency_health(Timeout)
    },
    
    Score = calculate_health_score(Components),
    OverallStatus = determine_overall_status(Score),
    
    Health = #{
        overall => OverallStatus,
        score => Score,
        components => Components,
        timestamp => erlang:system_time(millisecond)
    },
    
    ?set_attributes(SpanCtx, [
        {<<"health.score">>, Score},
        {<<"health.status">>, atom_to_binary(OverallStatus)},
        {<<"health.components_count">>, maps:size(Components)}
    ]),
    ?end_span(SpanCtx),
    
    Health.

%% @doc Check transport layer health
check_transport_health(Timeout) ->
    try
        % Check if transport processes are alive
        case whereis(erlmcp_transport_sup) of
            undefined -> 
                #{status => unhealthy, reason => <<"transport_supervisor_not_found">>, score => 0.0};
            _Pid ->
                Children = supervisor:which_children(erlmcp_transport_sup),
                ActiveCount = length([C || {_Id, Pid, _Type, _Modules} <- Children, is_pid(Pid)]),
                
                #{
                    status => if ActiveCount > 0 -> healthy; true -> degraded end,
                    active_transports => ActiveCount,
                    score => min(1.0, ActiveCount / 3.0) % Assume 3 is ideal
                }
        end
    catch
        Error:Reason ->
            #{status => unhealthy, error => Error, reason => Reason, score => 0.0}
    end.

%% @doc Check registry health
check_registry_health(Timeout) ->
    try
        case erlmcp_registry:get_all_capabilities() of
            {ok, Capabilities} ->
                Count = length(Capabilities),
                #{
                    status => if Count > 0 -> healthy; true -> degraded end,
                    capabilities_count => Count,
                    score => min(1.0, Count / 10.0) % Assume 10 capabilities is good
                };
            {error, Reason} ->
                #{status => unhealthy, reason => Reason, score => 0.0}
        end
    catch
        Error:Reason ->
            #{status => unhealthy, error => Error, reason => Reason, score => 0.0}
    end.

%% @doc Check memory health
check_memory_health(_Timeout) ->
    try
        MemInfo = erlang:memory(),
        Total = proplists:get_value(total, MemInfo, 0),
        Processes = proplists:get_value(processes, MemInfo, 0),
        System = proplists:get_value(system, MemInfo, 0),
        
        % Convert to MB
        TotalMB = Total div (1024 * 1024),
        ProcessesMB = Processes div (1024 * 1024),
        SystemMB = System div (1024 * 1024),
        
        % Simple heuristic: if total memory > 1GB, it might be concerning
        Status = if 
            TotalMB > 1024 -> degraded;
            TotalMB > 512 -> healthy;
            true -> healthy
        end,
        
        Score = max(0.0, min(1.0, (1024 - TotalMB) / 1024)),
        
        #{
            status => Status,
            total_mb => TotalMB,
            processes_mb => ProcessesMB,
            system_mb => SystemMB,
            score => Score
        }
    catch
        Error:Reason ->
            #{status => unhealthy, error => Error, reason => Reason, score => 0.0}
    end.

%% @doc Check connection health
check_connection_health(_Timeout) ->
    try
        % This would check active MCP connections
        % For now, return a basic health check
        #{
            status => healthy,
            active_connections => 0,
            failed_connections => 0,
            score => 1.0
        }
    catch
        Error:Reason ->
            #{status => unhealthy, error => Error, reason => Reason, score => 0.0}
    end.

%% @doc Check process health
check_process_health(_Timeout) ->
    try
        ProcessCount = erlang:system_info(process_count),
        ProcessLimit = erlang:system_info(process_limit),
        
        Usage = ProcessCount / ProcessLimit,
        Status = if 
            Usage > 0.8 -> unhealthy;
            Usage > 0.6 -> degraded;
            true -> healthy
        end,
        
        Score = max(0.0, min(1.0, (1.0 - Usage))),
        
        #{
            status => Status,
            process_count => ProcessCount,
            process_limit => ProcessLimit,
            usage_percent => Usage * 100,
            score => Score
        }
    catch
        Error:Reason ->
            #{status => unhealthy, error => Error, reason => Reason, score => 0.0}
    end.

%% @doc Check dependency health (external services)
check_dependency_health(_Timeout) ->
    % Placeholder for checking external dependencies
    #{
        status => healthy,
        dependencies => #{},
        score => 1.0
    }.

%% @doc Calculate overall health score
calculate_health_score(Components) ->
    Scores = [maps:get(score, Component, 0.0) || Component <- maps:values(Components)],
    case length(Scores) of
        0 -> 0.0;
        N -> lists:sum(Scores) / N
    end.

%% @doc Determine overall status from score
determine_overall_status(Score) when Score >= 0.8 -> healthy;
determine_overall_status(Score) when Score >= 0.5 -> degraded;
determine_overall_status(_Score) -> unhealthy.

%% @doc Collect system metrics
collect_metrics(_Config) ->
    SpanCtx = ?start_span(<<"collect_metrics">>),
    
    % Collect various metrics
    Metrics = #{
        timestamp => erlang:system_time(millisecond),
        
        % System metrics
        process_count => erlang:system_info(process_count),
        port_count => erlang:system_info(port_count),
        
        % Memory metrics
        memory_total => erlang:memory(total),
        memory_processes => erlang:memory(processes),
        memory_system => erlang:memory(system),
        memory_usage_percent => calculate_memory_usage_percent(),
        
        % Performance metrics
        avg_response_time_ms => get_avg_response_time(),
        error_rate_percent => get_error_rate(),
        connection_failure_rate => get_connection_failure_rate(),
        
        % Custom metrics
        active_spans => get_active_spans_count(),
        registry_size => get_registry_size()
    },
    
    ?set_attributes(SpanCtx, [
        {<<"metrics.timestamp">>, maps:get(timestamp, Metrics)},
        {<<"metrics.process_count">>, maps:get(process_count, Metrics)}
    ]),
    ?end_span(SpanCtx),
    
    Metrics.

%% @doc Calculate memory usage percentage
calculate_memory_usage_percent() ->
    Total = erlang:memory(total),
    % Assume 1GB as reference for percentage calculation
    RefMemory = 1024 * 1024 * 1024,
    min(100.0, (Total / RefMemory) * 100).

%% @doc Get average response time (placeholder)
get_avg_response_time() ->
    % This would integrate with your actual metrics collection
    rand:uniform(100) + 50. % Random value for demo

%% @doc Get error rate (placeholder)
get_error_rate() ->
    % This would integrate with your actual error tracking
    rand:uniform() * 5. % Random value for demo

%% @doc Get connection failure rate (placeholder)
get_connection_failure_rate() ->
    % This would integrate with your connection monitoring
    rand:uniform() * 10. % Random value for demo

%% @doc Get active spans count (placeholder)
get_active_spans_count() ->
    % This would integrate with OpenTelemetry
    rand:uniform(100).

%% @doc Get registry size (placeholder)
get_registry_size() ->
    try
        case erlmcp_registry:get_all_capabilities() of
            {ok, Capabilities} -> length(Capabilities);
            _ -> 0
        end
    catch
        _:_ -> 0
    end.

%% @doc Check alert rules and trigger alerts if needed
check_alert_rules(Rules, HealthStatus, Metrics) ->
    SpanCtx = ?start_span(<<"check_alert_rules">>),
    
    Now = erlang:system_time(millisecond),
    TriggeredCount = maps:fold(
        fun(RuleId, Rule, Acc) ->
            case should_check_rule(Rule, Now) of
                true ->
                    case evaluate_rule(Rule, HealthStatus, Metrics) of
                        true ->
                            trigger_alert(RuleId, Rule, HealthStatus, Metrics),
                            Acc + 1;
                        false ->
                            Acc
                    end;
                false ->
                    Acc
            end
        end,
        0,
        Rules
    ),
    
    ?set_attributes(SpanCtx, [
        {<<"alerts.rules_checked">>, maps:size(Rules)},
        {<<"alerts.triggered">>, TriggeredCount}
    ]),
    ?end_span(SpanCtx),
    
    ok.

%% @doc Check if rule should be evaluated (considering cooldown)
should_check_rule(#alert_rule{enabled = false}, _Now) ->
    false;
should_check_rule(#alert_rule{last_triggered = undefined}, _Now) ->
    true;
should_check_rule(#alert_rule{last_triggered = LastTriggered, cooldown = Cooldown}, Now) ->
    (Now - LastTriggered) > Cooldown.

%% @doc Evaluate alert rule condition
evaluate_rule(#alert_rule{condition = Condition}, HealthStatus, Metrics) ->
    try
        case erlang:fun_info(Condition, arity) of
            {arity, 1} -> Condition(Metrics);
            {arity, 2} -> Condition(HealthStatus, Metrics);
            _ -> false
        end
    catch
        _:_ -> false
    end.

%% @doc Trigger alert
trigger_alert(RuleId, Rule, HealthStatus, Metrics) ->
    SpanCtx = ?start_span(<<"trigger_alert">>),
    
    Alert = #{
        rule_id => RuleId,
        rule_name => Rule#alert_rule.name,
        severity => Rule#alert_rule.severity,
        timestamp => erlang:system_time(millisecond),
        health_status => HealthStatus,
        metrics => Metrics
    },
    
    % Send alert to handlers
    send_alert_to_handlers(Alert),
    
    ?set_attributes(SpanCtx, [
        {<<"alert.rule_id">>, RuleId},
        {<<"alert.severity">>, atom_to_binary(Rule#alert_rule.severity)},
        {<<"alert.rule_name">>, Rule#alert_rule.name}
    ]),
    ?end_span(SpanCtx),
    
    ok.

%% @doc Send alert to configured handlers
send_alert_to_handlers(Alert) ->
    % Console handler
    io:format("ALERT: ~s [~s] - Rule: ~s~n", [
        maps:get(severity, Alert),
        format_timestamp(maps:get(timestamp, Alert)),
        maps:get(rule_name, Alert)
    ]),
    
    % Log handler (would integrate with your logging system)
    % logger:warning("Alert triggered", #{alert => Alert}),
    
    ok.

%% @doc Update dashboard with current metrics
update_dashboard(HealthStatus, Metrics) ->
    % This would update a monitoring dashboard
    % For now, just log the status
    case maps:get(overall, HealthStatus) of
        healthy -> ok;
        Status ->
            io:format("Health Status: ~p, Score: ~.2f~n", [
                Status,
                maps:get(score, HealthStatus, 0.0)
            ])
    end,
    ok.

%% @doc Format timestamp for display
format_timestamp(Timestamp) ->
    DateTime = calendar:system_time_to_local_time(Timestamp, millisecond),
    iso8601:format(DateTime).

%% @doc Convert atom to binary safely
atom_to_binary(Atom) when is_atom(Atom) ->
    list_to_binary(atom_to_list(Atom));
atom_to_binary(Other) ->
    iolist_to_binary(io_lib:format("~p", [Other])).