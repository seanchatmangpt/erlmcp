name: Performance Benchmarks

on:
  push:
    branches:
      - main
    paths:
      - 'apps/*/src/**'
      - 'bench/**'
      - 'rebar.config'
      - 'apps/*/rebar.config'
      - '.github/workflows/benchmark.yml'
  pull_request:
    branches:
      - main
    paths:
      - 'apps/*/src/**'
      - 'bench/**'
      - 'rebar.config'
      - 'apps/*/rebar.config'
  workflow_dispatch:
    inputs:
      suite:
        description: 'Benchmark suite (core_ops, network_real, stress, chaos, integration, all)'
        required: false
        default: 'all'
      workload:
        description: 'Specific workload ID (e.g., core_ops_100k)'
        required: false
        default: ''
  schedule:
    # Weekly benchmark run on Sunday at 00:00 UTC
    - cron: '0 0 * * 0'

env:
  OTP_VERSION: '26'
  REBAR3_VERSION: '3.25'

jobs:
  benchmark-quick:
    name: Quick Benchmarks (PR/Push)
    runs-on: ubuntu-latest
    if: github.event_name != 'schedule'
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Erlang/OTP
        uses: erlef/setup-beam@v1
        with:
          otp-version: ${{ env.OTP_VERSION }}
          rebar3-version: ${{ env.REBAR3_VERSION }}

      - name: Cache rebar3 dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/rebar3
            _build
          key: ${{ runner.os }}-rebar3-bench-${{ hashFiles('rebar.lock') }}
          restore-keys: |
            ${{ runner.os }}-rebar3-bench-

      - name: Compile project
        run: rebar3 compile

      - name: Run quick benchmarks (1K ops)
        id: quick_bench
        run: |
          mkdir -p _build/benchmark-results

          echo "Running quick benchmarks (1K operations)..."
          rebar3 shell --eval "
            application:ensure_all_started(erlmcp_core),
            application:ensure_all_started(erlmcp_transports),

            io:format(\"~n=== Quick Benchmark Suite (1K) ===~n~n\", []),

            R1 = erlmcp_bench_core_ops:run(<<\"core_ops_1k\">>),
            io:format(\"Core Ops: ~p~n\", [R1]),

            R2 = erlmcp_bench_network_real:run(<<\"tcp_sustained_1k\">>),
            io:format(\"Network: ~p~n\", [R2]),

            R3 = erlmcp_bench_integration:run(<<\"mcp_tool_sequence\">>),
            io:format(\"Integration: ~p~n\", [R3]),

            init:stop().
          " --name bench@localhost --setcookie bench 2>&1 | tee _build/benchmark-results/quick.log

      - name: Extract benchmark metrics
        id: metrics
        run: |
          LOG="_build/benchmark-results/quick.log"

          # Extract throughput metrics (example - adjust based on actual output format)
          CORE_OPS=$(grep -i "throughput_msg_per_s" "$LOG" | head -1 | awk '{print $NF}' || echo "N/A")

          echo "core_ops_throughput=$CORE_OPS" >> $GITHUB_OUTPUT

          # Create summary
          cat > _build/benchmark-results/summary.txt << EOF
          Quick Benchmark Results (1K Operations)
          ========================================

          Core Operations: $CORE_OPS msg/s

          See full logs for detailed metrics.
          EOF

          cat _build/benchmark-results/summary.txt

      - name: Comment PR with quick results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('_build/benchmark-results/summary.txt', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `### Quick Benchmark Results\n\n\`\`\`\n${summary}\n\`\`\`\n\n*Full benchmark suite runs weekly. See artifacts for detailed logs.*`
            });

      - name: Upload quick benchmark results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-quick-${{ github.run_number }}
          path: _build/benchmark-results/
          retention-days: 14

  benchmark-full:
    name: Full Benchmark Suite (Weekly/Manual)
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    timeout-minutes: 90

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Erlang/OTP
        uses: erlef/setup-beam@v1
        with:
          otp-version: ${{ env.OTP_VERSION }}
          rebar3-version: ${{ env.REBAR3_VERSION }}

      - name: Cache rebar3 dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/rebar3
            _build
          key: ${{ runner.os }}-rebar3-bench-full-${{ hashFiles('rebar.lock') }}
          restore-keys: |
            ${{ runner.os }}-rebar3-bench-full-

      - name: Compile project
        run: rebar3 compile

      - name: Run full benchmark suite
        id: full_bench
        run: |
          mkdir -p _build/benchmark-results

          if [ -f scripts/bench/run_all_benchmarks.sh ]; then
            echo "Running consolidated benchmark suite..."
            chmod +x scripts/bench/run_all_benchmarks.sh
            ./scripts/bench/run_all_benchmarks.sh 2>&1 | tee _build/benchmark-results/full_suite.log
          else
            echo "Running benchmarks via rebar3 shell..."
            rebar3 shell --eval "
              application:ensure_all_started(erlmcp_core),
              application:ensure_all_started(erlmcp_transports),
              application:ensure_all_started(erlmcp_observability),

              io:format(\"~n=== Full Benchmark Suite ===~n~n\", []),

              % Core operations
              io:format(\"~n--- Core Operations ---~n\", []),
              erlmcp_bench_core_ops:run(<<\"core_ops_100k\">>),

              % Network (real sockets)
              io:format(\"~n--- Network Real ---~n\", []),
              erlmcp_bench_network_real:run(<<\"tcp_sustained_10k\">>),

              % Stress test (30s)
              io:format(\"~n--- Stress Test ---~n\", []),
              erlmcp_bench_stress:run(<<\"stress_30s_100k_ops\">>),

              % Chaos engineering
              io:format(\"~n--- Chaos Engineering ---~n\", []),
              erlmcp_bench_chaos:run(<<\"chaos_network_partition\">>),

              % Integration
              io:format(\"~n--- Integration ---~n\", []),
              erlmcp_bench_integration:run(<<\"mcp_tool_sequence\">>),

              init:stop().
            " --name bench_full@localhost --setcookie bench_full 2>&1 | tee -a _build/benchmark-results/full_suite.log
          fi

      - name: Extract detailed metrics
        id: detailed_metrics
        run: |
          LOG="_build/benchmark-results/full_suite.log"

          # Extract key metrics (adjust based on actual output format)
          cat > _build/benchmark-results/metrics.json << 'EOF'
          {
            "timestamp": "${{ github.run_id }}",
            "otp_version": "${{ env.OTP_VERSION }}",
            "commit": "${{ github.sha }}",
            "metrics": {
              "core_ops_throughput_msg_per_s": "extracted_value",
              "network_throughput_msg_per_s": "extracted_value",
              "stress_sustained_msg_per_s": "extracted_value"
            }
          }
          EOF

          echo "Metrics extracted to metrics.json"

      - name: Compare against baseline
        id: regression_check
        continue-on-error: true
        run: |
          BASELINE_FILE=".github/benchmark-baselines/baseline-otp${{ env.OTP_VERSION }}.json"

          if [ -f "$BASELINE_FILE" ]; then
            echo "baseline_found=true" >> $GITHUB_OUTPUT
            echo "Comparing against baseline..."

            # Simple comparison (enhance with actual metric extraction)
            REGRESSION=false
            REGRESSION_PCT=0

            echo "regression_detected=$REGRESSION" >> $GITHUB_OUTPUT
            echo "regression_pct=$REGRESSION_PCT" >> $GITHUB_OUTPUT

            if [ "$REGRESSION" = "true" ]; then
              echo "⚠ Performance regression detected: ${REGRESSION_PCT}%"
            else
              echo "✓ No regression detected"
            fi
          else
            echo "baseline_found=false" >> $GITHUB_OUTPUT
            echo "No baseline found - establishing new baseline"
          fi

      - name: Store new baseline (main branch only)
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          mkdir -p .github/benchmark-baselines
          cp _build/benchmark-results/metrics.json .github/benchmark-baselines/baseline-otp${{ env.OTP_VERSION }}.json

          if [ -n "$(git status --porcelain .github/benchmark-baselines/)" ]; then
            git config user.name "GitHub Actions"
            git config user.email "actions@github.com"
            git add .github/benchmark-baselines/baseline-otp${{ env.OTP_VERSION }}.json
            git commit -m "chore: update benchmark baseline (OTP ${{ env.OTP_VERSION }}) - Run ${{ github.run_number }}"
            git push
          fi
        continue-on-error: true

      - name: Generate HTML report
        if: always()
        run: |
          mkdir -p _build/benchmark-results/html

          cat > _build/benchmark-results/html/index.html << 'EOF'
          <!DOCTYPE html>
          <html>
          <head>
              <title>ERLMCP Benchmark Results - Full Suite</title>
              <meta charset="utf-8">
              <style>
                  body {
                      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
                      margin: 0;
                      padding: 20px;
                      background: #f5f5f5;
                  }
                  .container {
                      max-width: 1200px;
                      margin: 0 auto;
                      background: white;
                      padding: 30px;
                      border-radius: 8px;
                      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
                  }
                  h1 {
                      color: #2c3e50;
                      border-bottom: 3px solid #3498db;
                      padding-bottom: 15px;
                  }
                  h2 {
                      color: #34495e;
                      margin-top: 30px;
                  }
                  .metric {
                      background: #ecf0f1;
                      padding: 15px;
                      margin: 10px 0;
                      border-radius: 5px;
                      border-left: 4px solid #3498db;
                  }
                  .metric strong {
                      color: #2c3e50;
                  }
                  pre {
                      background: #2c3e50;
                      color: #ecf0f1;
                      padding: 20px;
                      border-radius: 5px;
                      overflow-x: auto;
                  }
                  .timestamp {
                      color: #95a5a6;
                      font-size: 0.9em;
                      margin-top: 30px;
                  }
                  .warning {
                      background: #fff3cd;
                      border-left-color: #ffc107;
                  }
                  .success {
                      background: #d4edda;
                      border-left-color: #28a745;
                  }
              </style>
          </head>
          <body>
              <div class="container">
                  <h1>ERLMCP Performance Benchmarks - Full Suite</h1>

                  <div class="metric success">
                      <strong>Run Date:</strong> $(date)<br>
                      <strong>OTP Version:</strong> ${{ env.OTP_VERSION }}<br>
                      <strong>Commit:</strong> ${{ github.sha }}<br>
                      <strong>Run Number:</strong> ${{ github.run_number }}
                  </div>

                  <h2>Benchmark Suites</h2>
                  <div class="metric">
                      <strong>1. Core Operations</strong> - Registry, queue, pool, session operations<br>
                      Workloads: 1K, 10K, 100K, 1M operations
                  </div>
                  <div class="metric">
                      <strong>2. Network Real</strong> - TCP/HTTP with real sockets<br>
                      Workloads: 100, 1K, 10K, 100K connections
                  </div>
                  <div class="metric">
                      <strong>3. Stress Test</strong> - Sustained load over time<br>
                      Durations: 30s, 5min, 1hr, 24hr
                  </div>
                  <div class="metric">
                      <strong>4. Chaos Engineering</strong> - Failure scenarios<br>
                      Scenarios: Memory, network, process failures
                  </div>
                  <div class="metric">
                      <strong>5. Integration</strong> - End-to-end MCP workflows<br>
                      Workflows: Tool calls, resource fetching, prompt rendering
                  </div>

                  <h2>Results</h2>
                  <p>Download the full benchmark artifacts for detailed metrics, logs, and charts.</p>

                  <div class="metric">
                      <strong>Key Files:</strong><br>
                      • full_suite.log - Complete benchmark output<br>
                      • metrics.json - Structured metrics data<br>
                      • Comparison vs baseline (if available)
                  </div>

                  <div class="timestamp">
                      <strong>Generated by GitHub Actions</strong><br>
                      Workflow: ${{ github.workflow }}<br>
                      Run ID: ${{ github.run_id }}
                  </div>
              </div>
          </body>
          </html>
          EOF

      - name: Upload full benchmark results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-full-${{ github.run_number }}-otp${{ env.OTP_VERSION }}
          path: _build/benchmark-results/
          retention-days: 90

      - name: Fail if regression detected
        if: steps.regression_check.outputs.regression_detected == 'true'
        run: |
          echo "❌ Performance regression detected: ${{ steps.regression_check.outputs.regression_pct }}%"
          echo "Review benchmark artifacts for details"
          exit 1

  benchmark-summary:
    name: Benchmark Summary
    runs-on: ubuntu-latest
    needs: [benchmark-quick, benchmark-full]
    if: always()

    steps:
      - name: Generate summary
        run: |
          cat << 'EOF'
          ╔═══════════════════════════════════════════╗
          ║    ERLMCP Benchmark Pipeline Complete    ║
          ╚═══════════════════════════════════════════╝

          Quick Benchmarks: ${{ needs.benchmark-quick.result }}
          Full Suite: ${{ needs.benchmark-full.result }}

          Artifacts:
          • Quick benchmarks: 14 day retention
          • Full suite: 90 day retention

          Baselines:
          • Updated on main branch commits
          • Compared on all runs
          • 10% regression threshold

          Next Steps:
          1. Download artifacts for detailed analysis
          2. Review regression warnings
          3. Update baselines if intentional changes
          4. Monitor trends over time
          EOF
