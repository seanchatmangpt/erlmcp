name: Performance Benchmarks

on:
  push:
    branches:
      - main
    paths:
      - 'src/**'
      - 'bench/**'
      - 'rebar.config'
      - '.github/workflows/benchmark.yml'
  pull_request:
    branches:
      - main
    paths:
      - 'src/**'
      - 'bench/**'
      - 'rebar.config'
  workflow_dispatch:
    inputs:
      suite:
        description: 'Benchmark suite (throughput, latency, all)'
        required: false
        default: 'all'
      duration:
        description: 'Benchmark duration (seconds)'
        required: false
        default: '10'

env:
  OTP_VERSION: '26'
  REBAR3_VERSION: '3.22.0'

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for comparisons

      - name: Setup Erlang/OTP
        uses: erlef/setup-beam@v1
        with:
          otp-version: ${{ env.OTP_VERSION }}
          rebar3-version: ${{ env.REBAR3_VERSION }}

      - name: Cache rebar3 dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/rebar3
          key: ${{ runner.os }}-rebar3-${{ hashFiles('rebar.lock') }}
          restore-keys: |
            ${{ runner.os }}-rebar3-

      - name: Compile project
        run: |
          rebar3 compile
          rebar3 ct --compile

      - name: Run throughput benchmarks
        id: throughput
        continue-on-error: true
        run: |
          mkdir -p _build/benchmark-results
          rebar3 ct --suite=throughput_SUITE 2>&1 | tee _build/benchmark-results/throughput.log
          echo "status=success" >> $GITHUB_OUTPUT

      - name: Run latency benchmarks
        id: latency
        continue-on-error: true
        run: |
          mkdir -p _build/benchmark-results
          rebar3 ct --suite=latency_SUITE 2>&1 | tee _build/benchmark-results/latency.log
          echo "status=success" >> $GITHUB_OUTPUT

      - name: Process benchmark results
        id: results
        run: |
          # Extract key metrics from benchmark logs
          THROUGHPUT_LOG="_build/benchmark-results/throughput.log"
          LATENCY_LOG="_build/benchmark-results/latency.log"

          # This is a simplified extraction; real implementation would parse detailed metrics
          echo "benchmark_summary<<EOF" >> $GITHUB_OUTPUT
          echo "### Performance Benchmarks" >> $GITHUB_OUTPUT
          echo "" >> $GITHUB_OUTPUT
          echo "✓ Throughput benchmarks: $([ -f "$THROUGHPUT_LOG" ] && echo "Passed" || echo "Pending")" >> $GITHUB_OUTPUT
          echo "✓ Latency benchmarks: $([ -f "$LATENCY_LOG" ] && echo "Passed" || echo "Pending")" >> $GITHUB_OUTPUT
          echo "" >> $GITHUB_OUTPUT
          echo "See attached logs for detailed metrics." >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Check for regressions
        id: regression_check
        continue-on-error: true
        run: |
          # Compare with baseline if it exists
          if [ -f ".github/benchmark-baselines/baseline-${{ env.OTP_VERSION }}.txt" ]; then
            echo "baseline_found=true" >> $GITHUB_OUTPUT
            # Run regression detection (placeholder)
            echo "Comparing against baseline metrics..."
          else
            echo "baseline_found=false" >> $GITHUB_OUTPUT
            echo "No baseline found - this is the first benchmark run"
          fi

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const { benchmark_summary } = process.env;
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `${{ steps.results.outputs.benchmark_summary }}`
            });

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results-${{ runner.os }}-${{ env.OTP_VERSION }}
          path: _build/benchmark-results/
          retention-days: 30

      - name: Store baseline (main branch only)
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          mkdir -p .github/benchmark-baselines
          cp _build/benchmark-results/throughput.log .github/benchmark-baselines/baseline-${{ env.OTP_VERSION }}.txt
          if [ -n "$(git status --porcelain .github/benchmark-baselines/)" ]; then
            git config user.name "GitHub Actions"
            git config user.email "actions@github.com"
            git add .github/benchmark-baselines/baseline-${{ env.OTP_VERSION }}.txt
            git commit -m "chore: update benchmark baseline (OTP ${{ env.OTP_VERSION }})"
            git push
          fi
        continue-on-error: true

      - name: Generate HTML report
        if: always()
        run: |
          # Create a simple HTML report combining log files
          cat > _build/benchmark-results/index.html << 'EOF'
          <!DOCTYPE html>
          <html>
          <head>
              <title>ERLMCP Benchmark Results</title>
              <meta charset="utf-8">
              <style>
                  body {
                      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
                      margin: 20px;
                      background: #f5f5f5;
                  }
                  .container {
                      max-width: 1000px;
                      margin: 0 auto;
                      background: white;
                      padding: 20px;
                      border-radius: 8px;
                      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                  }
                  h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }
                  pre { background: #f8f9fa; padding: 15px; border-radius: 4px; overflow-x: auto; }
                  .timestamp { color: #95a5a6; font-size: 0.9em; margin-top: 20px; }
              </style>
          </head>
          <body>
              <div class="container">
                  <h1>ERLMCP Performance Benchmarks</h1>
                  <p><strong>Run Date:</strong> $(date)</p>
                  <p><strong>OTP Version:</strong> ${{ env.OTP_VERSION }}</p>

                  <h2>Benchmark Results</h2>
                  <p>See artifact attachments for complete benchmark logs.</p>

                  <div class="timestamp">
                      <strong>This report was generated by GitHub Actions</strong><br>
                      For detailed metrics, download the benchmark artifacts.
                  </div>
              </div>
          </body>
          </html>
          EOF

      - name: Fail if benchmarks failed
        if: steps.throughput.outcome == 'failure' || steps.latency.outcome == 'failure'
        run: |
          echo "⚠ One or more benchmarks failed. See logs above for details."
          exit 1

  benchmark-report:
    name: Generate Performance Report
    runs-on: ubuntu-latest
    needs: benchmark
    if: always()
    timeout-minutes: 10

    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v3

      - name: Create summary report
        run: |
          echo "# Performance Benchmark Summary" > report.md
          echo "" >> report.md
          echo "**Status**: Benchmark run completed" >> report.md
          echo "**Date**: $(date)" >> report.md
          echo "**Branch**: ${{ github.ref_name }}" >> report.md
          echo "" >> report.md
          echo "## Artifacts" >> report.md
          echo "Benchmark results are available in the GitHub Actions artifacts." >> report.md
          cat report.md

      - name: Upload summary
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-summary
          path: report.md
          retention-days: 90
