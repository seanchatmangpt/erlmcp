name: Benchmark Validation & Performance Regression

on:
  push:
    branches: [main, 'release/**', 'feature/**']
  pull_request:
    branches: [main, 'release/**']
  schedule:
    # Run performance validation daily at 4 AM UTC
    - cron: '0 4 * * *'
  workflow_dispatch:
    inputs:
      benchmark_duration:
        description: 'Benchmark duration (seconds)'
        required: false
        default: '30'

env:
  REGRESSION_THRESHOLD: 10
  BENCHMARK_TIMEOUT_MINUTES: 20

jobs:
  # =============================================================================
  # JOB 1: Quick Benchmark Validation
  # =============================================================================
  quick-benchmark:
    name: Quick Benchmark Validation
    runs-on: ubuntu-22.04
    timeout-minutes: 10

    strategy:
      matrix:
        otp_version: [26]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Erlang/OTP
        uses: erlef/setup-beam@v1
        with:
          otp-version: ${{ matrix.otp_version }}
          rebar3-version: '3.25'

      - name: Cache benchmark build
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/rebar3
            _build
          key: ${{ runner.os }}-bench-quick-otp${{ matrix.otp_version }}-${{ hashFiles('rebar.lock', 'bench/*.erl') }}
          restore-keys: |
            ${{ runner.os }}-bench-quick-otp${{ matrix.otp_version }}-
            ${{ runner.os }}-bench-quick-

      - name: Compile benchmarks
        run: |
          echo "::group::Compiling Benchmarks"
          rebar3 compile
          echo "::endgroup::"

      - name: Run quick benchmark smoke test
        id: quick_bench
        run: |
          echo "::group::Quick Benchmark Smoke Test"

          rebar3 shell --eval "
            application:ensure_all_started(erlmcp_core),
            application:ensure_all_started(erlmcp_transports),

            io:format(\"~n=== Quick Benchmark: core_ops_1k ===~n\"),
            Result1 = erlmcp_bench_core_ops:run(<<\"core_ops_1k\">>),
            io:format(\"Registry throughput: ~p msg/s~n\", [maps:get(throughput_msg_per_s, Result1)]),

            io:format(\"~n=== Quick Benchmark: network_100_conn ===~n\"),
            Result2 = erlmcp_bench_network_real:run(<<\"tcp_conn_100\">>),
            io:format(\"Network throughput: ~p msg/s~n\", [maps:get(throughput_msg_per_s, Result2)]),

            %% Verify no critical failures
            case maps:get(error, Result1, undefined) of
              undefined -> ok;
              Error -> io:format(\"❌ Benchmark error: ~p~n\", [Error]), init:stop(1)
            end,

            init:stop()
          " --name bench_quick@localhost --setcookie bench_cookie

          BENCH_EXIT=$?
          echo "::endgroup::"

          if [ $BENCH_EXIT -ne 0 ]; then
            echo "::warning::Quick benchmark smoke test failed (non-blocking)"
            echo "benchmark_status=warning" >> $GITHUB_OUTPUT
          else
            echo "benchmark_status=success" >> $GITHUB_OUTPUT
          fi

      - name: Upload quick benchmark results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: quick-benchmark-results-otp${{ matrix.otp_version }}
          path: |
            benchmark_*.log
          retention-days: 7

  # =============================================================================
  # JOB 2: Full Benchmark Suite
  # =============================================================================
  full-benchmark:
    name: Full Benchmark Suite
    runs-on: ubuntu-22.04
    timeout-minutes: 45
    needs: quick-benchmark

    strategy:
      matrix:
        benchmark_suite:
          - core_ops
          - network_real
          - stress
          - chaos
          - integration

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Erlang/OTP
        uses: erlef/setup-beam@v1
        with:
          otp-version: '26'
          rebar3-version: '3.25'

      - name: Cache benchmark suite
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/rebar3
            _build
          key: ${{ runner.os }}-bench-full-${{ matrix.benchmark_suite }}-${{ hashFiles('bench/${{ matrix.benchmark_suite }}/*.erl') }}

      - name: Run ${{ matrix.benchmark_suite }} suite
        id: benchmark_suite
        run: |
          echo "::group::Benchmark Suite: ${{ matrix.benchmark_suite }}"

          case "${{ matrix.benchmark_suite }}" in
            core_ops)
              WORKLOADS=("core_ops_1k" "core_ops_10k" "core_ops_100k")
              ;;
            network_real)
              WORKLOADS=("tcp_conn_100" "tcp_conn_1k" "http_conn_100")
              ;;
            stress)
              WORKLOADS=("stress_30s" "sustained_1m")
              ;;
            chaos)
              WORKLOADS=("chaos_memory" "chaos_kill" "chaos_network")
              ;;
            integration)
              WORKLOADS=("mcp_tool_sequence" "mcp_resource_flow")
              ;;
          esac

          cat > run_benchmark.erl << 'EOF'
          #!/usr/bin/env escript
          main([Suite, Workload]) ->
              io:format("Running ~s: ~s~n", [Suite, Workload]),
              Module = list_to_existing_atom("erlmcp_bench_" ++ Suite),
              Result = case Module:run(list_to_binary(Workload)) of
                  #{throughput_msg_per_s := Throughput} = R ->
                      io:format("Throughput: ~p msg/s~n", [Throughput]),
                      io:format("Result: ~p~n", [R]),
                      {ok, R};
                  {error, Reason} ->
                      io:format("Error: ~p~n", [Reason]),
                      {error, Reason}
              end,
              halt(case Result of {ok, _} -> 0; {error, _} -> 1 end).
          EOF

          chmod +x run_benchmark.erl

          SUITE_RESULTS=()
          for workload in "${WORKLOADS[@]}"; do
            echo "Running: $workload"
            escript run_benchmark.erl "${{ matrix.benchmark_suite }}" "$workload" \
              >> "benchmark_${{ matrix.benchmark_suite }}.log" 2>&1
            RESULT=$?
            SUITE_RESULTS+=($RESULT)

            if [ $RESULT -ne 0 ]; then
              echo "::warning::Workload $workload failed"
            fi
          done

          echo "::endgroup::"

          # Check if all workloads passed
          FAILED=0
          for result in "${SUITE_RESULTS[@]}"; do
            if [ $result -ne 0 ]; then
              FAILED=$((FAILED + 1))
            fi
          done

          if [ $FAILED -gt 0 ]; then
            echo "suite_status=partial" >> $GITHUB_OUTPUT
            echo "::warning::$FAILED/${#SUITE_RESULTS[@]} workloads failed in ${{ matrix.benchmark_suite }}"
          else
            echo "suite_status=success" >> $GITHUB_OUTPUT
            echo "✅ All ${#SUITE_RESULTS[@]} workloads passed"
          fi

      - name: Extract benchmark metrics
        if: always()
        run: |
          echo "::group::Benchmark Metrics - ${{ matrix.benchmark_suite }}"

          if [ -f "benchmark_${{ matrix.benchmark_suite }}.log" ]; then
            echo "## ${{ matrix.benchmark_suite }} Metrics" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            grep -E "(Throughput:|Latency:|Memory:|Result:)" "benchmark_${{ matrix.benchmark_suite }}.log" || true
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

          echo "::endgroup::"

      - name: Upload benchmark suite results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-${{ matrix.benchmark_suite }}-results
          path: |
            benchmark_${{ matrix.benchmark_suite }}.log
            run_benchmark.erl
          retention-days: 14

  # =============================================================================
  # JOB 3: Performance Regression Detection
  # =============================================================================
  regression-detection:
    name: Performance Regression Detection
    runs-on: ubuntu-22.04
    timeout-minutes: 15
    needs: full-benchmark

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 100

      - name: Setup Erlang/OTP
        uses: erlef/setup-beam@v1
        with:
          otp-version: '26'
          rebar3-version: '3.25'

      - name: Download all benchmark results
        uses: actions/download-artifact@v3
        with:
          path: benchmark-results/

      - name: Compare with baseline
        id: regression_check
        run: |
          echo "::group::Performance Regression Detection"

          # Fetch baseline from main branch
          git fetch origin main:main_branch

          # Check if benchmark baseline exists
          if [ -f "bench/baseline.json" ]; then
            echo "Found baseline in repository"

            # Parse current results
            for result_file in benchmark-results/benchmark-*.log; do
              if [ -f "$result_file" ]; then
                echo "Processing $result_file"

                # Extract throughput metrics
                CURRENT_THROUGHPUT=$(grep "Throughput:" "$result_file" | head -1 | awk '{print $2}' || echo "0")
                BASELINE_THROUGHPUT=$(jq -r '.core_ops_1k.throughput' bench/baseline.json 2>/dev/null || echo "0")

                if [ "$CURRENT_THROUGHPUT" != "0" ] && [ "$BASELINE_THROUGHPUT" != "0" ]; then
                  # Calculate regression percentage
                  REGRESSION=$(echo "scale=2; (($BASELINE_THROUGHPUT - $CURRENT_THROUGHPUT) / $BASELINE_THROUGHPUT) * 100" | bc)

                  # Check if regression exceeds threshold
                  if (( $(echo "$REGRESSION > ${{ env.REGRESSION_THRESHOLD }}" | bc -l) )); then
                    echo "::error::Performance regression detected: ${REGRESSION}% > ${{ env.REGRESSION_THRESHOLD }}%"
                    echo "regression_status=failed" >> $GITHUB_OUTPUT
                    REGRESSION_DETECTED=true
                  else
                    echo "✅ No significant regression: ${REGRESSION}%"
                  fi
                fi
              fi
            done
          else
            echo "::warning::No baseline found, skipping regression check"
            echo "regression_status=skipped" >> $GITHUB_OUTPUT
          fi

          if [ "$REGRESSION_DETECTED" != "true" ]; then
            echo "regression_status=success" >> $GITHUB_OUTPUT
          fi

          echo "::endgroup::"

      - name: Create performance summary
        if: always()
        run: |
          echo "## Performance Regression Analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Current | Baseline | Regression |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|---------|----------|------------|" >> $GITHUB_STEP_SUMMARY

          # Add metrics if available
          if [ -f "benchmark-results/benchmark-core_ops.log" ]; then
            THROUGHPUT=$(grep "Throughput:" benchmark-results/benchmark-core_ops.log | head -1 | awk '{print $2}' || echo "N/A")
            echo "| Core Ops Throughput | ${THROUGHPUT} msg/s | See baseline | ${{ steps.regression_check.outputs.regression_status }} |" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Regression Threshold:** >${{ env.REGRESSION_THRESHOLD }}%" >> $GITHUB_STEP_SUMMARY

  # =============================================================================
  # JOB 4: Metrology Compliance Check
  # =============================================================================
  metrology-compliance:
    name: Benchmark Metrology Compliance
    runs-on: ubuntu-22.04
    timeout-minutes: 10
    needs: full-benchmark

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download benchmark results
        uses: actions/download-artifact@v3
        with:
          path: benchmark-results/

      - name: Validate metric units
        id: metrology_check
        run: |
          echo "::group::Metrology Compliance Check"

          # Run metrology validator
          rebar3 shell --eval "
            %% Validate benchmark results for proper units
            ResultFiles = filelib:wildcard(\"benchmark-results/benchmark-*.log\"),

            ValidationResults = lists:map(fun(File) ->
              {ok, Content} = file:read_file(File),
              Lines = binary:split(Content, <<\"\\n\">>, [global]),

              %% Check for proper units
              HasThroughputMsgPerS = lists:any(fun(Line) ->
                binary:match(Line, <<\"throughput_msg_per_s\">>) =/= nomatch
              end, Lines),

              HasLatencyUs = lists:any(fun(Line) ->
                binary:match(Line, <<\"latency_p\">>) =/= nomatch andalso
                binary:match(Line, <<\"_us\">>) =/= nomatch
              end, Lines),

              HasMemoryMib = lists:any(fun(Line) ->
                binary:match(Line, <<\"mib\">>) =/= nomatch
              end, Lines),

              {File, [
                {throughput_msg_per_s, HasThroughputMsgPerS},
                {latency_microseconds, HasLatencyUs},
                {memory_mib, HasMemoryMib}
              ]}
            end, ResultFiles),

            %% Report validation results
            lists:foreach(fun({File, Checks}) ->
              io:format(\"~n~s:~n\", [File]),
              lists:foreach(fun({Metric, Present}) ->
                Status = case Present of true -> \"✅\"; false -> \"❌\" end,
                io:format(\"  ~s ~s~n\", [Status, Metric])
              end, Checks)
            end, ValidationResults),

            %% Fail if any critical units missing
            AllValid = lists:all(fun({_File, Checks}) ->
              proplists:get_value(throughput_msg_per_s, Checks) =:= true
            end, ValidationResults),

            case AllValid of
              true -> init:stop(0);
              false ->
                io:format(\"~n❌ Metrology compliance check failed~n\"),
                init:stop(1)
            end
          " --name metrology@localhost --setcookie metrology_cookie

          METROLOGY_EXIT=$?
          echo "::endgroup::"

          if [ $METROLOGY_EXIT -ne 0 ]; then
            echo "::error::Metrology compliance check FAILED"
            echo "metrology_status=failed" >> $GITHUB_OUTPUT
          else
            echo "metrology_status=success" >> $GITHUB_OUTPUT
          fi

      - name: Generate metrology report
        if: always()
        run: |
          echo "## Benchmark Metrology Compliance" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "**Status:** ${{ steps.metrology_check.outputs.metrology_status }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Required canonical units:" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ throughput_msg_per_s (NOT req/s)" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ latency_p50_us, p95, p99 (microseconds)" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ memory_heap_mib_per_conn (mebibytes)" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ memory_rss_mib_per_node (mebibytes)" >> $GITHUB_STEP_SUMMARY

  # =============================================================================
  # JOB 5: Benchmark Summary & Badges
  # =============================================================================
  benchmark-summary:
    name: Benchmark Summary
    runs-on: ubuntu-22.04
    timeout-minutes: 5
    needs: [quick-benchmark, full-benchmark, regression-detection, metrology-compliance]

    steps:
      - name: Download all results
        uses: actions/download-artifact@v3
        with:
          path: all-results/

      - name: Generate benchmark summary
        run: |
          echo "╔═══════════════════════════════════════════╗" >> $GITHUB_STEP_SUMMARY
          echo "║      Benchmark Validation Summary       ║" >> $GITHUB_STEP_SUMMARY
          echo "╚═══════════════════════════════════════════╝" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "| Suite | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Quick Benchmark | ${{ needs.quick-benchmark.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Full Benchmark | ${{ needs.full-benchmark.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Regression Check | ${{ needs.regression-detection.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Metrology Compliance | ${{ needs.metrology-compliance.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Overall assessment
          if [ "${{ needs.regression-detection.result }}" = "success" ] && \
             [ "${{ needs.metrology-compliance.result }}" = "success" ]; then
            echo "✅ **Performance validation PASSED**" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ **Performance validation has issues**" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Create performance badge
        if: always()
        run: |
          # Determine badge color based on results
          if [ "${{ needs.regression-detection.result }}" = "success" ] && \
             [ "${{ needs.metrology-compliance.result }}" = "success" ]; then
            COLOR="brightgreen"
            STATUS="PASS"
          else
            COLOR="yellow"
            STATUS="WARN"
          fi

          cat > performance-badge.svg << EOF
          <svg xmlns="http://www.w3.org/2000/svg" width="200" height="24">
            <linearGradient id="b" x2="0" y2="100%">
              <stop offset="0" stop-color="#bbb" stop-opacity=".1"/>
              <stop offset="1" stop-opacity=".1"/>
            </linearGradient>
            <mask id="a">
              <rect width="200" height="24" rx="4" fill="#fff"/>
            </mask>
            <g mask="url(#a)">
              <path fill="#555" d="M0 0h100v24H0z"/>
              <path fill="${COLOR}" d="M100 0h100v24H100z"/>
              <path fill="url(#b)" d="M0 0h200v24H0z"/>
            </g>
            <g fill="#fff" text-anchor="middle" font-family="DejaVu Sans,Verdana,Geneva,sans-serif" font-size="11">
              <text x="50" y="17" fill="#010101" fill-opacity=".3">Performance</text>
              <text x="50" y="16">Performance</text>
              <text x="150" y="17" fill="#010101" fill-opacity=".3">${STATUS}</text>
              <text x="150" y="16">${STATUS}</text>
            </g>
          </svg>
          EOF

      - name: Upload performance badge
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: actions/upload-artifact@v3
        with:
          name: performance-badge
          path: performance-badge.svg

      - name: Upload combined benchmark report
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-summary-report
          path: all-results/
          retention-days: 30
